{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 754.3798828125,
      "end_time": 1600032871895.969
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rawDatafileDir='C:/Users/bhanup.NORTHAMERICA/OneDrive - Microsoft/hackathon_2020/bow/rawData'\n",
    "workingDir='C:/Users/bhanup.NORTHAMERICA/OneDrive - Microsoft/hackathon_2020/bow/workingDir'\n",
    "rawcaseDatafilename='SupportCaseData.csv'\n",
    "\n",
    "#work with Hive, since I do not how storage container name.  Hive might work best as one can write ANSI SQL to do the work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.505859375,
      "end_time": 1600036683194.165
     }
    }
   },
   "outputs": [],
   "source": [
    "#shows all of the output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after manually uploading case_data file to headnead from jupyter's upload functionality move files to hdfs \n",
    "%%sh\n",
    "hdfs dfs -mkdir /bow\n",
    "hdfs dfs -mkdir /bow/data\n",
    "hdfs dfs -put /var/lib/jupyter/SupportCaseData.csv /bow/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2259.466064453125,
      "end_time": 1600035981916.99
     }
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7d81fd233151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#now create dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcsvFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/bow/data/SupportCaseData.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#now create dataframe \n",
    "csvFile = spark.read.csv('/bow/data/SupportCaseData.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 780.613037109375,
      "end_time": 1600035993631.65
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|     IncidentId|     CreatedDateTime|               Title|    IssueDescription|             Subject|    DaysToSolution|RootCauseSupportTopic|InitialSupportTopicPath|         Symptomstxt|            Causetxt|       Resolutiontxt| RelatedICM_IDs|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|120022424000075|2020-02-24 01:11:...|Kerbaroes TGT err...|Question: What ti...|Kerbaroes TGT err...| 0.161405439267361| Root Cause : HDIn...|   Routing Azure HDI...|VM reboots very o...|The logs indicate...|The Vm in questio...|      177045841|\n",
      "|120022424001185|2020-02-24 13:42:...|some pipelines ar...|We're trying to d...|some pipelines ar...|  2.11228516486458| Root Cause : HDIn...|   Routing Azure Dat...|Out of memory exc...|Based on the inve...|For the jobs fail...|           null|\n",
      "|120022424001896|2020-02-24 15:25:...|Node manager unhe...|Question: What ti...|Node manager unhe...|0.0353126219814815| Root Cause : HDIn...|   Routing Azure HDI...|  warnings in ambari|Node manager unhe...|  restarted nodemgr.|           null|\n",
      "|120022424003850|2020-02-24 19:16:...|Cluster not scali...|Question: What ti...|Cluster not scali...|  2.05221198888889| Root Cause : HDIn...|   Routing Azure HDI...|     unable to scale|        zombie nodes|\"zombie nodes cle...|           null|\n",
      "|120022421002252|2020-02-24 21:33:...|not possible to s...|can't scale up th...|not possible to s...|0.0333978233738426| Root Cause : HDIn...|   Routing Azure HDI...|Issue : Scaling f...|Cause: cluster is...|Resolution : Rest...|           null|\n",
      "|120022421002304|2020-02-24 21:58:...|Overall cluster's...|Question: What ti...|Overall cluster's...|0.0790402790034722| Root Cause : HDIn...|   Routing Azure HDI...|Slower query perf...|Customer is loadi...|Setting hive.fetc...|      180049259|\n",
      "|120022523001923|2020-02-25 13:51:...|Creation of a HDi...|Question: What ti...|Creation of a HDi...|0.0284669677627315| Root Cause : HDIn...|   Routing Azure HDI...|120022523001923 -...|Product Group con...|Use HDInsight 4.0...|      177173517|\n",
      "|120022524004405|2020-02-25 18:22:...|Creation of hdins...|Question: What ti...|Creation of hdins...|0.0232945721643519| Root Cause : HDIn...|   Routing Azure HDI...|1: User is unable...|We had two issues...|For the first iss...|           null|\n",
      "|120022523002235|2020-02-25 19:35:...|  Show Server Error |Question: What ti...|  Show Server Error | 0.053775250193287| Root Cause : HDIn...|   Routing Azure HDI...|Ambari UI throwin...|Ambari-Server status|In our troublesho...|           null|\n",
      "|120022524005081|2020-02-25 19:36:...|Erroe while creat...|Question: What ti...|Erroe while creat...|0.0476059138680556| Root Cause : HDIn...|   Routing Azure HDI...|Unable to create ...|This is a transie...|Redeployed the lc...|           null|\n",
      "|120022524005104|2020-02-25 19:39:...|Health of cluster...|Question: What ti...|Health of cluster...|  0.96969738294213| Root Cause : HDIn...|   Routing Azure HDI...|Health of cluster...|Heartbeat lost on...|Rebooted both the...|           null|\n",
      "|120022521002033|2020-02-25 22:16:...|      Updating Error|Question: What ti...|      Updating Error| 0.988517668150463| Root Cause : HDIn...|   Routing Azure HDI...|Cluster stuck in ...|\"Symptoms. The sc...|            Consolas|Liberation Mono|\n",
      "|120022624002669|2020-02-26 15:48:...|corrupted authori...|one of our data a...|corrupted authori...|0.0348703490231481| Root Cause : HDIn...|   Routing Azure HDI...|No access to the ...|A user added thei...|User followed the...|           null|\n",
      "|120022624002974|2020-02-26 16:21:...|HIVE service unab...|Question: What ti...|HIVE service unab...| 0.290624636460648| Root Cause : HDIn...|   Routing Azure HDI...|Hive services una...|Hive metastore ve...|Updated the metas...|           null|\n",
      "|120022621002303|2020-02-26 19:13:...|Resource Manager ...|Question: What ti...|Resource Manager ...|0.0100522275752315| Root Cause : HDIn...|   Routing Azure HDI...|We determined tha...|Resource Manager ...|We determined tha...|           null|\n",
      "|120022624004955|2020-02-26 20:14:...|Need to stop Amba...|Question: What ti...|Need to stop Amba...|0.0496782808171296| Root Cause : HDIn...|   Routing Azure HDI...|You had an 'Allow...|Customer wanted t...|This was an NGS i...|           null|\n",
      "|120022621002654|2020-02-26 21:30:...|         HDI version|Question: What ti...|         HDI version|0.0260345540162037| Root Cause : HDIn...|   Routing Azure HDI...|Issue:HDI Version...| Cause:          ...|Fix:HDI Product g...|      177348164|\n",
      "|120022721000135|2020-02-27 01:39:...|FD QA :  kpphv806...|Question: What ti...|FD QA :  kpphv806...|  26.5825467346944| Root Cause : HDIn...|   Routing Azure HDI...|FD QA : kpphv806l...|When The client (...| To have query hb...|      179623156|\n",
      "|120022724002711|2020-02-27 15:05:...|HDI ambari GUI we...|Question: What ti...|HDI ambari GUI we...|  0.08759238603125| Root Cause : HDIn...|   Routing Azure HDI...|HDI ambari GUI we...|HDI ambari GUI we...|Support checked a...|           null|\n",
      "|120022724002914|2020-02-27 15:26:...|Issues Containerc...|Question: What ti...|Issues Containerc...|  5.10805360234722| Root Cause : HDIn...|   Routing Azure HDI...|slow listing of f...|             unknown|created copy of f...|           null|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "csvFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 51.8408203125,
      "end_time": 1600036014744.214
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IncidentId: long (nullable = true)\n",
      " |-- CreatedDateTime: timestamp (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- IssueDescription: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- DaysToSolution: double (nullable = true)\n",
      " |-- RootCauseSupportTopic: string (nullable = true)\n",
      " |-- InitialSupportTopicPath: string (nullable = true)\n",
      " |-- Symptomstxt: string (nullable = true)\n",
      " |-- Causetxt: string (nullable = true)\n",
      " |-- Resolutiontxt: string (nullable = true)\n",
      " |-- RelatedICM_IDs: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "csvFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 264.1630859375,
      "end_time": 1600036460086.268
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:135: DeprecationWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  \"Deprecated in 2.0, use createOrReplaceTempView instead.\", DeprecationWarning)"
     ]
    }
   ],
   "source": [
    "csvFile.registerTempTable(\"caseData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 781.755859375,
      "end_time": 1600036524420.454
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|     IncidentId|     CreatedDateTime|               Title|    IssueDescription|             Subject|    DaysToSolution|RootCauseSupportTopic|InitialSupportTopicPath|         Symptomstxt|            Causetxt|       Resolutiontxt| RelatedICM_IDs|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|120022424000075|2020-02-24 01:11:...|Kerbaroes TGT err...|Question: What ti...|Kerbaroes TGT err...| 0.161405439267361| Root Cause : HDIn...|   Routing Azure HDI...|VM reboots very o...|The logs indicate...|The Vm in questio...|      177045841|\n",
      "|120022424001185|2020-02-24 13:42:...|some pipelines ar...|We're trying to d...|some pipelines ar...|  2.11228516486458| Root Cause : HDIn...|   Routing Azure Dat...|Out of memory exc...|Based on the inve...|For the jobs fail...|           null|\n",
      "|120022424001896|2020-02-24 15:25:...|Node manager unhe...|Question: What ti...|Node manager unhe...|0.0353126219814815| Root Cause : HDIn...|   Routing Azure HDI...|  warnings in ambari|Node manager unhe...|  restarted nodemgr.|           null|\n",
      "|120022424003850|2020-02-24 19:16:...|Cluster not scali...|Question: What ti...|Cluster not scali...|  2.05221198888889| Root Cause : HDIn...|   Routing Azure HDI...|     unable to scale|        zombie nodes|\"zombie nodes cle...|           null|\n",
      "|120022421002252|2020-02-24 21:33:...|not possible to s...|can't scale up th...|not possible to s...|0.0333978233738426| Root Cause : HDIn...|   Routing Azure HDI...|Issue : Scaling f...|Cause: cluster is...|Resolution : Rest...|           null|\n",
      "|120022421002304|2020-02-24 21:58:...|Overall cluster's...|Question: What ti...|Overall cluster's...|0.0790402790034722| Root Cause : HDIn...|   Routing Azure HDI...|Slower query perf...|Customer is loadi...|Setting hive.fetc...|      180049259|\n",
      "|120022523001923|2020-02-25 13:51:...|Creation of a HDi...|Question: What ti...|Creation of a HDi...|0.0284669677627315| Root Cause : HDIn...|   Routing Azure HDI...|120022523001923 -...|Product Group con...|Use HDInsight 4.0...|      177173517|\n",
      "|120022524004405|2020-02-25 18:22:...|Creation of hdins...|Question: What ti...|Creation of hdins...|0.0232945721643519| Root Cause : HDIn...|   Routing Azure HDI...|1: User is unable...|We had two issues...|For the first iss...|           null|\n",
      "|120022523002235|2020-02-25 19:35:...|  Show Server Error |Question: What ti...|  Show Server Error | 0.053775250193287| Root Cause : HDIn...|   Routing Azure HDI...|Ambari UI throwin...|Ambari-Server status|In our troublesho...|           null|\n",
      "|120022524005081|2020-02-25 19:36:...|Erroe while creat...|Question: What ti...|Erroe while creat...|0.0476059138680556| Root Cause : HDIn...|   Routing Azure HDI...|Unable to create ...|This is a transie...|Redeployed the lc...|           null|\n",
      "|120022524005104|2020-02-25 19:39:...|Health of cluster...|Question: What ti...|Health of cluster...|  0.96969738294213| Root Cause : HDIn...|   Routing Azure HDI...|Health of cluster...|Heartbeat lost on...|Rebooted both the...|           null|\n",
      "|120022521002033|2020-02-25 22:16:...|      Updating Error|Question: What ti...|      Updating Error| 0.988517668150463| Root Cause : HDIn...|   Routing Azure HDI...|Cluster stuck in ...|\"Symptoms. The sc...|            Consolas|Liberation Mono|\n",
      "|120022624002669|2020-02-26 15:48:...|corrupted authori...|one of our data a...|corrupted authori...|0.0348703490231481| Root Cause : HDIn...|   Routing Azure HDI...|No access to the ...|A user added thei...|User followed the...|           null|\n",
      "|120022624002974|2020-02-26 16:21:...|HIVE service unab...|Question: What ti...|HIVE service unab...| 0.290624636460648| Root Cause : HDIn...|   Routing Azure HDI...|Hive services una...|Hive metastore ve...|Updated the metas...|           null|\n",
      "|120022621002303|2020-02-26 19:13:...|Resource Manager ...|Question: What ti...|Resource Manager ...|0.0100522275752315| Root Cause : HDIn...|   Routing Azure HDI...|We determined tha...|Resource Manager ...|We determined tha...|           null|\n",
      "|120022624004955|2020-02-26 20:14:...|Need to stop Amba...|Question: What ti...|Need to stop Amba...|0.0496782808171296| Root Cause : HDIn...|   Routing Azure HDI...|You had an 'Allow...|Customer wanted t...|This was an NGS i...|           null|\n",
      "|120022621002654|2020-02-26 21:30:...|         HDI version|Question: What ti...|         HDI version|0.0260345540162037| Root Cause : HDIn...|   Routing Azure HDI...|Issue:HDI Version...| Cause:          ...|Fix:HDI Product g...|      177348164|\n",
      "|120022721000135|2020-02-27 01:39:...|FD QA :  kpphv806...|Question: What ti...|FD QA :  kpphv806...|  26.5825467346944| Root Cause : HDIn...|   Routing Azure HDI...|FD QA : kpphv806l...|When The client (...| To have query hb...|      179623156|\n",
      "|120022724002711|2020-02-27 15:05:...|HDI ambari GUI we...|Question: What ti...|HDI ambari GUI we...|  0.08759238603125| Root Cause : HDIn...|   Routing Azure HDI...|HDI ambari GUI we...|HDI ambari GUI we...|Support checked a...|           null|\n",
      "|120022724002914|2020-02-27 15:26:...|Issues Containerc...|Question: What ti...|Issues Containerc...|  5.10805360234722| Root Cause : HDIn...|   Routing Azure HDI...|slow listing of f...|             unknown|created copy of f...|           null|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+------------------+---------------------+-----------------------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "csvFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 773.625,
      "end_time": 1600036764975.366
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|RootCauseSupportTopic|\n",
      "+---------------------+\n",
      "| Root Cause : HDIn...|\n",
      "| Root Cause : HDIn...|\n",
      "+---------------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sqlContext.sql(\"select distinct RootCauseSupportTopic from caseData limit 10\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5278.583984375,
      "end_time": 1600033409700.427
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk,  gensim\n",
    "import spacy\n",
    "\n",
    "#added by bp \n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1262.825927734375,
      "end_time": 1600036793947.473
     }
    }
   },
   "outputs": [],
   "source": [
    "#os.chdir(rawDatafileDir)\n",
    "\n",
    "#check directory\n",
    "#os.getcwd()\n",
    "\n",
    "#df_caseRawData= csvFile.toPandas()\n",
    "df_caseRawData= pd.read_csv(rawcaseDatafilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 245.7509765625,
      "end_time": 1600036798755.425
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>CreatedDateTime</th>\n",
       "      <th>Title</th>\n",
       "      <th>IssueDescription</th>\n",
       "      <th>Subject</th>\n",
       "      <th>DaysToSolution</th>\n",
       "      <th>RootCauseSupportTopic</th>\n",
       "      <th>InitialSupportTopicPath</th>\n",
       "      <th>Symptomstxt</th>\n",
       "      <th>Causetxt</th>\n",
       "      <th>Resolutiontxt</th>\n",
       "      <th>RelatedICM_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120022424000075</td>\n",
       "      <td>2020-02-24 01:11:13.5700473</td>\n",
       "      <td>Kerbaroes TGT error for the service account user</td>\n",
       "      <td>Question: What time did the problem begin?\\nAn...</td>\n",
       "      <td>Kerbaroes TGT error for the service account user</td>\n",
       "      <td>0.161405</td>\n",
       "      <td>Root Cause : HDInsight Service\\Azure platform ...</td>\n",
       "      <td>Routing Azure HDInsight V5\\hdfs commands do no...</td>\n",
       "      <td>VM reboots very often</td>\n",
       "      <td>The logs indicate that the VM crash is caused ...</td>\n",
       "      <td>The Vm in question is not hosted on the node t...</td>\n",
       "      <td>177045841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120022424001185</td>\n",
       "      <td>2020-02-24 13:42:48.5617557</td>\n",
       "      <td>some pipelines are failing with no applicatoin...</td>\n",
       "      <td>We're trying to determine where/why some of ou...</td>\n",
       "      <td>some pipelines are failing with no applicatoin...</td>\n",
       "      <td>2.112285</td>\n",
       "      <td>Root Cause : HDInsight Service\\Configuration\\S...</td>\n",
       "      <td>Routing Azure Data Factory V2\\Pipeline Activit...</td>\n",
       "      <td>Out of memory exceptions in the livy logs and ...</td>\n",
       "      <td>Based on the investigation done and the resolu...</td>\n",
       "      <td>For the jobs failing due to out of memory, inc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IncidentId              CreatedDateTime  \\\n",
       "0  120022424000075  2020-02-24 01:11:13.5700473   \n",
       "1  120022424001185  2020-02-24 13:42:48.5617557   \n",
       "\n",
       "                                               Title  \\\n",
       "0   Kerbaroes TGT error for the service account user   \n",
       "1  some pipelines are failing with no applicatoin...   \n",
       "\n",
       "                                    IssueDescription  \\\n",
       "0  Question: What time did the problem begin?\\nAn...   \n",
       "1  We're trying to determine where/why some of ou...   \n",
       "\n",
       "                                             Subject  DaysToSolution  \\\n",
       "0   Kerbaroes TGT error for the service account user        0.161405   \n",
       "1  some pipelines are failing with no applicatoin...        2.112285   \n",
       "\n",
       "                               RootCauseSupportTopic  \\\n",
       "0  Root Cause : HDInsight Service\\Azure platform ...   \n",
       "1  Root Cause : HDInsight Service\\Configuration\\S...   \n",
       "\n",
       "                             InitialSupportTopicPath  \\\n",
       "0  Routing Azure HDInsight V5\\hdfs commands do no...   \n",
       "1  Routing Azure Data Factory V2\\Pipeline Activit...   \n",
       "\n",
       "                                         Symptomstxt  \\\n",
       "0                              VM reboots very often   \n",
       "1  Out of memory exceptions in the livy logs and ...   \n",
       "\n",
       "                                            Causetxt  \\\n",
       "0  The logs indicate that the VM crash is caused ...   \n",
       "1  Based on the investigation done and the resolu...   \n",
       "\n",
       "                                       Resolutiontxt RelatedICM_IDs  \n",
       "0  The Vm in question is not hosted on the node t...      177045841  \n",
       "1  For the jobs failing due to out of memory, inc...            NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_caseRawData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31.805908203125,
      "end_time": 1600036802025.276
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(945, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseRawData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 244.317138671875,
      "end_time": 1600036804447.599
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        IncidentId  ... RelatedICM_IDs\n",
      "0  120022424000075  ...      177045841\n",
      "1  120022424001185  ...           None\n",
      "\n",
      "[2 rows x 12 columns]"
     ]
    }
   ],
   "source": [
    "df_caseRawData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31.619140625,
      "end_time": 1600036829988.731
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IncidentId', 'CreatedDateTime', 'Title', 'IssueDescription', 'Subject',\n",
       "       'DaysToSolution', 'RootCauseSupportTopic', 'InitialSupportTopicPath',\n",
       "       'Symptomstxt', 'Causetxt', 'Resolutiontxt', 'RelatedICM_IDs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseRawData.columns\n",
    "type(df_caseRawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.718017578125,
      "end_time": 1600039129144.12
     }
    }
   },
   "outputs": [],
   "source": [
    "df_caseData=df_caseRawData.loc[:, ['Title', 'Subject','DaysToSolution', 'InitialSupportTopicPath',\n",
    "       'Symptomstxt', 'Resolutiontxt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.97900390625,
      "end_time": 1600039131628.503
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  ...                                      Resolutiontxt\n",
      "0   Kerbaroes TGT error for the service account user  ...  The Vm in question is not hosted on the node t...\n",
      "1  some pipelines are failing with no applicatoin...  ...  For the jobs failing due to out of memory, inc...\n",
      "\n",
      "[2 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "#type(df_caseData)\n",
    "\n",
    "df_caseData.columns\n",
    "df_caseData.shape\n",
    "df_caseData.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.48291015625,
      "end_time": 1600039104312.802
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  ...                                    initCasusePath3\n",
      "0     Kerbaroes TGT error for the service account user  ...  ADLS Gen1, ADLS Gen2 in cluster with Enterpris...\n",
      "1    some pipelines are failing with no applicatoin...  ...  HDInsight (Hive, MapReduce, Pig, Spark, Stream...\n",
      "2                              Node manager unhealthy   ...                     MapReduce, Pig, Sqoop or Oozie\n",
      "3                               Cluster not scaling up  ...                              Issue with scaling up\n",
      "4                             not possible to scale up  ...                                               None\n",
      "..                                                 ...  ...                                                ...\n",
      "940             Need help with vulnerability questions  ...                                               Hive\n",
      "941    Getting list of hosts to be customized is empty  ...                                              Kafka\n",
      "942  Connection failed: 'NoneType' object has no at...  ...            Lost network connectivity between nodes\n",
      "943  MDH Distribuition:  Problem with deploying the...  ...                             Create failure - other\n",
      "944  Memory and VCores available in a Cluster for S...  ...                                              Spark\n",
      "\n",
      "[945 rows x 10 columns]"
     ]
    }
   ],
   "source": [
    "#drop old fields\n",
    "#df_caseData.drop(['Rootpath1','Rootpath2','Rootpath3'],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 237.51611328125,
      "end_time": 1600039138188.331
     }
    }
   },
   "outputs": [],
   "source": [
    "#split initial support cause column by \\ and expand to sperate columns \n",
    "\n",
    "new = df_caseData[\"InitialSupportTopicPath\"].str.split(\"\\\\\",expand = True) \n",
    "df_caseData['initCasusePath1']=new[0]\n",
    "df_caseData['initCasusePath2']=new[1]\n",
    "df_caseData['initCasusePath3']=new[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 36.923828125,
      "end_time": 1600039226082.741
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Subject', 'DaysToSolution', 'InitialSupportTopicPath',\n",
       "       'Symptomstxt', 'Resolutiontxt', 'initCasusePath1', 'initCasusePath2',\n",
       "       'initCasusePath3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.919921875,
      "end_time": 1600039293731.763
     }
    }
   },
   "outputs": [],
   "source": [
    "#rebuild df with only select fields, move root cause to the list\n",
    "\n",
    "df_caseData=df_caseData.loc[:, ['Title', 'Subject','DaysToSolution', 'initCasusePath1', 'initCasusePath2',\n",
    "       'initCasusePath3','Symptomstxt', 'Resolutiontxt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 29.501953125,
      "end_time": 1600039324895.052
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Subject', 'DaysToSolution', 'initCasusePath1',\n",
       "       'initCasusePath2', 'initCasusePath3', 'Symptomstxt', 'Resolutiontxt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData.columns\n",
    "#df_caseData.shape\n",
    "#df_caseData.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Some data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DaysToSolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>945.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.514929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.770805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.068524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.221627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.028494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.934380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DaysToSolution\n",
       "count      945.000000\n",
       "mean         3.514929\n",
       "std          9.770805\n",
       "min          0.000719\n",
       "25%          0.068524\n",
       "50%          0.221627\n",
       "75%          1.028494\n",
       "max         89.934380"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData['Title'].count()\n",
    "df_caseData['Title'].nunique()\n",
    "df_caseData.describe()\n",
    "#df_caseData['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subject</th>\n",
       "      <th>DaysToSolution</th>\n",
       "      <th>initCasusePath1</th>\n",
       "      <th>initCasusePath2</th>\n",
       "      <th>initCasusePath3</th>\n",
       "      <th>Symptomstxt</th>\n",
       "      <th>Resolutiontxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Subject, DaysToSolution, initCasusePath1, initCasusePath2, initCasusePath3, Symptomstxt, Resolutiontxt]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if title and subject are different\n",
    "df_caseData.loc[df_caseData['Title'] != df_caseData['Subject']]\n",
    "\n",
    "#Since title and SUbject are same drop one of the field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(945, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subject</th>\n",
       "      <th>DaysToSolution</th>\n",
       "      <th>initCasusePath1</th>\n",
       "      <th>initCasusePath2</th>\n",
       "      <th>initCasusePath3</th>\n",
       "      <th>Symptomstxt</th>\n",
       "      <th>Resolutiontxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kerbaroes TGT error for the service account user</td>\n",
       "      <td>Kerbaroes TGT error for the service account user</td>\n",
       "      <td>0.161405</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>hdfs commands do not work</td>\n",
       "      <td>ADLS Gen1, ADLS Gen2 in cluster with Enterpris...</td>\n",
       "      <td>VM reboots very often</td>\n",
       "      <td>The Vm in question is not hosted on the node t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some pipelines are failing with no applicatoin...</td>\n",
       "      <td>some pipelines are failing with no applicatoin...</td>\n",
       "      <td>2.112285</td>\n",
       "      <td>Routing Azure Data Factory V2</td>\n",
       "      <td>Pipeline Activities</td>\n",
       "      <td>HDInsight (Hive, MapReduce, Pig, Spark, Stream...</td>\n",
       "      <td>Out of memory exceptions in the livy logs and ...</td>\n",
       "      <td>For the jobs failing due to out of memory, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Node manager unhealthy</td>\n",
       "      <td>Node manager unhealthy</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>MapReduce, Pig, Sqoop or Oozie</td>\n",
       "      <td>warnings in ambari</td>\n",
       "      <td>restarted nodemgr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cluster not scaling up</td>\n",
       "      <td>Cluster not scaling up</td>\n",
       "      <td>2.052212</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>unable to scale</td>\n",
       "      <td>zombie nodes cleaned  Ambari server log will t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Overall cluster's performance is extremely slow</td>\n",
       "      <td>Overall cluster's performance is extremely slow</td>\n",
       "      <td>0.079040</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Performance - queries or jobs running slower t...</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Slower query performance</td>\n",
       "      <td>Setting hive.fetch.task.conversion=none (from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Creation of a HDinsight cluster creation with ...</td>\n",
       "      <td>Creation of a HDinsight cluster creation with ...</td>\n",
       "      <td>0.028467</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure with Azure Data Lake Storage Gen2</td>\n",
       "      <td>120022523001923 - Creation of an HDinsight clu...</td>\n",
       "      <td>Use HDInsight 4.0/Spark 2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creation of hdinsight with ESP failing,</td>\n",
       "      <td>Creation of hdinsight with ESP failing,</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure with Azure Data Lake Storage Gen2</td>\n",
       "      <td>1: User is unable to deploy a cluster and is g...</td>\n",
       "      <td>For the first issue: We were advised by the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Erroe while creating HDinsight cluster</td>\n",
       "      <td>Erroe while creating HDinsight cluster</td>\n",
       "      <td>0.047606</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>Unable to create Hdinsight cluster</td>\n",
       "      <td>Redeployed the lcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Health of cluster - heartbeat lost on all nodes</td>\n",
       "      <td>Health of cluster - heartbeat lost on all nodes</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Hadoop</td>\n",
       "      <td>Health of cluster - heartbeat lost on all nodes.</td>\n",
       "      <td>Rebooted both the head-nodes from the backend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Updating Error</td>\n",
       "      <td>Updating Error</td>\n",
       "      <td>0.988518</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with Autoscaling</td>\n",
       "      <td>Cluster stuck in Updating Error state when try...</td>\n",
       "      <td>To resolve this error, manually connect to eac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HIVE service unable to start</td>\n",
       "      <td>HIVE service unable to start</td>\n",
       "      <td>0.290625</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Hadoop</td>\n",
       "      <td>Hive services unable to start</td>\n",
       "      <td>Updated the metastore version to the latest ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Resource Manager Down</td>\n",
       "      <td>Resource Manager Down</td>\n",
       "      <td>0.010052</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Hadoop</td>\n",
       "      <td>We determined that the HDInsight cluster sense...</td>\n",
       "      <td>We determined that the HDInsight cluster sense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HDI version</td>\n",
       "      <td>HDI version</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Issue:HDI Version:We deployed two new spark hd...</td>\n",
       "      <td>Fix:HDI Product group has fixed the issue and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FD QA :  kpphv806llapfdqausc01 : Analyze and Q...</td>\n",
       "      <td>FD QA :  kpphv806llapfdqausc01 : Analyze and Q...</td>\n",
       "      <td>26.582547</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Interactive Query</td>\n",
       "      <td>FD QA : kpphv806llapfdqausc01 : Analyze and Qu...</td>\n",
       "      <td>To have query hbase tables from hive, they ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Issues Containercustom built</td>\n",
       "      <td>Issues Containercustom built</td>\n",
       "      <td>5.108054</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>MapReduce, Pig, Sqoop or Oozie</td>\n",
       "      <td>slow listing of files</td>\n",
       "      <td>created copy of files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yarn memory usage 100% with jupyter nodes</td>\n",
       "      <td>yarn memory usage 100% with jupyter nodes</td>\n",
       "      <td>0.228469</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Client tool issue</td>\n",
       "      <td>Notebooks</td>\n",
       "      <td>When the client launch one Jupyter notebook in...</td>\n",
       "      <td>Decrease the values for spark.executor.memory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ahd501dj pyspark fails to start with permissio...</td>\n",
       "      <td>ahd501dj pyspark fails to start with permissio...</td>\n",
       "      <td>49.121370</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Authentication failure</td>\n",
       "      <td>Ranger in cluster with Enterprise Security Pac...</td>\n",
       "      <td>120022721001340 - ahd501dj pyspark fails to st...</td>\n",
       "      <td>The issue is no longer happening. All of the u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Out of Memory Errors</td>\n",
       "      <td>Out of Memory Errors</td>\n",
       "      <td>0.980795</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Out of Memory Errors.</td>\n",
       "      <td>Need to optimize the DML hive query accordingl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[Azure Government] consent test for fairfax</td>\n",
       "      <td>[Azure Government] consent test for fairfax</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Metrics are missing</td>\n",
       "      <td>Kafka</td>\n",
       "      <td>120022724006081 -[Azure Government] consent te...</td>\n",
       "      <td>this was a test ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ProdSup:kp10tntncapllapnsprdsup01: Hive servic...</td>\n",
       "      <td>ProdSup:kp10tntncapllapnsprdsup01: Hive servic...</td>\n",
       "      <td>0.065447</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Interactive Query</td>\n",
       "      <td>ProdSup:kp10tntncapllapnsprdsup01: Hive servic...</td>\n",
       "      <td>Here are some mitigation steps that you can im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[03/02/2020][HDI]I am unable to connect even a...</td>\n",
       "      <td>[03/02/2020][HDI]I am unable to connect even a...</td>\n",
       "      <td>0.011395</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Interactive Query</td>\n",
       "      <td>Intermitient connection failures</td>\n",
       "      <td>Leverage the external metastore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Spark2 thrift server connection lost to headno...</td>\n",
       "      <td>Spark2 thrift server connection lost to headno...</td>\n",
       "      <td>0.049118</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Alerts firing on Services</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Spark2 thrift server connection lost to headno...</td>\n",
       "      <td>rebooted spark2 thrift sever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ubuntu 18.0.4 for HDInsight nodes</td>\n",
       "      <td>Ubuntu 18.0.4 for HDInsight nodes</td>\n",
       "      <td>0.086584</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>Ubuntu 18.0.4 for HDInsight nodes</td>\n",
       "      <td>After looking into the document   https://docs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PRDSUP: kp10tntncapllapnsprdsup01 : There is n...</td>\n",
       "      <td>PRDSUP: kp10tntncapllapnsprdsup01 : There is n...</td>\n",
       "      <td>0.855838</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Unable to execute any queries from Hive View a...</td>\n",
       "      <td>We do see two issues in this and below are the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Hive and MapReduce services are down from all ...</td>\n",
       "      <td>Hive and MapReduce services are down from all ...</td>\n",
       "      <td>1.175688</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Hive and MapReduce services are down from all ...</td>\n",
       "      <td>Our Engineering team mitigated the issue now a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Deployment failed for HDInsight Kafka cluster</td>\n",
       "      <td>Deployment failed for HDInsight Kafka cluster</td>\n",
       "      <td>24.921332</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>could not deploy without internet access</td>\n",
       "      <td>The only supported way to restrict outbound tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Hive Query Issue</td>\n",
       "      <td>Hive Query Issue</td>\n",
       "      <td>1.245454</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Unable to run Hive query through JDBC connnect...</td>\n",
       "      <td>You were able to solve this issue by paritioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Head nodes are in failed state.</td>\n",
       "      <td>Head nodes are in failed state.</td>\n",
       "      <td>0.118717</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>VM or Node unhealthy</td>\n",
       "      <td>Node unresponsive or sluggish</td>\n",
       "      <td>Issue: Headnodes for the kafka cluster in a ba...</td>\n",
       "      <td>Resolution : Restarted the problematic node to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Cluster doesn't start..Generic Error Message</td>\n",
       "      <td>Cluster doesn't start..Generic Error Message</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>Cluster doesn't start..Generic Error Message</td>\n",
       "      <td>I verified your Route table once again and no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HTTP Error 502.3 - Bad Gateway when checking log</td>\n",
       "      <td>HTTP Error 502.3 - Bad Gateway when checking log</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Can access to application logs</td>\n",
       "      <td>This is a known issue with an internal work it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>How to prevent HMaster process run? We found H...</td>\n",
       "      <td>How to prevent HMaster process run? We found H...</td>\n",
       "      <td>0.153467</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Spark</td>\n",
       "      <td>The other jobs are taking more time than usual</td>\n",
       "      <td>Kill the non-job related processSchedule the h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>spark history server takes too much of cpu con...</td>\n",
       "      <td>spark history server takes too much of cpu con...</td>\n",
       "      <td>0.106057</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Customer wanted the root cause and to know wha...</td>\n",
       "      <td>Sent customer this recommendation:Before killi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>User is not able to run SPARK job with Livy In...</td>\n",
       "      <td>User is not able to run SPARK job with Livy In...</td>\n",
       "      <td>0.362465</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Users are unable to run zeppelin commands</td>\n",
       "      <td>Container was deleted and restored a couple of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>FD QA - kpq101llapfdqawus201 - Node unresponsive</td>\n",
       "      <td>FD QA - kpq101llapfdqawus201 - Node unresponsive</td>\n",
       "      <td>0.038704</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>VM or Node unhealthy</td>\n",
       "      <td>Node unresponsive or sluggish</td>\n",
       "      <td>FD QA - kpq101llapfdqawus201 - Node unresponsive</td>\n",
       "      <td>Rebooted the WN25 node to mitigate the issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>[Azure Government] Jobs cannot be submitted</td>\n",
       "      <td>[Azure Government] Jobs cannot be submitted</td>\n",
       "      <td>0.694511</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Spark</td>\n",
       "      <td>120080724000334 - [Azure Government] Jobs cann...</td>\n",
       "      <td>After a hot fix was applied by the Product Gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Sandbox - kps126llaphpbiosbwus201</td>\n",
       "      <td>Sandbox - kps126llaphpbiosbwus201</td>\n",
       "      <td>0.314792</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>SecureHadoopWaitForOuContainerCreationActivity...</td>\n",
       "      <td>Delete and recreate cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>unable to create cluster</td>\n",
       "      <td>unable to create cluster</td>\n",
       "      <td>3.990067</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>unable to create cluster</td>\n",
       "      <td>Worked with customer on this and found that pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>[Azure Government] Status=---, Cannot delete c...</td>\n",
       "      <td>[Azure Government] Status=---, Cannot delete c...</td>\n",
       "      <td>0.874243</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Not able to renew adls cert. Therefore, tried ...</td>\n",
       "      <td>Restored the key, patched impacted areas and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>[Azure Government] Cant publish in kafka</td>\n",
       "      <td>[Azure Government] Cant publish in kafka</td>\n",
       "      <td>3.705879</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Kafka</td>\n",
       "      <td>Unable to publish topic</td>\n",
       "      <td>Issue with customer config</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Cannot Authenticate to YARN</td>\n",
       "      <td>Cannot Authenticate to YARN</td>\n",
       "      <td>1.316146</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Authentication failure</td>\n",
       "      <td>Ambari in cluster with Enterprise Security Pac...</td>\n",
       "      <td>cannot aauthenticate with yarn UI</td>\n",
       "      <td>asked cx to use domain credentials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Heartbeat lost, headnode down</td>\n",
       "      <td>Heartbeat lost, headnode down</td>\n",
       "      <td>2.384550</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Alerts firing on Services</td>\n",
       "      <td>Hive</td>\n",
       "      <td>Symptom: Heart beat lost and head node (hn0) w...</td>\n",
       "      <td>Restarted the headnode-0-vm-0 and started all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Unable to create Spark cluster with ADLS Gen2 ...</td>\n",
       "      <td>Unable to create Spark cluster with ADLS Gen2 ...</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure with Azure Data Lake Storage Gen2</td>\n",
       "      <td>Unable to create Spark cluster with ADLS Gen2 ...</td>\n",
       "      <td>we noticed that the Managed Identity does not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>The previous problem has reoccured, previous s...</td>\n",
       "      <td>The previous problem has reoccured, previous s...</td>\n",
       "      <td>0.688194</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Query or Job Failure</td>\n",
       "      <td>Kafka</td>\n",
       "      <td>Followed error: Uncaught error in kafka produc...</td>\n",
       "      <td>Client need to look at their producer code and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>General Question related to the HDI ESP clusters</td>\n",
       "      <td>General Question related to the HDI ESP clusters</td>\n",
       "      <td>0.670004</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Spark</td>\n",
       "      <td>General question about LDAP certificate renewal</td>\n",
       "      <td>Provided information that HDI automatically up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>0.321296</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Cluster stuck in accepting state</td>\n",
       "      <td>Resolution:The on-call engineering team has pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>NameNode Last Checkpoint</td>\n",
       "      <td>NameNode Last Checkpoint</td>\n",
       "      <td>0.113437</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Alerts firing on Services</td>\n",
       "      <td>Hbase</td>\n",
       "      <td>NameNode Last Checkpoint HDInsight Service</td>\n",
       "      <td>Log onto your HDInsight cluster:Restart Ambari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Use public IP for KAfka worker node for bootst...</td>\n",
       "      <td>Use public IP for KAfka worker node for bootst...</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Kafka</td>\n",
       "      <td>Use public IP for KAfka worker node for bootst...</td>\n",
       "      <td>I was able to find a documentation on what we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>adf sandbox - kps121sparkhpbiosbwus201 - adls ...</td>\n",
       "      <td>adf sandbox - kps121sparkhpbiosbwus201 - adls ...</td>\n",
       "      <td>0.069009</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>hdfs commands do not work</td>\n",
       "      <td>ADLS Gen1, ADLS Gen2 in standard cluster</td>\n",
       "      <td>adf sandbox - kps121sparkhpbiosbwus201 - adls ...</td>\n",
       "      <td>Checked and found that secondary ADL Gen2 acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...</td>\n",
       "      <td>hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...</td>\n",
       "      <td>0.173808</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Hbase</td>\n",
       "      <td>hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...</td>\n",
       "      <td>We help customer to reboot HN0 at 02:17 UTC 8/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Zeppelin not starting</td>\n",
       "      <td>Zeppelin not starting</td>\n",
       "      <td>0.422323</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Client tool issue</td>\n",
       "      <td>Notebooks</td>\n",
       "      <td>Zeppelin not starting</td>\n",
       "      <td>Worked with customer and engaged product group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>[Azure Government] This resource won't create ...</td>\n",
       "      <td>[Azure Government] This resource won't create ...</td>\n",
       "      <td>1.668335</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>Unable to deploy HDI cluster cluster creation ...</td>\n",
       "      <td>delete Azure Databricks workspace so we can r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>Importing snapshot</td>\n",
       "      <td>Importing snapshot</td>\n",
       "      <td>0.157529</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Client tool issue</td>\n",
       "      <td>HDInsight SDK</td>\n",
       "      <td>unable to export hbase snapshot from on-prem c...</td>\n",
       "      <td>Added the decrypted       key only to core-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>can not change the sshuser password throug Scr...</td>\n",
       "      <td>can not change the sshuser password throug Scr...</td>\n",
       "      <td>0.072846</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Authentication failure</td>\n",
       "      <td>Ambari in standard cluster</td>\n",
       "      <td>ssh user password change failed</td>\n",
       "      <td>Removed unsupported characters in the password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>After a reboot, we are unable to SSH in.  'Sys...</td>\n",
       "      <td>After a reboot, we are unable to SSH in.  'Sys...</td>\n",
       "      <td>0.051482</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>VM or Node unhealthy</td>\n",
       "      <td>Unable to SSH</td>\n",
       "      <td>Getting the error “connect to host hn1-greenh ...</td>\n",
       "      <td>Waiting for some time after the cluster rebooted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Need help with vulnerability questions</td>\n",
       "      <td>Need help with vulnerability questions</td>\n",
       "      <td>0.265836</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Hive</td>\n",
       "      <td>General question</td>\n",
       "      <td>Checked with PG team and provided suggestion t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>Getting list of hosts to be customized is empty</td>\n",
       "      <td>Getting list of hosts to be customized is empty</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Unexpected result</td>\n",
       "      <td>Kafka</td>\n",
       "      <td>Getting list of hosts to be customized is empty</td>\n",
       "      <td>Resolution: We see multiple PIDs related to Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>Connection failed: 'NoneType' object has no at...</td>\n",
       "      <td>Connection failed: 'NoneType' object has no at...</td>\n",
       "      <td>0.084342</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>VM or Node unhealthy</td>\n",
       "      <td>Lost network connectivity between nodes</td>\n",
       "      <td>Connection failed: 'NoneType' object has no at...</td>\n",
       "      <td>Resolution: As a mitigation, we restarted the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>MDH Distribuition:  Problem with deploying the...</td>\n",
       "      <td>MDH Distribuition:  Problem with deploying the...</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Create HDInsight cluster</td>\n",
       "      <td>Create failure - other</td>\n",
       "      <td>MDH Distribuition: Problem with deploying the ...</td>\n",
       "      <td>Resolution: Got the email from Ashish saying h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Memory and VCores available in a Cluster for S...</td>\n",
       "      <td>Memory and VCores available in a Cluster for S...</td>\n",
       "      <td>0.040329</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Metrics are missing</td>\n",
       "      <td>Spark</td>\n",
       "      <td>Memory and VCores available in a Cluster for S...</td>\n",
       "      <td>Resolution: Recommend to use the YARN UI to ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0     Kerbaroes TGT error for the service account user   \n",
       "1    some pipelines are failing with no applicatoin...   \n",
       "2                              Node manager unhealthy    \n",
       "3                               Cluster not scaling up   \n",
       "5      Overall cluster's performance is extremely slow   \n",
       "6    Creation of a HDinsight cluster creation with ...   \n",
       "7              Creation of hdinsight with ESP failing,   \n",
       "9               Erroe while creating HDinsight cluster   \n",
       "10     Health of cluster - heartbeat lost on all nodes   \n",
       "11                                      Updating Error   \n",
       "13                        HIVE service unable to start   \n",
       "14                               Resource Manager Down   \n",
       "16                                         HDI version   \n",
       "17   FD QA :  kpphv806llapfdqausc01 : Analyze and Q...   \n",
       "19                       Issues Containercustom built    \n",
       "20           yarn memory usage 100% with jupyter nodes   \n",
       "22   ahd501dj pyspark fails to start with permissio...   \n",
       "23                                Out of Memory Errors   \n",
       "24         [Azure Government] consent test for fairfax   \n",
       "26   ProdSup:kp10tntncapllapnsprdsup01: Hive servic...   \n",
       "27   [03/02/2020][HDI]I am unable to connect even a...   \n",
       "28   Spark2 thrift server connection lost to headno...   \n",
       "29                   Ubuntu 18.0.4 for HDInsight nodes   \n",
       "30   PRDSUP: kp10tntncapllapnsprdsup01 : There is n...   \n",
       "32   Hive and MapReduce services are down from all ...   \n",
       "33       Deployment failed for HDInsight Kafka cluster   \n",
       "34                                    Hive Query Issue   \n",
       "35                     Head nodes are in failed state.   \n",
       "36        Cluster doesn't start..Generic Error Message   \n",
       "37    HTTP Error 502.3 - Bad Gateway when checking log   \n",
       "..                                                 ...   \n",
       "911  How to prevent HMaster process run? We found H...   \n",
       "912  spark history server takes too much of cpu con...   \n",
       "913  User is not able to run SPARK job with Livy In...   \n",
       "914   FD QA - kpq101llapfdqawus201 - Node unresponsive   \n",
       "915        [Azure Government] Jobs cannot be submitted   \n",
       "916                  Sandbox - kps126llaphpbiosbwus201   \n",
       "917                           unable to create cluster   \n",
       "918  [Azure Government] Status=---, Cannot delete c...   \n",
       "920           [Azure Government] Cant publish in kafka   \n",
       "923                        Cannot Authenticate to YARN   \n",
       "924                      Heartbeat lost, headnode down   \n",
       "925  Unable to create Spark cluster with ADLS Gen2 ...   \n",
       "926  The previous problem has reoccured, previous s...   \n",
       "927   General Question related to the HDI ESP clusters   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "929  Scale up cluster failed and cluster is not in ...   \n",
       "930                           NameNode Last Checkpoint   \n",
       "931  Use public IP for KAfka worker node for bootst...   \n",
       "932  adf sandbox - kps121sparkhpbiosbwus201 - adls ...   \n",
       "933  hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...   \n",
       "934                              Zeppelin not starting   \n",
       "935  [Azure Government] This resource won't create ...   \n",
       "936                                 Importing snapshot   \n",
       "937  can not change the sshuser password throug Scr...   \n",
       "939  After a reboot, we are unable to SSH in.  'Sys...   \n",
       "940             Need help with vulnerability questions   \n",
       "941    Getting list of hosts to be customized is empty   \n",
       "942  Connection failed: 'NoneType' object has no at...   \n",
       "943  MDH Distribuition:  Problem with deploying the...   \n",
       "944  Memory and VCores available in a Cluster for S...   \n",
       "\n",
       "                                               Subject  DaysToSolution  \\\n",
       "0     Kerbaroes TGT error for the service account user        0.161405   \n",
       "1    some pipelines are failing with no applicatoin...        2.112285   \n",
       "2                              Node manager unhealthy         0.035313   \n",
       "3                               Cluster not scaling up        2.052212   \n",
       "5      Overall cluster's performance is extremely slow        0.079040   \n",
       "6    Creation of a HDinsight cluster creation with ...        0.028467   \n",
       "7              Creation of hdinsight with ESP failing,        0.023295   \n",
       "9               Erroe while creating HDinsight cluster        0.047606   \n",
       "10     Health of cluster - heartbeat lost on all nodes        0.969697   \n",
       "11                                      Updating Error        0.988518   \n",
       "13                        HIVE service unable to start        0.290625   \n",
       "14                               Resource Manager Down        0.010052   \n",
       "16                                         HDI version        0.026035   \n",
       "17   FD QA :  kpphv806llapfdqausc01 : Analyze and Q...       26.582547   \n",
       "19                       Issues Containercustom built         5.108054   \n",
       "20           yarn memory usage 100% with jupyter nodes        0.228469   \n",
       "22   ahd501dj pyspark fails to start with permissio...       49.121370   \n",
       "23                                Out of Memory Errors        0.980795   \n",
       "24         [Azure Government] consent test for fairfax        0.007482   \n",
       "26   ProdSup:kp10tntncapllapnsprdsup01: Hive servic...        0.065447   \n",
       "27   [03/02/2020][HDI]I am unable to connect even a...        0.011395   \n",
       "28   Spark2 thrift server connection lost to headno...        0.049118   \n",
       "29                   Ubuntu 18.0.4 for HDInsight nodes        0.086584   \n",
       "30   PRDSUP: kp10tntncapllapnsprdsup01 : There is n...        0.855838   \n",
       "32   Hive and MapReduce services are down from all ...        1.175688   \n",
       "33       Deployment failed for HDInsight Kafka cluster       24.921332   \n",
       "34                                    Hive Query Issue        1.245454   \n",
       "35                     Head nodes are in failed state.        0.118717   \n",
       "36        Cluster doesn't start..Generic Error Message        0.028984   \n",
       "37    HTTP Error 502.3 - Bad Gateway when checking log        0.031890   \n",
       "..                                                 ...             ...   \n",
       "911  How to prevent HMaster process run? We found H...        0.153467   \n",
       "912  spark history server takes too much of cpu con...        0.106057   \n",
       "913  User is not able to run SPARK job with Livy In...        0.362465   \n",
       "914   FD QA - kpq101llapfdqawus201 - Node unresponsive        0.038704   \n",
       "915        [Azure Government] Jobs cannot be submitted        0.694511   \n",
       "916                  Sandbox - kps126llaphpbiosbwus201        0.314792   \n",
       "917                           unable to create cluster        3.990067   \n",
       "918  [Azure Government] Status=---, Cannot delete c...        0.874243   \n",
       "920           [Azure Government] Cant publish in kafka        3.705879   \n",
       "923                        Cannot Authenticate to YARN        1.316146   \n",
       "924                      Heartbeat lost, headnode down        2.384550   \n",
       "925  Unable to create Spark cluster with ADLS Gen2 ...        0.004090   \n",
       "926  The previous problem has reoccured, previous s...        0.688194   \n",
       "927   General Question related to the HDI ESP clusters        0.670004   \n",
       "928  Scale up cluster failed and cluster is not in ...        0.276127   \n",
       "929  Scale up cluster failed and cluster is not in ...        0.321296   \n",
       "930                           NameNode Last Checkpoint        0.113437   \n",
       "931  Use public IP for KAfka worker node for bootst...        0.033386   \n",
       "932  adf sandbox - kps121sparkhpbiosbwus201 - adls ...        0.069009   \n",
       "933  hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...        0.173808   \n",
       "934                              Zeppelin not starting        0.422323   \n",
       "935  [Azure Government] This resource won't create ...        1.668335   \n",
       "936                                 Importing snapshot        0.157529   \n",
       "937  can not change the sshuser password throug Scr...        0.072846   \n",
       "939  After a reboot, we are unable to SSH in.  'Sys...        0.051482   \n",
       "940             Need help with vulnerability questions        0.265836   \n",
       "941    Getting list of hosts to be customized is empty        0.040079   \n",
       "942  Connection failed: 'NoneType' object has no at...        0.084342   \n",
       "943  MDH Distribuition:  Problem with deploying the...        0.006270   \n",
       "944  Memory and VCores available in a Cluster for S...        0.040329   \n",
       "\n",
       "                   initCasusePath1  \\\n",
       "0       Routing Azure HDInsight V5   \n",
       "1    Routing Azure Data Factory V2   \n",
       "2       Routing Azure HDInsight V5   \n",
       "3       Routing Azure HDInsight V5   \n",
       "5       Routing Azure HDInsight V5   \n",
       "6       Routing Azure HDInsight V5   \n",
       "7       Routing Azure HDInsight V5   \n",
       "9       Routing Azure HDInsight V5   \n",
       "10      Routing Azure HDInsight V5   \n",
       "11      Routing Azure HDInsight V5   \n",
       "13      Routing Azure HDInsight V5   \n",
       "14      Routing Azure HDInsight V5   \n",
       "16      Routing Azure HDInsight V5   \n",
       "17      Routing Azure HDInsight V5   \n",
       "19      Routing Azure HDInsight V5   \n",
       "20      Routing Azure HDInsight V5   \n",
       "22      Routing Azure HDInsight V5   \n",
       "23      Routing Azure HDInsight V5   \n",
       "24      Routing Azure HDInsight V5   \n",
       "26      Routing Azure HDInsight V5   \n",
       "27      Routing Azure HDInsight V5   \n",
       "28      Routing Azure HDInsight V5   \n",
       "29      Routing Azure HDInsight V5   \n",
       "30      Routing Azure HDInsight V5   \n",
       "32      Routing Azure HDInsight V5   \n",
       "33      Routing Azure HDInsight V5   \n",
       "34      Routing Azure HDInsight V5   \n",
       "35      Routing Azure HDInsight V5   \n",
       "36      Routing Azure HDInsight V5   \n",
       "37      Routing Azure HDInsight V5   \n",
       "..                             ...   \n",
       "911     Routing Azure HDInsight V5   \n",
       "912     Routing Azure HDInsight V5   \n",
       "913     Routing Azure HDInsight V5   \n",
       "914     Routing Azure HDInsight V5   \n",
       "915     Routing Azure HDInsight V5   \n",
       "916     Routing Azure HDInsight V5   \n",
       "917     Routing Azure HDInsight V5   \n",
       "918     Routing Azure HDInsight V5   \n",
       "920     Routing Azure HDInsight V5   \n",
       "923     Routing Azure HDInsight V5   \n",
       "924     Routing Azure HDInsight V5   \n",
       "925     Routing Azure HDInsight V5   \n",
       "926     Routing Azure HDInsight V5   \n",
       "927     Routing Azure HDInsight V5   \n",
       "928     Routing Azure HDInsight V5   \n",
       "929     Routing Azure HDInsight V5   \n",
       "930     Routing Azure HDInsight V5   \n",
       "931     Routing Azure HDInsight V5   \n",
       "932     Routing Azure HDInsight V5   \n",
       "933     Routing Azure HDInsight V5   \n",
       "934     Routing Azure HDInsight V5   \n",
       "935     Routing Azure HDInsight V5   \n",
       "936     Routing Azure HDInsight V5   \n",
       "937     Routing Azure HDInsight V5   \n",
       "939     Routing Azure HDInsight V5   \n",
       "940     Routing Azure HDInsight V5   \n",
       "941     Routing Azure HDInsight V5   \n",
       "942     Routing Azure HDInsight V5   \n",
       "943     Routing Azure HDInsight V5   \n",
       "944     Routing Azure HDInsight V5   \n",
       "\n",
       "                                       initCasusePath2  \\\n",
       "0                            hdfs commands do not work   \n",
       "1                                  Pipeline Activities   \n",
       "2                                    Unexpected result   \n",
       "3                              Scale HDInsight cluster   \n",
       "5    Performance - queries or jobs running slower t...   \n",
       "6                             Create HDInsight cluster   \n",
       "7                             Create HDInsight cluster   \n",
       "9                             Create HDInsight cluster   \n",
       "10                                   Service unhealthy   \n",
       "11                             Scale HDInsight cluster   \n",
       "13                                   Service unhealthy   \n",
       "14                                   Service unhealthy   \n",
       "16                                   Unexpected result   \n",
       "17                                Query or Job Failure   \n",
       "19                                   Unexpected result   \n",
       "20                                   Client tool issue   \n",
       "22                              Authentication failure   \n",
       "23                                Query or Job Failure   \n",
       "24                                 Metrics are missing   \n",
       "26                                   Service unhealthy   \n",
       "27                                   Service unhealthy   \n",
       "28                           Alerts firing on Services   \n",
       "29                            Create HDInsight cluster   \n",
       "30                                Query or Job Failure   \n",
       "32                                   Unexpected result   \n",
       "33                            Create HDInsight cluster   \n",
       "34                                Query or Job Failure   \n",
       "35                                VM or Node unhealthy   \n",
       "36                            Create HDInsight cluster   \n",
       "37                                   Service unhealthy   \n",
       "..                                                 ...   \n",
       "911                                  Service unhealthy   \n",
       "912                                  Service unhealthy   \n",
       "913                               Query or Job Failure   \n",
       "914                               VM or Node unhealthy   \n",
       "915                                  Service unhealthy   \n",
       "916                           Create HDInsight cluster   \n",
       "917                           Create HDInsight cluster   \n",
       "918                                  Service unhealthy   \n",
       "920                                  Service unhealthy   \n",
       "923                             Authentication failure   \n",
       "924                          Alerts firing on Services   \n",
       "925                           Create HDInsight cluster   \n",
       "926                               Query or Job Failure   \n",
       "927                                  Unexpected result   \n",
       "928                            Scale HDInsight cluster   \n",
       "929                            Scale HDInsight cluster   \n",
       "930                          Alerts firing on Services   \n",
       "931                                  Unexpected result   \n",
       "932                          hdfs commands do not work   \n",
       "933                                  Service unhealthy   \n",
       "934                                  Client tool issue   \n",
       "935                           Create HDInsight cluster   \n",
       "936                                  Client tool issue   \n",
       "937                             Authentication failure   \n",
       "939                               VM or Node unhealthy   \n",
       "940                                  Unexpected result   \n",
       "941                                  Unexpected result   \n",
       "942                               VM or Node unhealthy   \n",
       "943                           Create HDInsight cluster   \n",
       "944                                Metrics are missing   \n",
       "\n",
       "                                       initCasusePath3  \\\n",
       "0    ADLS Gen1, ADLS Gen2 in cluster with Enterpris...   \n",
       "1    HDInsight (Hive, MapReduce, Pig, Spark, Stream...   \n",
       "2                       MapReduce, Pig, Sqoop or Oozie   \n",
       "3                                Issue with scaling up   \n",
       "5                                                 Hive   \n",
       "6     Create failure with Azure Data Lake Storage Gen2   \n",
       "7     Create failure with Azure Data Lake Storage Gen2   \n",
       "9                               Create failure - other   \n",
       "10                                              Hadoop   \n",
       "11                              Issue with Autoscaling   \n",
       "13                                              Hadoop   \n",
       "14                                              Hadoop   \n",
       "16                                               Spark   \n",
       "17                                   Interactive Query   \n",
       "19                      MapReduce, Pig, Sqoop or Oozie   \n",
       "20                                           Notebooks   \n",
       "22   Ranger in cluster with Enterprise Security Pac...   \n",
       "23                                                Hive   \n",
       "24                                               Kafka   \n",
       "26                                   Interactive Query   \n",
       "27                                   Interactive Query   \n",
       "28                                               Spark   \n",
       "29                              Create failure - other   \n",
       "30                                                Hive   \n",
       "32                                                Hive   \n",
       "33                              Create failure - other   \n",
       "34                                                Hive   \n",
       "35                       Node unresponsive or sluggish   \n",
       "36                              Create failure - other   \n",
       "37                                               Spark   \n",
       "..                                                 ...   \n",
       "911                                              Spark   \n",
       "912                                              Spark   \n",
       "913                                              Spark   \n",
       "914                      Node unresponsive or sluggish   \n",
       "915                                              Spark   \n",
       "916                             Create failure - other   \n",
       "917                             Create failure - other   \n",
       "918                                              Spark   \n",
       "920                                              Kafka   \n",
       "923  Ambari in cluster with Enterprise Security Pac...   \n",
       "924                                               Hive   \n",
       "925   Create failure with Azure Data Lake Storage Gen2   \n",
       "926                                              Kafka   \n",
       "927                                              Spark   \n",
       "928                              Issue with scaling up   \n",
       "929                              Issue with scaling up   \n",
       "930                                              Hbase   \n",
       "931                                              Kafka   \n",
       "932           ADLS Gen1, ADLS Gen2 in standard cluster   \n",
       "933                                              Hbase   \n",
       "934                                          Notebooks   \n",
       "935                             Create failure - other   \n",
       "936                                      HDInsight SDK   \n",
       "937                         Ambari in standard cluster   \n",
       "939                                      Unable to SSH   \n",
       "940                                               Hive   \n",
       "941                                              Kafka   \n",
       "942            Lost network connectivity between nodes   \n",
       "943                             Create failure - other   \n",
       "944                                              Spark   \n",
       "\n",
       "                                           Symptomstxt  \\\n",
       "0                                VM reboots very often   \n",
       "1    Out of memory exceptions in the livy logs and ...   \n",
       "2                                   warnings in ambari   \n",
       "3                                      unable to scale   \n",
       "5                             Slower query performance   \n",
       "6    120022523001923 - Creation of an HDinsight clu...   \n",
       "7    1: User is unable to deploy a cluster and is g...   \n",
       "9                   Unable to create Hdinsight cluster   \n",
       "10    Health of cluster - heartbeat lost on all nodes.   \n",
       "11   Cluster stuck in Updating Error state when try...   \n",
       "13                       Hive services unable to start   \n",
       "14   We determined that the HDInsight cluster sense...   \n",
       "16   Issue:HDI Version:We deployed two new spark hd...   \n",
       "17   FD QA : kpphv806llapfdqausc01 : Analyze and Qu...   \n",
       "19                               slow listing of files   \n",
       "20   When the client launch one Jupyter notebook in...   \n",
       "22   120022721001340 - ahd501dj pyspark fails to st...   \n",
       "23                               Out of Memory Errors.   \n",
       "24   120022724006081 -[Azure Government] consent te...   \n",
       "26   ProdSup:kp10tntncapllapnsprdsup01: Hive servic...   \n",
       "27                    Intermitient connection failures   \n",
       "28   Spark2 thrift server connection lost to headno...   \n",
       "29                   Ubuntu 18.0.4 for HDInsight nodes   \n",
       "30   Unable to execute any queries from Hive View a...   \n",
       "32   Hive and MapReduce services are down from all ...   \n",
       "33            could not deploy without internet access   \n",
       "34   Unable to run Hive query through JDBC connnect...   \n",
       "35   Issue: Headnodes for the kafka cluster in a ba...   \n",
       "36        Cluster doesn't start..Generic Error Message   \n",
       "37                      Can access to application logs   \n",
       "..                                                 ...   \n",
       "911    The other jobs are taking more time than usual    \n",
       "912  Customer wanted the root cause and to know wha...   \n",
       "913          Users are unable to run zeppelin commands   \n",
       "914   FD QA - kpq101llapfdqawus201 - Node unresponsive   \n",
       "915  120080724000334 - [Azure Government] Jobs cann...   \n",
       "916  SecureHadoopWaitForOuContainerCreationActivity...   \n",
       "917                           unable to create cluster   \n",
       "918  Not able to renew adls cert. Therefore, tried ...   \n",
       "920                            Unable to publish topic   \n",
       "923                  cannot aauthenticate with yarn UI   \n",
       "924  Symptom: Heart beat lost and head node (hn0) w...   \n",
       "925  Unable to create Spark cluster with ADLS Gen2 ...   \n",
       "926  Followed error: Uncaught error in kafka produc...   \n",
       "927    General question about LDAP certificate renewal   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "929                  Cluster stuck in accepting state    \n",
       "930         NameNode Last Checkpoint HDInsight Service   \n",
       "931  Use public IP for KAfka worker node for bootst...   \n",
       "932  adf sandbox - kps121sparkhpbiosbwus201 - adls ...   \n",
       "933  hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...   \n",
       "934                              Zeppelin not starting   \n",
       "935  Unable to deploy HDI cluster cluster creation ...   \n",
       "936  unable to export hbase snapshot from on-prem c...   \n",
       "937                    ssh user password change failed   \n",
       "939  Getting the error “connect to host hn1-greenh ...   \n",
       "940                                   General question   \n",
       "941    Getting list of hosts to be customized is empty   \n",
       "942  Connection failed: 'NoneType' object has no at...   \n",
       "943  MDH Distribuition: Problem with deploying the ...   \n",
       "944  Memory and VCores available in a Cluster for S...   \n",
       "\n",
       "                                         Resolutiontxt  \n",
       "0    The Vm in question is not hosted on the node t...  \n",
       "1    For the jobs failing due to out of memory, inc...  \n",
       "2                                   restarted nodemgr.  \n",
       "3    zombie nodes cleaned  Ambari server log will t...  \n",
       "5    Setting hive.fetch.task.conversion=none (from ...  \n",
       "6                         Use HDInsight 4.0/Spark 2.4   \n",
       "7    For the first issue: We were advised by the pr...  \n",
       "9                              Redeployed the lcluster  \n",
       "10   Rebooted both the head-nodes from the backend ...  \n",
       "11   To resolve this error, manually connect to eac...  \n",
       "13   Updated the metastore version to the latest ma...  \n",
       "14   We determined that the HDInsight cluster sense...  \n",
       "16   Fix:HDI Product group has fixed the issue and ...  \n",
       "17    To have query hbase tables from hive, they ne...  \n",
       "19                               created copy of files  \n",
       "20   Decrease the values for spark.executor.memory ...  \n",
       "22   The issue is no longer happening. All of the u...  \n",
       "23   Need to optimize the DML hive query accordingl...  \n",
       "24                              this was a test ticket  \n",
       "26   Here are some mitigation steps that you can im...  \n",
       "27                     Leverage the external metastore  \n",
       "28                        rebooted spark2 thrift sever  \n",
       "29   After looking into the document   https://docs...  \n",
       "30   We do see two issues in this and below are the...  \n",
       "32   Our Engineering team mitigated the issue now a...  \n",
       "33   The only supported way to restrict outbound tr...  \n",
       "34   You were able to solve this issue by paritioni...  \n",
       "35   Resolution : Restarted the problematic node to...  \n",
       "36    I verified your Route table once again and no...  \n",
       "37   This is a known issue with an internal work it...  \n",
       "..                                                 ...  \n",
       "911  Kill the non-job related processSchedule the h...  \n",
       "912  Sent customer this recommendation:Before killi...  \n",
       "913  Container was deleted and restored a couple of...  \n",
       "914       Rebooted the WN25 node to mitigate the issue  \n",
       "915  After a hot fix was applied by the Product Gro...  \n",
       "916                        Delete and recreate cluster  \n",
       "917  Worked with customer on this and found that pe...  \n",
       "918  Restored the key, patched impacted areas and r...  \n",
       "920                         Issue with customer config  \n",
       "923                 asked cx to use domain credentials  \n",
       "924  Restarted the headnode-0-vm-0 and started all ...  \n",
       "925  we noticed that the Managed Identity does not ...  \n",
       "926  Client need to look at their producer code and...  \n",
       "927  Provided information that HDI automatically up...  \n",
       "928       System backup and run with scale-up 36 nodes  \n",
       "929  Resolution:The on-call engineering team has pu...  \n",
       "930  Log onto your HDInsight cluster:Restart Ambari...  \n",
       "931  I was able to find a documentation on what we ...  \n",
       "932  Checked and found that secondary ADL Gen2 acco...  \n",
       "933   We help customer to reboot HN0 at 02:17 UTC 8/12  \n",
       "934  Worked with customer and engaged product group...  \n",
       "935   delete Azure Databricks workspace so we can r...  \n",
       "936    Added the decrypted       key only to core-s...  \n",
       "937     Removed unsupported characters in the password  \n",
       "939   Waiting for some time after the cluster rebooted  \n",
       "940  Checked with PG team and provided suggestion t...  \n",
       "941  Resolution: We see multiple PIDs related to Am...  \n",
       "942  Resolution: As a mitigation, we restarted the ...  \n",
       "943  Resolution: Got the email from Ashish saying h...  \n",
       "944  Resolution: Recommend to use the YARN UI to ch...  \n",
       "\n",
       "[827 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(945, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing values\n",
    "df_caseData.shape\n",
    "#drop any row that has missing value. \n",
    "df_caseData.dropna()\n",
    "#df_caseData.head(2)\n",
    "df_caseData.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28064733e48>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGwZJREFUeJzt3X+UlnWd//HnixkQMBTFgVhGRVoOSqIjTkbrr0XcTakVVmXzR0gkoEWhWWez2pM/ztrRToma33RnY/0iuoo/MNiNbxuibXhMcQTEFIuJCCZIJtJRA0X0/f3j/gwOcDFz8+Oae+B+Pc6Zc1/X5/5c1/2emwteXJ/rlyICMzOzHXUpdQFmZtY5OSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDJVlrqAvXHEEUfEwIEDS12Gmdl+5fnnn/9TRFS112+/DoiBAwdSX19f6jLMzPYrkn5fTD8PMZmZWSYHhJmZZXJAmJlZpv36GISZ5efdd9+lsbGRt99+u9Sl2B7q3r071dXVdO3adY+Wd0CYWabGxkZ69erFwIEDkVTqcmw3RQQbN26ksbGRY445Zo/W4SEmM8v09ttv06dPH4fDfkoSffr02as9QAeEme2Sw2H/trd/fg4IMzPL5GMQZlaUgdf+ZJ+ub/XNn2q3T0VFBcOGDePdd9+lsrKSCRMmcPXVV9Oly97/33bjxo2MGjUKgD/+8Y9UVFRQVVW4uHjx4sV069Ytc7kbb7yR2bNnU1FRQUVFBXV1dXzsYx/b5eecdtpp3HnnndTU1Oyyz5w5cxg6dCjHHnssAN/61rc4++yzGTly5J7+evtE2QbEvt7Yd0cxfzHMDHr06MGyZcsA2LBhA5dccgnNzc3ccMMNe73uPn36bFv39ddfz4c+9CG+9rWvtbnMokWL+NnPfsbSpUvp1q0bTU1NbN26da9rmTNnDl26dNkWEDfddNNer3Nf8BCTme0X+vbtS11dHXfeeScRwerVqzn99NMZPnw4w4cP5+mnnwZg/PjxzJ07d9tyl156KfPmzeOll17ilFNOoaamhhNOOIGVK1e2+Xnf/e53Of744zn++OP5wQ9+AMD69eupqqratndRVVVF//79AViwYAE1NTUMGzaMyZMns2XLlu3Wt3XrVnr37r1t/sEHH2TSpEksWrSI+fPn85WvfIWamhpWr17NZz/7WX784x+3ud7q6mquv/56TjrpJE444QR+85vf7M3Xm8kBYWb7jUGDBvH++++zYcMG+vbty4IFC1iyZAmzZ89m2rRpAEyaNIl77rkHgObmZp5++mlGjx7N3XffzVVXXcWyZcuor6+nurp6l5+zePFi7r//fhYvXswvf/lLfvjDH7J8+XLOOeccfvvb3zJkyBCmTp3KokWLANi0aROf//znefTRR3nxxRfZtGkTdXV1Rf1Op59+OqNHj2b69OksW7aM1jcgbW+9/fr1Y+nSpUyaNIlbb711d7/OdjkgzGy/EhFA4UK+yZMnM2zYMMaNG8fLL78MwJlnnklDQwMbNmzggQce4IILLqCyspJPfOITfOc73+GWW27h97//PT169NjlZyxatIgLLriAnj170qtXL8aOHctTTz3FIYccwpIlS7j77rvp06cPF154IbNmzWLFihUMHjyYj3zkIwBcdtll/OIXv9jr37W99Z5//vkAnHzyyaxevXqvP29HDggz22+sWrWKiooK+vbty/Tp0+nXrx8vvPAC9fX12w3pjB8/nvvvv5977rmHiRMnAnDJJZcwb948evTowSc/+UmeeOKJXX5OSwhlqaysZOTIkdx4443cfvvtzJkzp83+Lbp06bJdv2KuT2hvvQcddBBQOJi/L46F7MgBYWb7haamJq688kq+9KUvIYnm5mb69+9Ply5dmDVrFu+99962vp/73Oe47bbbAPjoRz8KFMJl0KBBTJs2jfPOO4/ly5fv8rPOOOMMHnvsMTZv3sxbb73F3LlzOf3001mxYgUNDQ3b+r3wwgscffTRDB06lJUrV7Jq1SoA7rvvPs4888zt1tmlSxcOO+wwVq5cyfvvv89jjz227b1evXrx5ptv7lRHMevNU65nMUn6CjAJCOBFYCLQH3gQOBxYAoyPiC2SDgLuBU4GNgKfiYjVedZnZsUrxdl3mzdvpqamZttpruPHj+eaa64B4Itf/CIXXHABDz/8MCNHjuTggw/etly/fv047rjjGDt27La22bNnc99999G1a1c+/OEP8+1vf3uXn3vKKadw8cUXbzt99Qtf+ALDhg3jueeeY9q0aTQ3N1NRUcGQIUOoq6ujZ8+ezJgxg/PPP5/33nuPj3/840yePHmn9d5yyy2cc845HHXUUQwdOpR33nkHgIsvvpgrrriC73//+9sOTgNFrzcvKmbXaI9WLA0AngKGRsRmSQ8B84HRwJyIeFDS3cALEXGXpC8CJ0TElZIuAv4xIj7T1mfU1tbGnj4wyKe5mrVtxYoVHHfccaUuY49s2rSJYcOGsWTJEg499NBSl1NSWX+Okp6PiNr2ls17iKkS6CGpEugJrAfOAh5J788EWiJ+TJonvT9Kvs7fzHbT448/zrHHHsuXv/zlsg+HvZXbEFNE/EHS94A1wGbgZ8DzwOsR0XI0pREYkKYHAGvTslslNQN9gD/lVaOZHXjOPvts1qxZU+oyDgi57UFIOozCXsExwF8BBwPnZnRtGePK2lvYafxL0hRJ9ZLqm5qa9lW5ZpYhryFo6xh7++eX5xDT2cDvIqIpIt4F5gB/A/ROQ04A1cC6NN0IHAmQ3j8U+POOK42IuoiojYjalvummNm+1717dzZu3OiQ2E+1PA+ie/fue7yOPM9iWgOMkNSTwhDTKKAeeBK4kMKZTBOAlmvi56X5X6b3nwhvmWYlU11dTWNjI95T33+1PFFuT+V5DOJZSY9QOJV1K7AUqAN+Ajwo6V9T24y0yAxglqQGCnsOF+VVm5m1r2vXrnv8JDI7MOR6HUREXAdct0PzKuCUjL5vA+PyrMfMzIrnK6nNzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMuUWEJKGSFrW6ucNSVdLOlzSAkkr0+thqb8k3SGpQdJyScPzqs3MzNqXW0BExK8joiYiaoCTgU3AY8C1wMKIGAwsTPMA5wKD088U4K68ajMzs/Z11BDTKOC3EfF7YAwwM7XPBMam6THAvVHwDNBbUv8Oqs/MzHbQUQFxEfBAmu4XEesB0mvf1D4AWNtqmcbUZmZmJZB7QEjqBpwHPNxe14y2yFjfFEn1kuqbmpr2RYlmZpahI/YgzgWWRMSraf7VlqGj9LohtTcCR7ZarhpYt+PKIqIuImojoraqqirHss3MyltHBMTFfDC8BDAPmJCmJwBzW7Vfls5mGgE0twxFmZlZx6vMc+WSegJ/B1zRqvlm4CFJlwNrgHGpfT4wGmigcMbTxDxrMzOztuUaEBGxCeizQ9tGCmc17dg3gKl51mNmZsXzldRmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmXINCEm9JT0i6RVJKyR9QtLhkhZIWpleD0t9JekOSQ2SlksanmdtZmbWtrz3IG4HfhoRxwInAiuAa4GFETEYWJjmAc4FBqefKcBdOddmZmZtyC0gJB0CnAHMAIiILRHxOjAGmJm6zQTGpukxwL1R8AzQW1L/vOozM7O25bkHMQhoAu6RtFTSjyQdDPSLiPUA6bVv6j8AWNtq+cbUth1JUyTVS6pvamrKsXwzs/KWZ0BUAsOBuyLiJOAvfDCclEUZbbFTQ0RdRNRGRG1VVdW+qdTMzHaSZ0A0Ao0R8Wyaf4RCYLzaMnSUXje06n9kq+WrgXU51mdmZm3ILSAi4o/AWklDUtMo4GVgHjAhtU0A5qbpecBl6WymEUBzy1CUmZl1vMqc1/9l4H5J3YBVwEQKofSQpMuBNcC41Hc+MBpoADalvmZmViK5BkRELANqM94aldE3gKl51mNmZsXzldRmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlqmogJB0fN6FmJlZ51LsHsTdkhZL+qKk3rlWZGZmnUJRARERpwGXUrgdd72k/5T0d7lWZmZmJVX0MYiIWAn8C/B14EzgDkmvSDo/r+LMzKx0ij0GcYKk6cAK4CzgHyLiuDQ9Pcf6zMysRIq93fedwL8D34yIzS2NEbFO0r/kUpmZmZVUsQExGtgcEe8BSOoCdI+ITRExa1cLSVoNvAm8B2yNiFpJhwOzgYHAauCfIuI1SQJuT5+1CfhcRCzZo9/KzMz2WrHHIB4HerSa75naijEyImoiouXBQdcCCyNiMLAwzQOcCwxOP1OAu4pcv5mZ5aDYgOgeEW+1zKTpnnv4mWOAmWl6JjC2Vfu9UfAM0FtS/z38DDMz20vFBsRfJA1vmZF0MrC5jf4tAviZpOclTUlt/SJiPUB67ZvaBwBrWy3bmNrMzKwEij0GcTXwsKR1ab4/8Jkiljs1HcjuCyyQ9EobfZXRFjt1KgTNFICjjjqqiBLMzGxPFBUQEfGcpGOBIRT+IX8lIt4tYrl16XWDpMeAU4BXJfWPiPVpCGlD6t5I4UK8FtXAOnYQEXVAHUBtbe1OAWJmZvvG7tys72PACcBJwMWSLmurs6SDJfVqmQb+HvgVMA+YkLpNAOam6XnAZSoYATS3DEWZmVnHK2oPQtIs4CPAMgqnrEJh+OfeNhbrBzxWOHuVSuA/I+Knkp4DHpJ0ObAGGJf6z6dwimsDhdNcJ+7er2JmZvtSsccgaoGhEVH0kE5ErAJOzGjfCIzKaA9garHrNzOzfBU7xPQr4MN5FmJmZp1LsXsQRwAvS1oMvNPSGBHn5VKVmZmVXLEBcX2eRZiZWedT7Gmu/yvpaGBwRDwuqSdQkW9pZmZWSsXe7nsy8Ajwb6lpAPDjvIoyM7PSK/Yg9VTgVOAN2PbwoL5tLmFmZvu1YgPinYjY0jIjqZKM22CYmdmBo9iA+F9J3wR6pGdRPwz8V35lmZlZqRUbENcCTcCLwBUUrnr2k+TMzA5gxZ7F9D6FR47+e77lmJlZZ1HsvZh+R8Yxh4gYtM8rMjOzTmF37sXUojuFG+wdvu/LMTOzzqKoYxARsbHVzx8i4jbgrJxrMzOzEip2iGl4q9kuFPYoeuVSkZmZdQrFDjF9v9X0VmA18E/7vBozM+s0ij2LaWTehZiZWedS7BDTNW29HxG3trFsBVAP/CEiPi3pGOBBCge5lwDjI2KLpIMoPKHuZGAj8JmIWF3Ub2FmZvtcsRfK1QJfoHCTvgHAlcBQCsch2jsWcRWwotX8LcD0iBgMvAZcntovB16LiL8Gpqd+ZmZWIsUGxBHA8Ij4akR8lcL/8qsj4oaIuGFXC0mqBj4F/CjNi8LZT4+kLjOBsWl6TJonvT8q9TczsxIoNiCOAra0mt8CDCxiuduAfwbeT/N9gNcjYmuab6SwR0J6XQuQ3m9O/bcjaYqkekn1TU1NRZZvZma7q9iAmAUslnS9pOuAZykcL9glSZ8GNkTE862bM7pGEe990BBRFxG1EVFbVVVVXPVmZrbbij2L6SZJ/w84PTVNjIil7Sx2KnCepNEUrr4+hMIeRW9JlWkvoRpYl/o3AkcCjel24ocCf96t38bMzPaZYvcgAHoCb0TE7RT+ET+mrc4R8Y2IqI6IgcBFwBMRcSnwJHBh6jYBmJum56V50vtPRISfOWFmViLFPnL0OuDrwDdSU1fgvj38zK8D10hqoHCMYUZqnwH0Se3XULjFuJmZlUixV1L/I3AShesWiIh1koq+1UZE/Bz4eZpeBZyS0edtCjcBNDOzTqDYIaYtabgnACQdnF9JZmbWGRQbEA9J+jcKB5gnA4/jhweZmR3Qij2L6XvpWdRvAEOAb0fEglwrMzOzkmo3INK9lP4nIs4GHApmZmWi3SGmiHgP2CTp0A6ox8zMOoliz2J6G3hR0gLgLy2NETEtl6rMzKzkig2In6QfMzMrE20GhKSjImJNRMxsq5+ZmR142jsG8eOWCUmP5lyLmZl1Iu0FROs7rA7KsxAzM+tc2guI2MW0mZkd4No7SH2ipDco7En0SNOk+YiIQ3KtzszMSqbNgIiIio4qxMzMOpfdeR6EmZmVEQeEmZllyi0gJHWXtFjSC5JeknRDaj9G0rOSVkqaLalbaj8ozTek9wfmVZuZmbUvzz2Id4CzIuJEoAY4R9II4BZgekQMBl4DLk/9Lwdei4i/BqanfmZmViK5BUQUvJVmu6afAM4CHkntM4GxaXpMmie9P0pS6+swzMysA+V6DEJShaRlwAYKtwr/LfB6RGxNXRqBAWl6ALAWIL3fTOGZ1WZmVgK5BkREvBcRNUA1hedQH5fVLb1m7S3sdHGepCmS6iXVNzU17btizcxsOx1yFlNEvA78HBhB4bGlLddfVAPr0nQjcCRAev9Q4M8Z66qLiNqIqK2qqsq7dDOzspXnWUxVknqn6R7A2cAK4EngwtRtAjA3Tc9L86T3n4gI397DzKxEin0exJ7oD8xMjyztAjwUEf8t6WXgQUn/CiwFZqT+M4BZkhoo7DlclGNtZmbWjtwCIiKWAydltK+icDxix/a3gXF51WNmZrvHV1KbmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZcrzkaNHSnpS0gpJL0m6KrUfLmmBpJXp9bDULkl3SGqQtFzS8LxqMzOz9uW5B7EV+GpEHAeMAKZKGgpcCyyMiMHAwjQPcC4wOP1MAe7KsTYzM2tHbgEREesjYkmafhNYAQwAxgAzU7eZwNg0PQa4NwqeAXpL6p9XfWZm1rYOOQYhaSCF51M/C/SLiPVQCBGgb+o2AFjbarHG1GZmZiWQe0BI+hDwKHB1RLzRVteMtshY3xRJ9ZLqm5qa9lWZZma2g1wDQlJXCuFwf0TMSc2vtgwdpdcNqb0ROLLV4tXAuh3XGRF1EVEbEbVVVVX5FW9mVubyPItJwAxgRUTc2uqtecCEND0BmNuq/bJ0NtMIoLllKMrMzDpeZY7rPhUYD7woaVlq+yZwM/CQpMuBNcC49N58YDTQAGwCJuZYm5mZtSO3gIiIp8g+rgAwKqN/AFPzqsfMzHaPr6Q2M7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy5TnI0f/Q9IGSb9q1Xa4pAWSVqbXw1K7JN0hqUHScknD86rLzMyKk+cexP8Fztmh7VpgYUQMBhameYBzgcHpZwpwV451mZlZEXILiIj4BfDnHZrHADPT9ExgbKv2e6PgGaC3pP551WZmZu3r6GMQ/SJiPUB67ZvaBwBrW/VrTG07kTRFUr2k+qamplyLNTMrZ53lILUy2iKrY0TURURtRNRWVVXlXJaZWfnq6IB4tWXoKL1uSO2NwJGt+lUD6zq4NjMza6WjA2IeMCFNTwDmtmq/LJ3NNAJobhmKMjOz0qjMa8WSHgD+FjhCUiNwHXAz8JCky4E1wLjUfT4wGmgANgET86rLzMyKk1tARMTFu3hrVEbfAKbmVYuZme2+znKQ2szMOhkHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllyu1WG7ZrA6/9SUk+d/XNnyrJ55rZ/sl7EGZmlsl7EGWkVHsu4L0Xs/2R9yDMzCyTA8LMzDJ1qiEmSecAtwMVwI8i4uYSl2T7iA/Mm+1/Os0ehKQK4P8A5wJDgYslDS1tVWZm5asz7UGcAjRExCoASQ8CY4CXS1qV7de852K25zpTQAwA1raabwQ+XqJazPZKKc8YK0elCuQD/czAzhQQymiLnTpJU4ApafYtSb/ew887AvjTHi57IPL3sT1/Hx/o9N+FbunQj+sU38de/s5HF9OpMwVEI3Bkq/lqYN2OnSKiDqjb2w+TVB8RtXu7ngOFv4/t+fv4gL+L7ZXT99FpDlIDzwGDJR0jqRtwETCvxDWZmZWtTrMHERFbJX0J+B8Kp7n+R0S8VOKyzMzKVqcJCICImA/M76CP2+thqgOMv4/t+fv4gL+L7ZXN96GInY4Dm5mZdapjEGZm1omUZUBIOkfSryU1SLq21PV0JElHSnpS0gpJL0m6KrUfLmmBpJXp9bBS19qRJFVIWirpv9P8MZKeTd/H7HTiRFmQ1FvSI5JeSdvJJ8p1+5D0lfT35FeSHpDUvZy2jbILCN/Sg63AVyPiOGAEMDX9/tcCCyNiMLAwzZeTq4AVreZvAaan7+M14PKSVFUatwM/jYhjgRMpfC9lt31IGgBMA2oj4ngKJ89cRBltG2UXELS6pUdEbAFabulRFiJifUQsSdNvUvjLP4DCdzAzdZsJjC1NhR1PUjXwKeBHaV7AWcAjqUvZfB+SDgHOAGYARMSWiHid8t0+KoEekiqBnsB6ymjbKMeAyLqlx4AS1VJSkgYCJwHPAv0iYj0UQgToW7rKOtxtwD8D76f5PsDrEbE1zZfTNjIIaALuSUNuP5J0MGW4fUTEH4DvAWsoBEMz8DxltG2UY0AUdUuPA52kDwGPAldHxBulrqdUJH0a2BARz7duzuhaLttIJTAcuCsiTgL+QhkMJ2VJx1nGAMcAfwUcTGFoekcH7LZRjgFR1C09DmSSulIIh/sjYk5qflVS//R+f2BDqerrYKcC50laTWG48SwKexS907AClNc20gg0RsSzaf4RCoFRjtvH2cDvIqIpIt4F5gB/QxltG+UYEGV9S480vj4DWBERt7Z6ax4wIU1PAOZ2dG2lEBHfiIjqiBhIYVt4IiIuBZ4ELkzdyun7+COwVtKQ1DSKwi33y3H7WAOMkNQz/b1p+S7KZtsoywvlJI2m8L/Ellt63FTikjqMpNOARcCLfDDm/k0KxyEeAo6i8BdjXET8uSRFloikvwW+FhGfljSIwh7F4cBS4LMR8U4p6+sokmooHLDvBqwCJlL4z2TZbR+SbgA+Q+Hsv6XAJArHHMpi2yjLgDAzs/aV4xCTmZkVwQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQZmaWyQFhZmaZ/j/mfEz19cS9qQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_caseData.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 29.068115234375,
      "end_time": 1600039331022.285
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_caseData['DaysToSolution'] = df_caseData['DaysToSolution'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create title (subject+symptoms - in future we can add issue description) and body (resolution) objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  VM reboots very often\n",
       "1      Out of memory exceptions in the livy logs and ...\n",
       "2                                     warnings in ambari\n",
       "3                                        unable to scale\n",
       "4                                Issue : Scaling failure\n",
       "5                               Slower query performance\n",
       "6      120022523001923 - Creation of an HDinsight clu...\n",
       "7      1: User is unable to deploy a cluster and is g...\n",
       "8                  Ambari UI throwing 500, server error.\n",
       "9                     Unable to create Hdinsight cluster\n",
       "10      Health of cluster - heartbeat lost on all nodes.\n",
       "11     Cluster stuck in Updating Error state when try...\n",
       "12                No access to the cluster's headnodes. \n",
       "13                         Hive services unable to start\n",
       "14     We determined that the HDInsight cluster sense...\n",
       "15     You had an 'Allow_External' rules that applied...\n",
       "16     Issue:HDI Version:We deployed two new spark hd...\n",
       "17     FD QA : kpphv806llapfdqausc01 : Analyze and Qu...\n",
       "18                       HDI ambari GUI went unreachable\n",
       "19                                 slow listing of files\n",
       "20     When the client launch one Jupyter notebook in...\n",
       "21                         Not able to Delete HDID02SPRK\n",
       "22     120022721001340 - ahd501dj pyspark fails to st...\n",
       "23                                 Out of Memory Errors.\n",
       "24     120022724006081 -[Azure Government] consent te...\n",
       "25           HBase Accelerated Writes setup and guidance\n",
       "26     ProdSup:kp10tntncapllapnsprdsup01: Hive servic...\n",
       "27                      Intermitient connection failures\n",
       "28     Spark2 thrift server connection lost to headno...\n",
       "29                     Ubuntu 18.0.4 for HDInsight nodes\n",
       "                             ...                        \n",
       "915    120080724000334 - [Azure Government] Jobs cann...\n",
       "916    SecureHadoopWaitForOuContainerCreationActivity...\n",
       "917                             unable to create cluster\n",
       "918    Not able to renew adls cert. Therefore, tried ...\n",
       "919                      Unable to delete resource group\n",
       "920                              Unable to publish topic\n",
       "921                        Unable to login Ambari Portal\n",
       "922        The Yarn UI for the cluster is not available.\n",
       "923                    cannot aauthenticate with yarn UI\n",
       "924    Symptom: Heart beat lost and head node (hn0) w...\n",
       "925    Unable to create Spark cluster with ADLS Gen2 ...\n",
       "926    Followed error: Uncaught error in kafka produc...\n",
       "927      General question about LDAP certificate renewal\n",
       "928    Scale up cluster failed and cluster is not in ...\n",
       "929                    Cluster stuck in accepting state \n",
       "930           NameNode Last Checkpoint HDInsight Service\n",
       "931    Use public IP for KAfka worker node for bootst...\n",
       "932    adf sandbox - kps121sparkhpbiosbwus201 - adls ...\n",
       "933    hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.inter...\n",
       "934                                Zeppelin not starting\n",
       "935    Unable to deploy HDI cluster cluster creation ...\n",
       "936    unable to export hbase snapshot from on-prem c...\n",
       "937                      ssh user password change failed\n",
       "938     Compoments/service are not loading on Ambari UI.\n",
       "939    Getting the error “connect to host hn1-greenh ...\n",
       "940                                     General question\n",
       "941      Getting list of hosts to be customized is empty\n",
       "942    Connection failed: 'NoneType' object has no at...\n",
       "943    MDH Distribuition: Problem with deploying the ...\n",
       "944    Memory and VCores available in a Cluster for S...\n",
       "Name: Symptomstxt, Length: 945, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData['Symptomstxt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 244.578125,
      "end_time": 1600039477388.916
     }
    }
   },
   "outputs": [],
   "source": [
    "#create docs. this is nothing but all text is grouped into one field\n",
    "df_caseData['Titles']=pd.DataFrame(df_caseData.loc[:,'Title']+df_caseData.loc[:,'Symptomstxt']+df_caseData.loc[:,'initCasusePath3'])\n",
    "df_caseData['Resolution']=pd.DataFrame(df_caseData.loc[:,'Resolutiontxt'])\n",
    "#df_caseDoc=pd.DataFrame(df_caseData['Doc'])  \n",
    "#df_caseData.loc[:,'Doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 37.458984375,
      "end_time": 1600039492460.491
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Subject', 'DaysToSolution', 'initCasusePath1',\n",
       "       'initCasusePath2', 'initCasusePath3', 'Symptomstxt', 'Resolutiontxt',\n",
       "       'Titles', 'Body', 'Resolution'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData.columns\n",
    "#df_caseData.shape\n",
    "#df_caseData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\\#$%&()*+-./:;<=>?@[]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "processed_title = []\n",
    "processed_Resolution = []\n",
    "\n",
    "for column in df_caseData['Titles']:\n",
    "    processed_title.append(word_tokenize(str(preprocess(str(column)))))\n",
    "for column in df_caseData['Resolution']:\n",
    "    processed_Resolution.append(word_tokenize(str(preprocess(str(column)))))\n",
    "    #print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_title)\n",
    "len(processed_Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['***advisory', 'case***azure', 'hdinsight', 'cluster', 'connectivity', 'from', 'sas', 'eg', 'server***advisory', 'case***azure', 'hdinsight', 'cluster', 'connectivity', 'from', 'sas', 'eg', 'serverodbc', 'or', 'jdbc', 'connecting', 'to', 'standard', 'cluster']),\n",
       "       list(['/dev/sda1', 'of', 'hn0', 'has', '81', '%', 'of', 'disk', 'usage/dev/sda1', 'of', 'hn0', 'has', '81', '%', 'of', 'disk', 'usagespark']),\n",
       "       list(['/hbaserest', 'timesout', 'for', 'stage', 'hdi', 'cluster/hbaserest', 'timesout', 'for', 'stage', 'hdi', 'clusterhbase']),\n",
       "       list(['/usr/bin/hive', 'org.apache.thrift.transport.ttransportexception', ':', 'http', 'response', 'code', ':', '431few', 'users', 'are', 'not', 'able', 'to', 'connect', 'to', 'beeline.ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['/var/lib/spark2/shs_db', 'keep', 'growing/var/lib/spark2/shs_db', 'keep', 'growingspark']),\n",
       "       list(['2', '.', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'showing', 'in', 'alerts', 'on', 'wn404', '.', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'showing', 'in', 'alerts', 'on', 'wn404.issue', 'with', 'scaling', 'down']),\n",
       "       list(['20', 'nodes', 'on', 'cluster', 'is', 'deadheart', 'beat', 'loss', 'for', 'multiple', 'workernodeslost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['504', 'gateway', 'time-out', '”', 'error', '.', 'e504', 'gateway', 'time-out', '”', 'error.hive']),\n",
       "       list(['[', '03/02/2020', ']', '[', 'hdi', ']', 'i', 'am', 'unable', 'to', 'connect', 'even', 'after', 'restating', 'the', 'hive2interatcive', 'services', 'using', 'ambari.intermitient', 'connection', 'failuresinteractive', 'query']),\n",
       "       list(['[', 'azure', 'government', ']', 'can', 'not', 'scale', 'up', 'cluster', 'due', 'to', \"'cluster\", 'size', \"'\", 'page', 'error120070924004986', '-', 'can', 'not', 'scale', 'up', 'cluster', 'due', 'to', \"'cluster\", 'size', \"'\", 'page', 'error', 'hdinsight', 'serviceissue', 'with', 'scaling', 'up']),\n",
       "       list(['[', 'azure', 'government', ']', 'cant', 'publish', 'in', 'kafkaunable', 'to', 'publish', 'topickafka']),\n",
       "       list(['[', 'azure', 'government', ']', 'confirm', 'msi', 'certificate', 'renewalmsi', 'certificate', 'renewals', 'failingspark']),\n",
       "       list(['[', 'azure', 'government', ']', 'consent', 'test', 'for', 'fairfax120022724006081', '-', '[', 'azure', 'government', ']', 'consent', 'test', 'for', 'fairfaxkafka']),\n",
       "       list(['[', 'azure', 'government', ']', 'deploying', 'a', 'new', 'hdinsight', 'cluster', 'and', 'getting', 'an', 'error', ':', 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '.', 'please', 'retry', 'later.', 'can', 'not', 'deploy', 'clustercreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['[', 'azure', 'government', ']', 'edge', 'node', 'can', 'not', 'be', 'reachedcustomer', 'could', 'not', 'ssh', 'to', 'the', 'edge', 'nodeunable', 'to', 'ssh']),\n",
       "       list(['[', 'azure', 'government', ']', 'exception', 'in', 'thread', \"'main\", \"'\", 'java.io.ioexception', ':', 'failed', 'to', 'create', 'a', 'temp', 'directory', '(', 'under', '/tmp', ')', 'after', '10', 'attempts', '!', 'can', 'not', 'run', 'spark', 'workloadspark']),\n",
       "       list(['[', 'azure', 'government', ']', 'failedtoconnectwithclustererrorcode', '-', 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '[', '{', '``', 'errorcode', \"''\", ':', \"''\", 'failedtoconnectwithclustererrorcode', \"''\", ',', \"''\", 'errordescription', \"''\", ':', \"''\", 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '.', 'please', 'retry', 'later', '.', '``', '}', ']', 'create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['[', 'azure', 'government', ']', 'how', 'to', 'create', 'change', 'data', 'capture', 'topic', 'for', 'postgres', 'db120042824006120', '[', 'azure', 'government', ']', 'how', 'to', 'create', 'change', 'data', 'capture', 'topic', 'for', 'postgres', 'db', 'hdinsight', 'servicekafka']),\n",
       "       list(['[', 'azure', 'government', ']', 'jobs', 'can', 'not', 'be', 'submitted120071724003515', '-', '[', 'azure', 'government', ']', 'jobs', 'can', 'not', 'be', 'submittedspark']),\n",
       "       list(['[', 'azure', 'government', ']', 'jobs', 'can', 'not', 'be', 'submitted120080724000334', '-', '[', 'azure', 'government', ']', 'jobs', 'can', 'not', 'be', 'submittedspark']),\n",
       "       list(['[', 'azure', 'government', ']', 'minimize', 'cost', 'on', 'kafka-cluster', '(', 'hd-insight', ')', ')', '120042824005793', '-', '[', 'azure', 'government', ']', 'customer', 'wants', 'to', 'use', 'a', 'start/stop', 'feature', 'in', 'order', 'to', 'minimize', 'cost', 'on', 'kafka', 'clusterissue', 'with', 'scaling', 'down']),\n",
       "       list(['[', 'azure', 'government', ']', 'not', 'able', 'to', 'schedule', 'new', 'task', ',', 'need', 'to', 'be', 'repairedzk', 'could', \"n't\", 'form', 'a', 'quorumhive']),\n",
       "       list(['[', 'azure', 'government', ']', 'not', 'able', 'to', 'schedule', 'new', 'tasks', ',', 'cluster', 'needs', 'to', 'be', 'repaired.similar', 'issue', 'reported', 'on', 'sr', '120032624000096hive']),\n",
       "       list(['[', 'azure', 'government', ']', 'our', 'spark', 'jobs', 'are', 'not', 'being', 'submitted', 'to', 'the', 'cluster', '.', 'they', 'are', 'stuck', 'in', 'the', 'adf', 'pipeline', 'where', 'they', 'should', 'be', 'deployed', 'to', 'hdi.120071524005741', '-', 'our', 'spark', 'jobs', 'are', 'not', 'being', 'submitted', 'to', 'the', 'cluster', '.', 'they', 'are', 'stuck', 'in', 'the', 'adf', 'pipeline', 'where', 'they', 'should', 'be', 'deployed', 'to', 'hdi.spark']),\n",
       "       list(['[', 'azure', 'government', ']', 'status=', '--', '-', ',', 'can', 'not', 'delete', 'clustersnot', 'able', 'to', 'renew', 'adls', 'cert', '.', 'therefore', ',', 'tried', 'deleting', 'clusters', '&', 're-creating', 'to', 'keep', 'the', 'environement', 'running', '.', 'but', 'stuck', 'in', 'delete', 'error', 'state', '.', 'spark']),\n",
       "       list(['[', 'azure', 'government', ']', 'this', 'resource', 'wo', \"n't\", 'create', 'correctlyunable', 'to', 'deploy', 'hdi', 'cluster', 'cluster', 'creation', 'fails', 'with', '[', '{', '``', 'errorcode', \"''\", ':', \"''\", 'internalservererror', \"''\", ',', \"''\", 'errordescription', \"''\", ':', \"''\", 'fetching', 'the', 'customer', \"'s\", 'tenant', 'id', 'for', 'subscription', \"'5948035a-2b05-4ae0-adb1-95a3ca8807bb\", \"'\", '.', 'invalid', 'status', 'code', 'or', 'www-authenticate', 'header', 'is', 'empty', 'or', 'invalid', 'format', ':', \"'statuscode\", '=', 'forbidden\\\\r\\\\nheaders\\\\r\\\\npragma=no-cache\\\\r\\\\nx-ms-failure-cause=gateway\\\\r\\\\nx-ms-request-id=b77a8869-8913-4462-a015-f497eec14270\\\\r\\\\nx-', 'ms-correlation-request-id=b77a8869-8913-4462-a015-f497eec14270\\\\r\\\\nx-ms-routing-request-id=usgovtexas:20200812t215111z', ':', 'b77a8869-8913-4462-a015-f497eec14270', '\\\\r\\\\nstrict-transport-security=max-age=31536000', ';', 'includesubdomains\\\\r\\\\nx-content-type-options=nosniff\\\\r\\\\nconnection=close\\\\r\\\\ncache-control=no-cache\\\\r\\\\ndate=wed', ',', '12', 'aug', '2020', '21:51:10', 'gmt', \"'\", \"''\", '}', ']', 'create', 'failure', '-', 'other']),\n",
       "       list(['[', 'azure', 'government', ']', 'unable', 'to', 'create', 'edge', 'node', 'with', 'arm', 'deployment', 'script', 'internal', 'error', 'message', 'from', 'log', 'entrycreate', 'failure', '-', 'other']),\n",
       "       list(['[', 'azure', 'government', ']', 'unable', 'to', 'get', 'storage', 'account', 'integrated', 'with', 'msicustomer', 'was', 'receiving', 'an', 'error', 'that', 'the', 'managed', 'identity', 'in', 'self-hosted', 'hadoop', 'cluster', 'could', 'not', 'get', 'the', 'token', 'from', 'azure', 'ad.issues', 'using', 'azure', 'ad', '(', 'rbac', '&', 'oauth', ')']),\n",
       "       list(['[', 'hdi', ']', 'error', 'in', 'llap', 'cluster', 'when', 'we', 'changed', 'the', 'adds', 'certificatecannot', 'access', 'ambari', 'uiambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['aad', 'app', 'registration', 'certificate', 'expireaad', 'app', 'registration', 'certificate', 'expire', 'and', 'refreash', 'gen1', 'failurecreate', 'failure', '-', 'other']),\n",
       "       list(['access', 'the', 'json', 'file', 'from', 'spark', 'webuiadvisoryspark']),\n",
       "       list(['access', 'to', 'the', 'cluster', 'sometimes', 'fails', 'with', \"'gateway\", 'failure', \"'\", 'including', 'in', 'job', 'submissionaccess', 'to', 'the', 'cluster', 'sometimes', 'fails', 'with', \"'gateway\", 'failure', \"'\", 'including', 'in', 'job', 'submission.hadoop']),\n",
       "       list(['acid', 'transaction', 'failing', 'even', 'after', 'configuring', 'hiveerror', ':', 'error', 'while', 'compiling', 'statement', ':', 'failed', ':', 'semanticexception', '[', 'error', '10302', ']', ':', 'updating', 'values', 'of', 'bucketing', 'columns', 'is', 'not', 'supported', '.', 'column', 'id', '.', '(', 'state=42000', ',', 'code=10302', ')', ';', 'hive', 'view']),\n",
       "       list(['added', 'additional', 'node', 'and', 'getting', 'alerts', 'on', 'web', 'uisadded', 'additional', 'node', 'and', 'getting', 'alerts', 'on', 'web', 'uishadoop']),\n",
       "       list(['adding', 'ad', 'groups', 'and', 'users', 'to', 'ranger', 'user', 'sync', 'and', 'group', 'sync', 'issues', 'adding', 'ad', 'groups', 'and', 'users', 'to', 'ranger', 'user', 'sync', 'and', 'group', 'synchive']),\n",
       "       list(['adding', 'additiional', 'storage', 'account', 'to', 'exisitng', 'clusteradding', 'additiional', 'storage', 'account', 'to', 'exisitng', 'clusteradls', 'gen1', ',', 'adls', 'gen2', 'in', 'standard', 'cluster']),\n",
       "       list(['additional', 'storage', 'on', 'edge', 'nodehow', 'can', 'i', 'add', 'additional', 'volume/disk', 'to', 'the', 'edge', 'node', '?', 'create', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['adf', 'job', 'can', 'not', 'submit', 'to', 'sparkadf', 'job', 'can', 'not', 'submit', 'to', 'sparkspark']),\n",
       "       list(['adf', 'qa', '-', 'kpq063sparkespadfqawus201', '-', 'his2', 'interactive', 'downllap', 'service', 'doesnt', 'startinteractive', 'query']),\n",
       "       list(['adf', 'qa', '-', 'kpq063sparkespadfqawus201', '-', 'multiple', 'services', 'downadf', 'qa', '-', 'kpq063sparkespadfqawus201', '-', 'multiple', 'services', 'downspark']),\n",
       "       list(['adf', 'sandbox', '-', 'kps053sparkespadfsbwus201', '-', 'multiple', 'services', 'not', 'startinghive', 'interactive', 'down', 'issues', 'on', 'spark', 'hdi', '4.0', 'clusterspark']),\n",
       "       list(['adf', 'sandbox', '-', 'kps121sparkhpbiosbwus201', '-', 'adls', 'accessadf', 'sandbox', '-', 'kps121sparkhpbiosbwus201', '-', 'adls', 'accessadls', 'gen1', ',', 'adls', 'gen2', 'in', 'standard', 'cluster']),\n",
       "       list(['adls', 'certificate', 'update', 'failingadls', 'certificate', 'update', 'failinghive']),\n",
       "       list(['after', 'a', 'reboot', ',', 'we', 'are', 'unable', 'to', 'ssh', 'in', '.', \"'system\", 'is', 'booting', 'up', '.', 'see', 'pam_nologin', '(', '8', ')', \"'getting\", 'the', 'error', '“', 'connect', 'to', 'host', 'hn1-greenh', 'port', '22', ':', 'connection', 'refused', '”', 'when', 'i', 'attempt', 'to', 'ssh', 'from', 'hn0', 'to', 'hn1.unable', 'to', 'ssh']),\n",
       "       list(['after', 'deleting', 'inbound', 'nat', 'rules', ',', 'backend', 'pools', 'become', 'invisible', 'and', 'throwing', '404', 'errorsafter', 'deleting', 'inbound', 'nat', 'rules', ',', 'backend', 'pools', 'become', 'invisible', 'and', 'throwing', '404', 'errorsinteractive', 'query']),\n",
       "       list(['ahd501dj', 'hdfs', 'alerts', 'on', 'storage', 'usagehdfs', 'storage', 'usage', 'alert', 'firing', 'in', 'ambarihadoop']),\n",
       "       list(['ahd501dj', 'pyspark', 'fails', 'to', 'start', 'with', 'permissionmismatch', 'error120022721001340', '-', 'ahd501dj', 'pyspark', 'fails', 'to', 'start', 'with', 'permissionmismatch', 'errorranger', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['ahd649dj', '-', 'edge', 'node', 'ed10-ahd649', 'is', 'getting', 'rebooted', 'everydayreboot', 'system', 'boot', '4.15.0-1082-azur', 'wed', 'may', '20', '01:43', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'tue', 'may', '19', '00:40', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'mon', 'may', '18', '00:09', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'sun', 'may', '17', '00:23', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'fri', 'may', '15', '23:29', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'thu', 'may', '14', '23:24', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'thu', 'may', '14', '00:49', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'wed', 'may', '13', '00:58', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'tue', 'may', '12', '01:03', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'mon', 'may', '11', '00:17', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'sun', 'may', '10', '00:44', 'still', 'runningreboot', 'system', 'boot', '4.15.0-1082-azur', 'sat', 'may', '9', '01:30', 'still', 'runninghadoop']),\n",
       "       list(['alert', 'for', 'kafka', 'broker', '-', 'connection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split'alert\", 'for', 'kafka', 'broker', '-', 'connection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split'kafka\"]),\n",
       "       list(['alerts', 'on', 'different', 'uisalerts', 'on', 'different', 'uishadoop']),\n",
       "       list(['all', 'nodes', 'are', 'unhealthynode', 'manager', 'unhealthynode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['all', 'nodes', 'showing', 'heartbeat', 'lost', 'in', 'ambaariall', 'nodes', 'showing', 'heartbeat', 'lost', 'in', 'ambaarilost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['all', 'the', 'fact', 'cdl', 'processing', 'is', 'getting', 'failed', 'due', 'to', 'json', 'parameter', 'could', 'not', 'be', 'decoded', 'at', 'cdlengine.py', 'process', 'run', 'key:22540131all', 'the', 'fact', 'cdl', 'processing', 'is', 'getting', 'failed', 'due', 'to', 'json', 'parameter', 'could', 'not', 'be', 'decoded', 'at', 'cdlengine.py', 'process', 'run', 'key:22540131odbc', 'or', 'jdbc', 'connecting', 'to', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['all', 'the', 'nodes', 'are', 'unhealthy', 'and', 'not', 'reacheable', 'services', 'unhealthy.hadoop']),\n",
       "       list(['altering', 'pk', 'constraint', '-', 'phoenix', 'tablealtering', 'pk', 'constraint', '-', 'phoenix', 'tablehbase']),\n",
       "       list(['ambari', 'can', 'not', 'connect', 'to', 'zk2-hbase', 'as', 'well', 'as', 'ssh', 'to', 'zk2-hbase', 'is', 'not', 'workingambari', 'can', 'not', 'connect', 'to', 'zk2-hbase', 'as', 'well', 'as', 'ssh', 'to', 'zk2-hbase', 'is', 'not', 'working.lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['ambari', 'does', 'not', 'provide', 'error', 'message', 'ambari', 'does', 'not', 'provide', 'error', 'messagehive']),\n",
       "       list(['ambari', 'metric', 'collector', 'service', 'issuesunable', 'to', 'start', 'ambari', 'metrics', 'collector', 'serviceinteractive', 'query']),\n",
       "       list(['ambari', 'metric', 'not', 'showing', 'ambari', 'metric', 'not', 'showinghadoop']),\n",
       "       list(['ambari', 'metrics', 'collector', 'service', 'throwing', 'alerts', 'consistently', 'ambari', 'metrics', 'collector', 'service', 'throwing', 'alerts', 'consistentlyspark']),\n",
       "       list(['ambari', 'metrics', 'not', 'available.ats', 'service', 'is', 'going', 'downhadoop']),\n",
       "       list(['ambari', 'metrics', 'not', 'availableams', 'metrics', 'issue..hbase']),\n",
       "       list(['ambari', 'metrics', 'not', 'workingambari', 'metrics', 'not', 'workinghadoop']),\n",
       "       list(['ambari', 'server', 'is', 'in', 'failed', 'status', 'on', 'hn0', '(', 'non-active', ')', 'root', '@', 'hn1-reldfh', ':', '~', '#', 'sudo', 'systemctl', 'status', 'ambari-server', '--', 'ambari-server.service', '-', 'ambari-server', 'loaded', ':', 'loaded', '(', '/etc/systemd/system/ambari-server.service', ';', 'disabled', ';', 'vendor', 'preset', ':', 'enabled', ')', 'active', ':', 'failed', '(', 'result', ':', 'exit-code', ')', 'since', 'thu', '2020-07-09', '12:27:17', 'utc', ';', '1', 'weeks', '1', 'days', 'ago', 'main', 'pid', ':', '16653', '(', 'code=exited', ',', 'status=143', ')', 'warning', ':', 'journal', 'has', 'been', 'rotated', 'since', 'unit', 'was', 'started', '.', 'log', 'output', 'is', 'incomplete', 'or', 'unavailable.hbase']),\n",
       "       list(['ambari', 'subgroup', 'does', \"n't\", 'inherited', 'of', 'groupaccessambari', 'subgroup', 'does', \"n't\", 'inherited', 'of', 'groupaccessspark']),\n",
       "       list(['ambari-server', 'restarted', 'unexpectedly', ',', 'server', 'ca', \"n't\", 'bind', 'to', 'port', '42700ambari-server', 'restarted', 'unexpectedly', ',', 'error', '-', 'failed', 'to', 'bind', 'to', 'port', '42700hadoop']),\n",
       "       list(['anaconda', 'installation', 'failinganaconda', 'installation', 'failingspark']),\n",
       "       list(['apache', 'ranger', 'on', 'adlsapache', 'ranger', 'on', 'adls', 'feature', 'availabilityranger', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['app', 'team', 'not', 'able', 'to', 'run', 'their', 'webjobsapp', 'team', 'not', 'able', 'to', 'restart', 'the', 'web', 'jobs', 'and', 'giving', 'errors', 'that', 'unable', 'to', 'make', 'a', 'connection', 'with', 'zookeeper.pfa', 'logs', 'for', 'datalogger', 'and', 'trends', 'webjob', '.', 'hbase']),\n",
       "       list(['application', 'log', 'missingapplication', 'log', 'missingspark']),\n",
       "       list(['application', 'stuck', 'in', 'accepted', 'statue', 'with', 'plenty', 'of', 'free', 'memory', '&', 'coresall', 'jobs', 'stuck', 'at', 'accepted', 'statespark']),\n",
       "       list(['application', 'stuck', 'in', 'accepted', 'statue', 'with', 'plenty', 'of', 'free', 'memory', '&', 'coresapplication', 'stuck', 'in', 'accepted', 'statue', 'with', 'plenty', 'of', 'free', 'memory', '&', 'coresspark']),\n",
       "       list(['application', 'unable', 'to', 'connectapplication', 'unable', 'to', 'connectodbc', 'or', 'jdbc']),\n",
       "       list(['applied', 'nsg', ',', 'jupyter', 'notebook', 'not', 'working.issuewhen', 'you', 'access', 'the', 'jupyter', 'service', 'on', 'hdinsight', ',', 'you', 'see', 'an', 'error', 'box', 'saying', '“', 'not', 'found', '”', '.if', 'you', 'check', 'the', 'jupyter', 'logs', '(', 'on', 'hn0', '/var/log/jupyter', ')', ',', 'you', 'will', 'see', 'something', 'like', 'this', ':', '[', 'w', '2018-08-21', '17:43:33.352', 'notebookapp', ']', '404', 'put', '/api/contents/pyspark/notebook.ipynb', '(', '10.16.0.144', ')', '4504.03ms', 'referer=https', ':', '//pnhr01hdi-corpdir.msappproxy.net/jupyter/notebooks/pyspark/notebook.ipynbblocking', 'cross', 'origin', 'api', 'request', '.', 'origin', ':', 'https', ':', '//xxx.xxx.xxx', ',', 'host', ':', 'hn0-pnhr01.j101qxjrl4zebmhb0vmhg044xe.ax.internal.cloudapp.net:8001you', 'may', 'also', 'see', 'an', 'ip', 'address', 'in', 'the', '“', 'origin', '”', 'field', 'in', 'the', 'jupyter', 'log.create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['are', 'there', 'any', 'recommended', 'configuration', 'tuning', 'or', 'best', 'practices', 'available', 'to', 'reduce', 'time', 'taken', 'for', 'auto', 'scale', 'up/down', 'a', 'worker', 'node', '?', 'are', 'there', 'any', 'recommended', 'configuration', 'tuning', 'or', 'best', 'practices', 'available', 'to', 'reduce', 'time', 'taken', 'for', 'auto', 'scale', 'up/down', 'a', 'worker', 'node', '?', 'issue', 'with', 'autoscaling']),\n",
       "       list(['arm', 'deployment', 'hdinsight-202003241800', 'under', 'azr1473-va8-poc-hdinsight-base', 'resource', 'group', 'failsmultipule', 'errors', ':', 'payload', 'incorrect', 'errorcreate', 'failure', '-', 'other']),\n",
       "       list(['ats', 'check', 'failed', 'on', 'hive', 'service', 'checkshive', 'view', 'not', 'accessible', 'from', 'ambarihive', 'view']),\n",
       "       list(['attempting', 'to', 'scale', 'the', 'cluster', 'to', '256', 'nodes', 'failscale', 'up', 'failed', 'for', 'nodes', 'greater', 'than', '100', 'issue', 'with', 'scaling', 'up']),\n",
       "       list(['aumentar', 'capacidade', 'do', 'hdinsightalert', 'in', 'azure', 'portal', 'when', 'attempting', 'to', 'scale', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['authentication', 'error', 'for', 'metastore', 'during', 'configurationauthentication', 'error', 'for', 'metastore', 'during', 'configurationcreate', 'failure', '-', 'other']),\n",
       "       list(['auto', 'scale', 'inmanual', 'scale', 'up', 'failed', 'twice', 'in', 'a', 'row', 'and', 'took', '2', '-', '3', 'hours', 'to', 'fail.issue', 'with', 'scaling', 'up']),\n",
       "       list(['auto-scale', 'job', 'failurejob', 'failurespark']),\n",
       "       list(['automação', 'do', 'hdinsight', 'através', 'do', 'armprovisioning', 'via', 'java', 'sdk', 'failed', 'with', 'error', ':', \"''\", 'server', 'error', \"''\", 'create', 'failure', '-', 'other']),\n",
       "       list(['azure', 'hd', '-', 'hive', 'services', 'are', 'running', 'out', 'of', 'heap', 'spacesgeneral', 'guidance', 'or', 'advisory', '(', 'preview', ')', 'on', 'the', 'capacity', 'in', 'adf/databrickproblem', 'with', 'health', 'data', 'freshness']),\n",
       "       list(['azure', 'monitor', 'integration', 'not', 'workingphbs01adlbatch', ':', 'i', 'enabled', 'azure', 'monitor', 'for', 'the', 'first', 'time', 'on', 'the', 'cluster', ',', 'the', 'ambari', 'steps', 'completed', 'successfully', ',', 'but', 'no', 'data', 'is', 'showing', 'in', 'the', 'table', '.', 'i', 'then', 'checked', 'and', 'we', 'are', 'getting', 'the', 'same', '401', 'auth', 'error', 'for', 'the', 'hdiwatchdog', 'user', '.', 'so', 'seems', 'the', 'removal', 'and', 're-sync', 'of', 'hdiwatchdog', 'user', 'is', 'necessary', '.', 'phsp02adlspark', ':', 'azure', 'monitor', 'on', 'portal', 'shows', 'disabled', 'but', 'logs', 'already', 'showing', 'up', 'in', 'table', ',', 'so', 'seems', 'it', 'was', 'attempted', 'to', 'be', 'enabled', 'before', 'and', 'like', 'we', 'saw', 'with', 'prep', 'the', 'previous', 'enable', 'steps', 'must', 'have', 'failed', '.', 'i', 'also', 'confirmed', 'no', 'persisted', 'script', 'action', '.', 'i', 'first', 'tried', 'just', 'enabling', 'to', 'see', 'what', 'would', 'happen', ',', 'and', 'seems', 'the', 'oms', 'install', 'on', 'some', 'of', 'the', 'nodes', 'are', 'getting', 'stuck', '.', 'this', 'one', 'might', 'require', 'running', 'the', 'uninstall', 'script', 'action', 'and', 'then', 're-install', '.', 'however', ',', 'for', 'the', '1', 'node', 'it', 'is', 'stuck', 'on', '(', 'wn39', ')', ',', 'i', 'am', 'trying', 'to', 'ssh', 'into', 'it', 'and', 'can', '’', 't', 'get', 'in', 'and', 'get', 'permission', 'denied', 'errors', 'as', 'if', 'my', 'password', 'is', 'wrong', ',', 'but', 'it', 'is', 'not', '.', 'we', 'faced', 'the', 'same', 'issue', 'in', 'non-prod', '(', 'case', '120040824005545', ')', 'and', 'now', 'are', 'opening', 'new', 'case', 'for', 'prod', '.', 'aditya', 'kola', 'will', 'be', 'picking', 'up', 'this', 'case', 'and', 'working', 'with', 'meazure', 'log', 'analytics', 'integration']),\n",
       "       list(['azure', 'portal', ':', 'zookeeper', 'info', 'is', 'not', 'available', 'for', 'spark', '(', 'esp', '&', 'non', 'esp', ')', 'clusters', '.azure', 'portal', ':', 'zookeeper', 'info', 'is', 'not', 'available', 'for', 'spark', '(', 'esp', '&', 'non', 'esp', ')', 'clusters', '.spark']),\n",
       "       list(['bad', 'gateway', 'error', ':', 'not', 'able', 'to', 'access', 'the', 'ambari', 'uibad', 'gateway', 'error', ':', 'not', 'able', 'to', 'access', 'the', 'ambari', 'uiinteractive', 'query']),\n",
       "       list(['bdm', 'jobs', 'failed', 'with', 'unhealthy', 'node', 'issuebdm', 'jobs', 'failed', 'with', 'unhealthy', 'node', 'issuehive']),\n",
       "       list(['beeline', 'client', 'is', 'not', 'working', 'after', 'scaleupbeeline', 'client', 'is', 'not', 'working', 'after', 'scaleupspark']),\n",
       "       list(['both', 'headnodes', 'are', 'down', ',', 'i', 'cant', 'get', 'them', 'to', 'start', 'upboth', 'headnodes', 'are', 'down', ',', 'i', 'can', '’', 't', 'get', 'them', 'to', 'start', 'up', '.', 'name', 'nodes', 'were', 'down', '.', 'node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['brute', 'force', 'attackbrute', 'force', 'attackhadoop']),\n",
       "       list(['ca', \"n't\", 'access', 'to', 'yarn', 'ui120033026003522', '-', 'ca', \"n't\", 'access', 'to', 'yarn', 'uihadoop']),\n",
       "       list(['ca', \"n't\", 'authenicate', 'with', 'users', 'with', 'mfa', 'enabledca', \"n't\", 'authenicate', 'with', 'users', 'with', 'mfa', 'enabledambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['ca', \"n't\", 'authenticate', 'ambari', 'database', 'while', 'provisioning', 'hdica', \"n't\", 'authenticate', 'ambari', 'database', 'while', 'provisioning', 'hdicreate', 'failure', '-', 'other']),\n",
       "       list(['ca', \"n't\", 'change', 'version', 'of', 'hadoop', 'componentssymptom', ':', 'spark', 'application', 'are', 'crashing', 'due', 'to', 'version', 'compatibility.create', 'failure', '-', 'other']),\n",
       "       list(['ca', \"n't\", 'create', 'cluster', '(', 'error', '500', 'conflict', ')', '2020-06-15', '20:36:54.7228768', 'westeurope', '7afd8a0e-516f-4623-894d-022e4e8e4c71', 'zcfoyscgwl-projectspark', '26214cab1dd94e508d1ecca958fddaf9', 'error', '3.6.1000.67.2004291541', '1', 'spark', '[', '{', '``', 'errorcode', \"''\", ':', \"''\", 'internalerror', \"''\", ',', \"''\", 'errordescription', \"''\", ':', \"''\", 'an', 'unexpected', 'error', 'occurred', \"''\", '}', ']', '{', '``', 'locationheadervalue', \"''\", ':', \"''\", 'https', ':', '//management.azure.com/subscriptions/6c690792-2d1a-44b4-9326-c3e886c64719/providers/microsoft.aad/locations/northeurope/operationresults/69580b16-7f00-4087-8697-cd1df00a2c20', '?', 'api-version=2017-06-01', '&', 'operationresultresponsetype=location', \"''\", ',', \"''\", 'retryafter', \"''\", ':', \"''\", '00:00:10', \"''\", ',', \"''\", 'httpstatuscode', \"''\", ':500', ',', \"''\", 'data', \"''\", ':', \"''\", \"''\", ',', \"''\", 'errorinfo', \"''\", ':', '{', '``', 'error', \"''\", ':', '{', '``', 'code', \"''\", ':', \"''\", 'internalerror', \"''\", ',', \"''\", 'message', \"''\", ':', \"''\", 'an', 'unexpected', 'error', 'occurred', \"''\", '}', '}', '}', 'create', 'failure', '-', 'other']),\n",
       "       list(['ca', \"n't\", 'login', 'hadoop', 'ambari', 'with', 'admin', 'account120031324004731', '-', 'ca', \"n't\", 'login', 'to', 'the', 'ambari', 'ui', 'with', 'the', 'admin', 'account\\u200bambari', 'in', 'standard', 'cluster']),\n",
       "       list(['ca', \"n't\", 'run', 'spark', 'jobsca', \"n't\", 'run', 'spark', 'jobsspark']),\n",
       "       list(['can', 'i', 'add', 'same', 'azure', 'blob', 'storage', 'to', 'two', 'different', 'hdinsight', 'clustersclient', 'ask', 'if', 'they', 'can', 'add', 'same', 'azure', 'blob', 'storage', 'to', 'two', 'different', 'hdinsight', 'cluster.create', 'failure', '-', 'other']),\n",
       "       list(['can', 'not', 'access', 'adls', 'gen', '2', 'from', 'this', 'cluster', 'using', 'the', 'serviceprincipal', 'and', 'cert', 'auththey', 'can', 'not', 'access', 'gen2', 'storage', 'account', 'from', 'the', 'cluster.spark']),\n",
       "       list(['can', 'not', 'access', 'associated', 'storage', 'containercannot', 'access', 'associated', 'storage', 'containeradls', 'gen1', ',', 'adls', 'gen2', 'in', 'standard', 'cluster']),\n",
       "       list(['can', 'not', 'access', 'yarnuicannot', 'access', 'yarnuispark']),\n",
       "       list(['can', 'not', 'authenticate', 'to', 'yarncannot', 'aauthenticate', 'with', 'yarn', 'uiambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['can', 'not', 'change', 'the', 'sshuser', 'password', 'throug', 'script', 'actionssh', 'user', 'password', 'change', 'failedambari', 'in', 'standard', 'cluster']),\n",
       "       list(['can', 'not', 'convert', 'notebook', 'to', 'v5', 'because', 'that', 'version', 'does', \"n't\", 'existcannot', 'open', 'jupyter', 'notebook.spark']),\n",
       "       list(['can', 'not', 'figure', 'out', 'how', 'to', 'access', 'running', 'applications', 'apiunable', 'to', 'list', 'out', 'or', 'filter', 'out', 'the', 'running', 'jobshdinsight', 'sdk']),\n",
       "       list(['can', 'not', 'find', 'the', 'correct', 'sponsorhe', 'could', \"n't\", 'deploy', 'a', 'cluster', 'on', 'one', 'of', 'his', 'subscriptionissues', 'signing', 'in', 'or', 'accessing', 'my', 'subscriptions']),\n",
       "       list(['can', 'not', 'scale', 'clusterunable', 'to', 'scale', 'cluster', 'and', 'may', 'see', 'below', 'exception', 'and', 'alos', 'zombie', 'hosts', 'in', 'ambari,2020-06-21', '14:16:00.0000000', 'warn', 'resourcemanager', 'dfsinputstream.java', '777', 'dfs', 'read', 'org.apache.hadoop.hdfs.blockmissingexception', 'could', 'not', 'obtain', 'block', ':', 'bp-768983909-10.8.1.133-1583269809076', ':', 'blk_1073950945_212685', 'file=/yarn/node-labels/nodelabel.mirror', 'org.apache.hadoop.hdfs.blockmissingexception', ':', 'could', 'not', 'obtain', 'block', ':', 'bp-768983909-10.8.1.133-1583269809076', ':', 'blk_1073950945_212685', 'file=/yarn/node-labels/nodelabel.mirrorissue', 'with', 'scaling', 'up']),\n",
       "       list(['can', 'not', 'scale', 'upautoscale', 'fails', 'with', '``', '[', '{', '``', 'errorcode', \"''\", ':', \"''\", 'internalservererror', \"''\", ',', \"''\", 'errordescription', \"''\", ':', \"''\", 'encountered', 'failure', '(', 's', ')', 'scaling', 'cluster', 'to', '10', 'nodes', '.', 'scaling', 'operation', 'partially', 'succeeded', 'with', 'a', 'final', 'node', 'count', 'of', '1.', 'scaling', 'error', 'code', ':', 'internalservererror', '.', 'scaling', 'error', 'message', ':', 'failed', 'to', 'setup', 'hosts', 'during', 'scale', 'up', '.', '``', '}', ']', \"''\", 'issue', 'with', 'scaling', 'up']),\n",
       "       list(['can', 'not', 'scale', 'upunable', 'to', 'scaleupissue', 'with', 'scaling', 'up']),\n",
       "       list(['can', 'not', 'scalue', 'upcannot', 'scalue', 'upissue', 'with', 'scaling', 'up']),\n",
       "       list(['can', 'not', 'start', 'cluster', 'afer', 'restarting', 'itodbc', 'connection', 'to', 'the', 'cluster', 'from', 'customers', 'application', 'dropped', 'out', 'and', 'customer', 'was', 'unable', 'to', 'restart', 'the', 'components', 'from', 'ambari', ',', 'they', 'then', 'rebooted', 'each', 'node', 'and', 'services', 'came', 'back', 'up.hive']),\n",
       "       list(['can', 'not', 'start', 'resource', 'managercannot', 'start', 'resource', 'managerhadoop']),\n",
       "       list(['can', 'not', 'use', 'variables', 'in', 'hive', 'commandscannot', 'use', 'variables', 'in', 'hive', 'command.hive']),\n",
       "       list(['can', 'we', 'install', 'red', 'cloak', ',', 'cylance', ',', 'splunk', 'on', 'hdi', 'vm', \"'s\", 'can', 'we', 'install', 'red', 'cloak', ',', 'cylance', ',', 'splunk', 'on', 'hdi', \"vm'slost\", 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['cert', 'expiry', 'for', 'app', 'registration', 'that', 'controls', 'access', 'to', 'adls', 'gen1cert', 'expiry', 'for', 'app', 'registration', 'that', 'controls', 'access', 'to', 'adls', 'gen1ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['chkp22adlstreamcould', 'not', 'deploy', 'second', 'edgenodenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['ci/cd', 'deployment', 'from', 'azure', 'devops', 'erased', 'several', 'resources', 'in', 'the', 'resource', 'group.customer', 'reportedspark']),\n",
       "       list(['clamav', 'is', 'using', '100', '%', 'cpu', 'on', 'edge', 'nodesclamav', 'is', 'using', '100', '%', 'cpu', 'on', 'edge', 'nodesnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['clamscan', 'is', 'using', 'excessive', 'amount', 'of', 'cpu', 'we', 'have', 'two', 'issues', 'for', 'which', 'we', 'need', 'support', 'from', 'mirosoft', ':', '1.', 'clamscan', 'is', 'using', 'excessive', 'amount', 'of', 'cpu', '-', 'adjustments', 'or', 'remove', 'diretories.2', '.', 'investigation', 'on', 'moving', 'sda', 'to', 'ssd', '.', 'interactive', 'query']),\n",
       "       list(['class', 'org.apache.hadoop.fs.adl.hdiadlfilesystem', 'not', 'found', 'class', 'org.apache.hadoop.fs.adl.hdiadlfilesystem', 'not', 'foundmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['cluster', 'creaetino', 'fails', 'with', 'remainingcores', 'is', 'too', 'small', 'even', 'if', 'i', 'have', 'enough', 'cluster', 'creation', 'fails', 'with', 'remaining', 'cores', 'is', 'too', 'small', 'even', 'if', 'i', 'have', 'enoughcreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'creation', 'failedcluster', 'creation', 'failedcreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'creation', 'is', 'failedunable', 'to', 'create', 'the', 'cluster', 'in', 'ne', 'and', 'we', 'regions', 'due', 'to', 'capacitycreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'creation', 'with', 'vnet', 'from', 'different', 'subscription', 'in', 'the', 'same', 'region', 'failscluster', 'creation', 'with', 'vnet', 'from', 'different', 'subscription', 'in', 'the', 'same', 'region', 'fails.create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'deployment', 'failedcluster', 'deployment', 'failedcreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'deployment', 'failscluster', 'deployment', 'failscreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'deployments', 'are', 'failinghdinsight', 'clusters', 'periodically', 'fail', 'provisioning', 'with', 'the', 'following', 'error', ':', 'vmgroup', 'overprovisioning', 'has', 'failed', 'for', 'vmgroup', 'gateway', 'with', 'code', '=', 'timedout', '.', 'status', 'code', 'before', 'timeout', 'is', 'notenoughvmshaveprovisioningagentup.create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'does', \"n't\", 'start..generic', 'error', 'messagecluster', 'does', \"n't\", 'start..generic', 'error', 'messagecreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'failed', 'to', 'deploy', ',', 'generic', 'error', 'code', ',', 'vnet', 'and', 'gen2the', 'cluster', 'failed', 'due', 'to', 'azure', 'resource', 'generic', 'errorcreate', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['cluster', 'failing', 'to', 'create', 'with', 'error', 'that', 'the', 'name', 'is', 'already', 'in', 'usecluster', 'failing', 'to', 'create', 'with', 'error', 'that', 'the', 'name', 'is', 'already', 'in', 'usecreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'fuera', 'de', 'lineacluster', 'scale', 'up', 'failed.hbase']),\n",
       "       list(['cluster', 'headnode', 'unhealthycluster', 'headnode', 'unhealthy.resource_management.core.exceptions.executionfailed', ':', 'execution', 'of', \"'/usr/bin/apt-get\", '-q', '-o', 'dpkg', ':', ':options', ':', ':=', '--', 'force-confdef', '--', 'allow-unauthenticated', '--', 'assume-yes', 'install', 'liblzo2-2', \"'\", 'returned', '100.', 'e', ':', 'dpkg', 'was', 'interrupted', ',', 'you', 'must', 'manually', 'run', \"'dpkg\", '--', 'configure', '-a', \"'\", 'to', 'correct', 'the', 'problem.exception', 'in', 'thread', '``', 'main', \"''\", 'org.apache.hadoop.yarn.exceptions.applicationnotfoundexception', ':', 'the', 'entity', 'for', 'application', 'application_1584397808761_15233', 'does', \"n't\", 'exist', 'in', 'the', 'timeline', 'store', 'at', 'org.apache.hadoop.yarn.server.applicationhistoryservice.applicationhistorymanagerontimelinestore.getapplication', '(', 'applicationhistorymanagerontimelinestore.java:678', ')', 'spark']),\n",
       "       list(['cluster', 'identity', 'certificate', 'update', 'failedunable', 'to', 'run', 'any', 'queries', 'and', 'cert', 'refresh', 'would', 'not', 'refresh.hive']),\n",
       "       list(['cluster', 'in', 'applying', 'changes', 'state', 'for', 'a', 'long', 'period', 'of', 'timecluster', 'deployment', 'took', 'longer', 'than', 'expected', 'with', 'deployment', 'failure', '.', 'create', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'is', 'deploys', 'and', 'few', 'services', 'stop', 'when', 'deployed', 'cluster', 'is', 'deploys', 'and', 'few', 'services', 'stop', 'when', 'deployedinteractive', 'query']),\n",
       "       list(['cluster', 'is', 'getting', 'failed', 'while', 'creating', '.', 'esp', 'cluster', 'creation', ',', 'multiple', 'issues.create', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'is', 'in', 'applying', 'changes', 'mode', 'from', 'last', '5', 'dayswhen', 'running', 'a', 'script', 'action', 'cluster', 'gets', 'stuck', 'in', 'apply', 'changes', 'statespark']),\n",
       "       list(['cluster', 'is', 'not', 'coming', 'in', 'running', 'statecapacity', 'issue', 'on', 'data', 'centercreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'is', 'not', 'getting', 'up', 'and', 'running', 'statecluster', 'is', 'not', 'getting', 'up', 'and', 'running', 'statecreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'is', 'not', 'scaling', 'and', 'more', 'than', '20', 'node', 'managers', 'are', 'not', 'livecluster', 'in', 'error', 'state', ',', 'observing', 'several', 'node', 'managers', 'down', 'after', 'scaling', 'the', 'cluster', 'up', ',', 'ambari', 'db', 'asc', 'graph', 'shows', 'db', 'is', 'exhausted.issue', 'with', 'autoscaling']),\n",
       "       list(['cluster', 'is', 'performing', 'extremely', 'slow', ',', 'even', 'though', 'all', 'the', 'nodes', 'and', 'services', 'are', 'up', 'and', 'runningcluster', 'is', 'performing', 'extremely', 'slow', ',', 'even', 'though', 'all', 'the', 'nodes', 'and', 'services', 'are', 'up', 'and', 'runninghive']),\n",
       "       list(['cluster', 'is', 'unable', 'to', 'determine', 'the', 'number', 'of', 'nodesdeleted', 'a', 'worker', 'as', 'well', 'as', 'edge', 'node', 'from', 'ambari', 'console.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['cluster', 'launch', 'failedcluster', 'launch', 'failedcreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'not', 'accessiblecluster', 'not', 'accessiblehadoop']),\n",
       "       list(['cluster', 'not', 'scaling', 'up', ',', 'yarn', 'not', 'accessiblecluster', 'scale', 'up', 'failing', 'for', 'our', 'production', 'job', '.', 'the', 'scale', 'up', 'is', 'triggered', 'through', 'azure', 'cli', 'in', 'our', 'production', 'environment', '.', 'tried', 'scaling', 'up', 'through', 'portal', ',', 'stuck', 'at', 'that', 'for', 'the', 'past', '2', 'hours', '.', 'unable', 'to', 'access', 'yarn', ',', 'getting', 'http', 'error', '502.3', '-', 'bad', 'gateway.issue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'not', 'scaling', 'upunable', 'to', 'scaleissue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'not', 'scalingerror', 'message', 'from', 'portal', 'clearly', 'states', 'it', \"'s\", 'an', 'issue', 'with', 'the', 'resource', 'group', 'deployment', 'history.issue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'not', 'sclae', 'up', 'due', 'to', 'rmcluster', 'not', 'scale', 'up', 'due', 'to', 'rmissue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'provisioning', 'state', 'failedcluster', 'deployment', 'failure', 'with', 'metastore', 'schema', 'errorcreate', 'failure', '-', 'other']),\n",
       "       list(['cluster', 'scaling', 'failingunable', 'to', 'scale', 'up', '.', 'either', 'scale', 'up', 'happened', 'and', 'new', 'node', 'is', 'removed', '.', 'also', 'keytab', 'was', 'not', 'being', 'copied', 'over', '.', 'issue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'scaling', 'not', 'working', 'as', 'intendedcluster', 'scaling', 'not', 'working', 'as', 'intendedissue', 'with', 'scaling', 'down']),\n",
       "       list(['cluster', 'stuck', 'at', 'scaling', 'up', 'because', 'rm', 'not', 'being', 'allocatedunable', 'to', 'submit', 'applications', 'to', 'standby', 'rmissue', 'with', 'scaling', 'up']),\n",
       "       list(['cluster', 'stuck', 'in', \"'applying\", 'changes', \"'\", 'modecluster', 'stuck', 'in', \"'applying\", 'changes', \"'\", 'modeissue', 'with', 'scaling', 'down']),\n",
       "       list(['cluster', 'stuck', 'in', 'hdinsight', 'configuration', 'stepcluster', 'stuck', 'in', 'configurationsunable', 'to', 'ssh']),\n",
       "       list(['cluster', 'taking', 'over', '2', 'hours', 'to', 'get', 'to', 'running', 'statuscluster', 'in', 'operational', 'statecreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['cluster', 'unavailablecluster', 'unavailable.hive']),\n",
       "       list(['cluster', 'went', 'into', 'an', 'error', 'state', 'after', 'trying', 'to', 'scale', 'up', 'clustercluster', 'went', 'into', 'an', 'error', 'state', 'after', 'trying', 'to', 'scale', 'up', 'cluster.issue', 'with', 'scaling', 'up']),\n",
       "       list(['clusters', 'failing', 'to', 'createhdinsight', 'provisions', 'virtual', 'machines', 'associated', 'with', 'clusters', 'in', 'our', '‘', 'provisioning', 'subscriptions', '’', '.', 'each', 'region', 'has', 'multiple', 'such', 'provisioning', 'subscriptions', '.', 'one', 'of', 'our', 'provisioning', 'subscriptions', 'in', 'the', 'region', 'faced', 'capacity', 'issues', 'due', 'to', 'a', 'spike', 'in', 'cluster', 'creations', '.', 'we', 'have', 'addressed', 'the', 'issue', 'by', 'adding', 'additional', 'capacity', 'for', 'the', 'affected', 'subscription', '.', 'we', 'have', 'also', 'made', 'improvements', 'to', 'our', 'alerts', 'by', 'fine-tuning', 'our', 'thresholds', 'to', 'catch', 'such', 'issues', 'quicker.create', 'failure', '-', 'other']),\n",
       "       list(['clusters', 'failling', '-', 'production', 'env', 'affectedclusters', 'failling', '-', 'production', 'env', 'affected', '-', 'unable', 'to', 'deploy', 'the', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['clusters', 'have', 'no', 'resources', 'or', 'unexpected', 'behaviourclusters', 'have', 'no', 'resources', 'or', 'unexpected', 'behaviourspark']),\n",
       "       list(['command', 'yarn', 'application', '-list', 'taking', 'an', 'hourcommand', 'yarn', 'application', '-list', 'taking', 'an', 'hourspark']),\n",
       "       list(['confusing', 'resource', 'setup', ';', 'unable', 'to', 'scale', 'upconfusing', 'resource', 'setup', ';', 'unable', 'to', 'scale', 'up.issue', 'with', 'autoscaling']),\n",
       "       list(['connection', 'errorconnection', 'error', 'on', 'amsspark']),\n",
       "       list(['connection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'to', 'region', 'serverconnection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'to', 'region', 'serverlost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['connection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'to', 'wn3-prddfh.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net:30010connection', 'failed', ':', \"'nonetype\", \"'\", 'object', 'has', 'no', 'attribute', \"'split\", \"'\", 'to', 'wn3-prddfh.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net:30010hbase']),\n",
       "       list(['connection', 'failed', 'to', 'headnodeconnection', 'failed', 'to', 'headnode.hive']),\n",
       "       list(['connection', 'failure', 'from', 'kafka', 'to', 'spark', 'clusterunable', 'to', 'connect', 'between', 'kafka', 'and', 'spark', 'clusterskafka']),\n",
       "       list(['connection', 'issue', 'with', 'kafkanot', 'able', 'to', 'connect', 'to', 'kafka', 'from', 'the', 'application', ',', 'pfa', 'logs', 'for', 'connectivity', 'issue.error', 'message', ':', '[', '2020-04-10', '15:14:43,656', ']', 'warn', '[', 'replicafetcher', 'replicaid=1001', ',', 'leaderid=1004', ',', 'fetcherid=1', ']', 'error', 'in', 'response', 'for', 'fetch', 'request', '(', 'type=fetchrequest', ',', 'replicaid=1001', ',', 'maxwait=2000', ',', 'minbytes=1', ',', 'maxbytes=10485760', ',', 'fetchdata=', '{', '}', ',', 'isolationlevel=read_uncommitted', ',', 'toforget=', ',', 'metadata=', '(', 'sessionid=1942697549', ',', 'epoch=8060', ')', ')', '(', 'kafka.server.replicafetcherthread', ')', 'java.io.ioexception', ':', 'connection', 'to', '1004', 'was', 'disconnected', 'before', 'the', 'response', 'was', 'readat', 'org.apache.kafka.clients.networkclientutils.sendandreceive', '(', 'networkclientutils.java:97', ')', 'at', 'kafka.server.replicafetcherblockingsend.sendrequest', '(', 'replicafetcherblockingsend.scala:97', ')', 'at', 'kafka.server.replicafetcherthread.fetchfromleader', '(', 'replicafetcherthread.scala:190', ')', 'at', 'kafka.server.abstractfetcherthread.kafka', '$', 'server', '$', 'abstractfetcherthread', '$', '$', 'processfetchrequest', '(', 'abstractfetcherthread.scala:241', ')', 'at', 'kafka.server.abstractfetcherthread', '$', '$', 'anonfun', '$', 'maybefetch', '$', '1.apply', '(', 'abstractfetcherthread.scala:130', ')', 'at', 'kafka.server.abstractfetcherthread', '$', '$', 'anonfun', '$', 'maybefetch', '$', '1.apply', '(', 'abstractfetcherthread.scala:129', ')', 'at', 'scala.option.foreach', '(', 'option.scala:257', ')', 'at', 'kafka.server.abstractfetcherthread.maybefetch', '(', 'abstractfetcherthread.scala:129', ')', 'at', 'kafka.server.abstractfetcherthread.dowork', '(', 'abstractfetcherthread.scala:111', ')', 'at', 'kafka.utils.shutdownablethread.run', '(', 'shutdownablethread.scala:82', ')', '?', '2020-04-10', '15:28:04,282', ']', 'warn', '[', 'replicafetcher', 'replicaid=1001', ',', 'leaderid=1004', ',', 'fetcherid=4', ']', 'error', 'in', 'response', 'for', 'fetch', 'request', '(', 'type=fetchrequest', ',', 'replicaid=1001', ',', 'maxwait=2000', ',', 'minbytes=1', ',', 'maxbytes=10485760', ',', 'fetchdata=', '{', 'admin-request-topic-15=', '(', 'offset=0', ',', 'logstartoffset=0', ',', 'maxbytes=1048576', ',', 'currentleaderepoch=optional', '[', '4', ']', ')', '}', ',', 'isolationlevel=read_uncommitted', ',', 'toforget=', ',', 'metadata=', '(', 'sessionid=1345030285', ',', 'epoch=initial', ')', ')', '(', 'kafka.server.replicafetcherthread', ')', 'java.io.ioexception', ':', 'connection', 'to', '1004', 'was', 'disconnected', 'before', 'the', 'response', 'was', 'readat', 'org.apache.kafka.clients.networkclientutils.sendandreceive', '(', 'networkclientutils.java:97', ')', 'at', 'kafka.server.replicafetcherblockingsend.sendrequest', '(', 'replicafetcherblockingsend.scala:97', ')', 'at', 'kafka.server.replicafetcherthread.fetchfromleader', '(', 'replicafetcherthread.scala:190', ')', 'at', 'kafka.server.abstractfetcherthread.kafka', '$', 'server', '$', 'abstractfetcherthread', '$', '$', 'processfetchrequest', '(', 'abstractfetcherthread.scala:241', ')', 'at', 'kafka.server.abstractfetcherthread', '$', '$', 'anonfun', '$', 'maybefetch', '$', '1.apply', '(', 'abstractfetcherthread.scala:130', ')', 'at', 'kafka.server.abstractfetcherthread', '$', '$', 'anonfun', '$', 'maybefetch', '$', '1.apply', '(', 'abstractfetcherthread.scala:129', ')', 'at', 'scala.option.foreach', '(', 'option.scala:257', ')', 'at', 'kafka.server.abstractfetcherthread.maybefetch', '(', 'abstractfetcherthread.scala:129', ')', 'at', 'kafka.server.abstractfetcherthread.dowork', '(', 'abstractfetcherthread.scala:111', ')', 'at', 'kafka.utils.shutdownablethread.run', '(', 'shutdownablethread.scala:82', ')', 'kafka']),\n",
       "       list(['connectivity', 'issue', 'when', 'submit', 'job', 'to', 'livy', 'using', 'vmconnectivity', 'issue', 'when', 'submit', 'job', 'to', 'livy', 'using', 'vmspark']),\n",
       "       list(['container', 'exited', 'with', 'a', 'non-zero', 'exit', 'code', '50.', 'error', 'file', ':', 'prelaunch.err.econtainer', 'exited', 'with', 'a', 'non-zero', 'exit', 'code', '50.', 'error', 'file', ':', 'prelaunch.err.spark']),\n",
       "       list(['corrupted', 'kerberos', 'tickets', 'on', 'the', 'scaled', 'up', 'worker', 'nodesworker', 'nodes', 'following', 'keytabs', 'are', 'not', 'generated', 'properly', 'with', 'incorrect', 'file', 'size', ':', 'smokeuser.headless.keytabinteractive', 'query']),\n",
       "       list(['could', 'not', 'retrieve', 'cluster', 'information', 'from', 'clustererror', 'from', 'oozie', ':', 'could', 'not', 'retrieve', 'cluster', 'information', 'from', 'https', ':', '//rtg40prod.hdi.datalakefeeder.prd.euw.gbis.sg-azure.com/api/v1/clusters', '.', 'reason', ':', '{', \"'code\", \"'\", ':', \"'unauthorized\", \"'\", ',', \"'message\", \"'\", ':', \"'token\", 'acquisition', 'failed', \"'\", ',', \"'correlationid\", \"'\", ':', \"'818c61969d614b249b4e7d4c8c5bf431\", \"'\", ',', \"'responsetimestamp\", \"'\", ':', \"'2020-06-19t13:23:27.2116386z\", \"'\", '}', 'hadoop']),\n",
       "       list(['could', 'not', 'run', 'hive', 'queries', 'due', 'to', 'errorcould', 'not', 'run', 'hive', 'queries', 'due', 'to', 'errorhive']),\n",
       "       list(['could', 'not', 'use', 'python', '3', 'in', 'clustercould', 'not', 'use', 'python', '3', 'in', 'clusterspark']),\n",
       "       list(['create', 'fails', 'with', \"'deployment\", \"'hdinsight__2020-04-30t16.36.51.790z\", \"'\", 'xpm-hdi-rg', 'was', 'not', 'found', '.', \"'create\", 'fails', 'with', \"'deployment\", \"'hdinsight__2020-04-30t16.36.51.790z\", \"'\", 'xpm-hdi-rg', 'was', 'not', 'found', '.', \"'create\", 'failure', '-', 'other']),\n",
       "       list(['creation', 'faild', 'at', 'cluser', 'container', 'stepcreation', 'faild', 'at', 'cluser', 'container', 'stepcreate', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['creation', 'of', 'a', 'hdinsight', 'cluster', 'creation', 'with', 'hdi', '3.6', 'and', 'spark', '2.1', 'version', 'along', 'with', 'data', 'lake', 'gen2', 'is', 'being', 'a', 'problem120022523001923', '-', 'creation', 'of', 'an', 'hdinsight', 'cluster', 'creation', 'with', 'hdi', '3.6', 'and', 'spark', '2.1', 'version', 'along', 'with', 'data', 'lake', 'gen2', 'is', 'a', 'problem.create', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['creation', 'of', 'hdinsight', 'with', 'esp', 'failing,1', ':', 'user', 'is', 'unable', 'to', 'deploy', 'a', 'cluster', 'and', 'is', 'getting', 'an', 'error', 'message', 'that', 'reads', ':', '{', \"'code\", \"'\", ':', \"'deploymentfailed\", \"'\", ',', \"'message\", \"'\", ':', \"'at\", 'least', 'one', 'resource', 'deployment', 'operation', 'failed', '.', 'please', 'list', 'deployment', 'operations', 'for', 'details', '.', 'please', 'see', 'https', ':', '//aka.ms/deployoperations', 'for', 'usage', 'details.2', ':', 'username', 'in', 'ambari', 'showing', 'incorrectly', 'as', 'opposed', 'to', 'what', 'is', 'in', 'aads', '.', 'create', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['credential', 'reset', 'did', \"n't\", 'take', 'placecannot', 'ssh', 'into', 'clusterambari', 'in', 'standard', 'cluster']),\n",
       "       list(['credential', 'token', 'error', 'for', 'the', 'service', 'account', 'which', 'causing', 'failures', 'for', 'production', 'jobscredential', 'token', 'error', 'for', 'the', 'service', 'account', 'which', 'causing', 'failures', 'for', 'production', 'jobshadoop']),\n",
       "       list(['critical', 'alerts', 'on', 'ambari', 'on', 'head', 'node', '1', 'critical', 'alerts', 'on', 'ambari', 'on', 'head', 'node', '1spark']),\n",
       "       list(['critical', 'alerts', 'reported', 'in', 'ambari', 'critical', 'alerts', 'reported', 'in', 'ambarikafka']),\n",
       "       list(['critsit', '||', 'prem', '||', 'azure', 'hdinsight', 'service', '||', 'workernodes', 'are', 'being', 'compromised', '(', 'hdihdcatmanpsdev', 'and', 'hdihddemandforecastpsprod', ')', 'workernodes', 'are', 'being', 'compromised', '(', 'hdihdcatmanpsdev', 'and', 'hdihddemandforecastpsprod', ')', 'creating', 'and', 'managing', 'vpn', 'gateway', ',', 'connection', ',', 'and', 'routing']),\n",
       "       list(['daployment', 'failed', 'and', 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '.', 'daployment', 'failed', 'and', 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint.create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['dashboard', 'analytics', 'does', 'not', 'workdashboard', 'analytics', 'does', 'not', 'worklost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['data', 'injestion', 'lockexceptions', 'data', 'injestion', 'lockexceptions', ':', 'spark']),\n",
       "       list(['datalakecapabilitynotenabledforadladditionalfs', 'error', 'when', 'creating', 'hdi', 'cluster', 'with', 'adl', 'gen1', 'accessdatalakecapabilitynotenabledforadladditionalfs', 'error', 'when', 'creating', 'hdi', 'cluster', 'with', 'adl', 'gen1', 'accesscreate', 'failure', 'due', 'to', 'azure', 'policy']),\n",
       "       list(['datastore', 'access', 'issue', 'while', 'running', 'spark', 'action', 'through', 'ooziearchive', 'functionality', 'is', 'not', 'archiving', 'folder', 'and', 'subfolder', 'in', 'command', 'line', 'through', 'spark', 'submit', 'as', 'well', 'it', 'it', 'working', 'only', 'through', 'rest', 'api', 'call', '.', 'wasb', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['dead', 'data', 'nodeambari', 'agent', 'can', 'not', 'send', 'heartbeat', 'for', 'node', ':', 'wn8-cas-sp.1rfmscas145e5kc4xr2mhmegza.ax.internal.cloudapp.netspark']),\n",
       "       list(['deleted', 'hdfs', 'data', 'and', 'need', 'to', 'recover', 'deleted', 'hdfs', 'data', 'and', 'need', 'to', 'recovermapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['delta', 'as', 'a', 'format', 'is', 'not', 'working', 'in', 'the', 'clusterwhen', 'client', 'wants', 'to', 'use', 'the', 'package', 'io.delta', ':', 'delta-core_2.11:0.6.1', 'to', 'use', 'the', 'function:1', ')', 'val', 'somedf', '=', 'seq', '(', '(', '8', ',', '``', 'bat', \"''\", ')', ',', '(', '64', ',', '``', 'mouse', \"''\", ')', ',', '(', '-27', ',', '``', 'horse', \"''\", ')', ')', '.todf', '(', '``', 'number', \"''\", ',', '``', 'word', \"''\", ')', '2', ')', 'somedf.write.format', '(', '``', 'delta', \"''\", ')', '.mode', '(', '``', 'append', \"''\", ')', '.save', '(', '``', '<', 'path', '>', \"''\", ')', 'was', 'getting', ';', 'com.google.common.util.concurrent.executionerror', ':', 'java.lang.nosuchmethoderror', ':', 'com.fasterxml.jackson.module.scala.experimental.scalaobjectmapper.com', '$', 'fasterxml', '$', 'jackson', '$', 'module', '$', 'scala', '$', 'experimen', '...', '49', 'elidedcaused', 'by', ':', 'java.lang.nosuchmethoderror', ':', 'com.fasterxml.jackson.module.scala.experimental.scalaobjectmapper.com', '$', 'fasterxml', '$', 'jackson', '$', 'module', '$', 'scala', '$', 'experimental', '$', 'scalaobjectmapper', '$', '_setter_', '$', 'com', '$', 'fast', 'erxml', '$', 'jackson', '$', 'module', '$', 'scala', '$', 'experimental', '$', 'scalaobjectmapper', '$', '$', 'map_', '$', 'eq', '(', 'ljava/lang/class', ';', ')', 'vspark']),\n",
       "       list(['deployment', 'failed', '(', 'code', ':', 'internal', 'error', ')', 'deployment', 'failed', '(', 'code', ':', 'internal', 'error', ')', 'create', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['deployment', 'failed', 'for', 'hdinsight', 'kafka', 'clustercould', 'not', 'deploy', 'without', 'internet', 'accesscreate', 'failure', '-', 'other']),\n",
       "       list(['deployment', 'failed', 'with', 'an', 'internal', 'server', 'errorfailed', 'deploymentcreate', 'failure', '-', 'other']),\n",
       "       list(['deployment', 'failuredeployment', 'failureand', 'yarn', 'rm', 'ui', 'not', 'accessible', '.', 'create', 'failure', '-', 'other']),\n",
       "       list(['deploymentfailed', 'in', 'qa', 'and', 'prodhdinsight', 'deployment', 'failurescreate', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['different', 'issues', 'regarding', 'connection', 'to', 'sparkhistory', 'a', 'user', 'that', 'is', 'member', 'of', 'many', 'groups', 'in', 'ad', 'will', 'have', 'the', 'following', 'error', 'message', ':', 'bad', 'message', '431', ':', 'reason', ':', 'request', 'header', 'fields', 'too', 'large', '.', 'i', 'already', 'have', 'this', 'problem', 'for', 'hive', 'service', ',', 'to', 'fix', 'it', 'i', 'have', 'added', 'the', 'property', ':', 'hive.server2.thrift.http.request.header.size', '=', '64000', 'and', 'hive.server2.thrift.http.response.header.size=64000', '.', 'i', \"'m\", 'not', 'sure', 'of', 'what', 'i', 'have', 'to', 'add', 'in', 'order', 'to', 'fix', 'it', 'for', 'sparkhistory', '-', 'the', 'secod', 'scenario', 'is', 'that', 'the', 'user', 'has', \"n't\", 'a', 'big', 'header', 'field', ',', 'but', 'has', \"n't\", 'authorization', ',', 'the', 'error', 'message', 'is', 'the', 'following', 'one', ':', 'http', 'error', '403', 'problem', 'accessing', '/history/application_1591273077058_0001/1/jobs/', '.', 'reason', ':', 'user', 'is', 'not', 'authorized', 'to', 'access', 'this', 'page.ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['disk', 'usage', 'fulldisk', 'usage', 'fullspark']),\n",
       "       list(['dns', 'resolution', 'of', 'headnode', 'with', 'windows', 'box', 'in', 'subnetcustomer', 'reported', 'that', 'their', 'hybrid', 'automation', 'runbook', 'was', 'failing', 'because', 'part', 'of', 'their', 'script', 'executed', 'an', 'ssh', 'connection', 'with', 'the', 'cluster', 'but', 'this', 'was', 'not', 'succeeding.hadoop']),\n",
       "       list(['do', \"n't\", 'have', 'access', 'to', 'yarn', 'apiissues', 'with', 'api', 'from', 'custom', 'gatewayranger', 'policy', 'auditing']),\n",
       "       list(['edge', 'is', 'not', 'is', 'not', 'respondingunable', 'to', 'ssh', 'into', 'edge', 'node.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['edge', 'node', 'heartbeat', 'lost.edgenode', 'lost', 'heartbeatnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['edge', 'node', 'issueedge', 'node', 'issueissue', 'with', 'scaling', 'down']),\n",
       "       list(['edge', 'node', 'root', 'filesystem', 'sizeedge', 'node', 'root', 'filesystem', 'sizeodbc', 'or', 'jdbc']),\n",
       "       list(['edge', 'node', 'unreachableunable', 'to', 'ssh', 'and', 'all', 'looks', 'good', 'in', 'ambari', 'ui', 'for', 'this', 'hostnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['edge', 'nodes', 'were', 'removed', 'from', 'amabari', 'but', 'still', 'available', 'in', 'azure', '.', 'please', 'remove.120030224005315', '-', 'edge', 'nodes', 'were', 'removed', 'from', 'ambari', 'but', 'still', 'available', 'in', 'azure', '.', 'please', 'remove.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['enable', 'ssl', 'for', 'hive', 'connectionjdbc', 'hive', 'connection', 'string', 'not', 'working', 'to', 'enable', 'ssl', 'for', 'hive', 'and', 'qlikview', 'connectivty', 'guidanceodbc', 'or', 'jdbc']),\n",
       "       list(['enable', 'ssl', 'need', 'help', 'in', 'setting', 'up', 'kafka', 'sslkafka']),\n",
       "       list(['enabling', 'azure', 'monitor', 'does', 'not', 'log', 'data', 'to', 'sparkapplication_stats_xxxx_cl', 'tables', 'in', 'log', 'analytics', 'workspace', '(', 'maxplatformprod', ')', 'enabling', 'azure', 'monitor', 'does', 'not', 'log', 'data', 'to', 'sparkapplication_stats_xxxx_cl', 'tables', 'in', 'log', 'analytics', 'workspaceazure', 'log', 'analytics', 'integration']),\n",
       "       list(['erroe', 'while', 'creating', 'hdinsight', 'clusterunable', 'to', 'create', 'hdinsight', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['error', '502.3', 'on', 'accessing', 'ambari', 'on', 'bdqclustererror', '502.3', 'on', 'accessing', 'ambari', 'on', 'bdqclusterhadoop']),\n",
       "       list(['error', ':', 'connection', 'reset', 'by', 'peer', ':', 'socket', 'write', 'errorissue', ':', 'error', ':', 'connection', 'reset', 'by', 'peer', ':', 'socket', 'write', 'error', 'while', 'connecting', 'to', 'hdinsight', 'from', 'third', 'party', 'clients', 'such', 'as', 'dbeaver', 'and', 'sisensehive']),\n",
       "       list(['error', 'could', 'not', 'connect', 'to', 'metastore', 'with', 'the', 'given', 'credentials', 'when', 'creating', 'clustercannot', 'deploy', 'esp', 'cluster', 'with', 'ambari', 'dbcreate', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['error', 'creating', 'the', 'clustererror', 'creating', 'the', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['error', 'deploying', 'ml', 'service', 'on', 'hd', 'insight120071021001708', '-', 'error', 'deploying', 'ml', 'service', 'on', 'hdinsight', 'servicevalidation', 'error', ':', 'hdi', \"version'3.6\", \"'\", 'is', 'not', 'supported', 'for', 'clustertype', \"'mlservices\", \"'\", 'and', 'componentversion', \"'default'.create\", 'failure', '-', 'other']),\n",
       "       list(['error', 'during', 'hive', 'joberror', 'during', 'hive', 'jobhive']),\n",
       "       list(['error', 'fetching', 'data', 'from', 'adls', 'gen1issue', ':', 'error', 'fetching', 'data', 'from', 'adls', 'gen1symptom', ':', 'error', 'while', 'running', 'simple', 'hdfs', 'move', 'command.error', ':', 'status', ':', 'failederror', ':', 'vertex', 'failed', ',', 'vertexname=map', '4', ',', 'vertexid=vertex_1592601459200_8651_1_02', ',', 'diagnostics=', '[', 'vertex', 'vertex_1592601459200_8651_1_0\\\\2', '[', 'map', '4', ']', 'killed/failed', 'due', 'to', ':', 'root_input_init_failure', ',', 'vertex', 'input', ':', 'mfstore', 'initializer', 'failed', ',', 'vertex=vertex_1592601459200_8651_\\\\1_02', '[', 'map', '4', ']', ',', 'java.lang.runtimeexception', ':', 'serious', 'problemhive']),\n",
       "       list(['error', 'getting', 'while', 'accessing', 'tokenmanager', 'end', 'pointthrows', 'gss', 'excepttion', 'when', 'running', 'for', 'ooziecaused', 'by', ':', 'org.apache.hadoop.security.authentication.client.authenticationexception', ':', 'gssexception', ':', 'no', 'valid', 'credentials', 'provided', '(', 'mechanism', 'level', ':', 'failed', 'to', 'find', 'any', 'kerberos', 'tgt', ')', 'ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['error', 'in', 'adls', 'gen2', 'writing', 'spark', 'checkpointerror', 'in', 'adls', 'gen2', 'writing', 'spark', 'checkpointspark']),\n",
       "       list(['error', 'in', 'group', 'sync', 'to', 'ambariuser', 'groups', 'are', 'not', 'synching', 'to', 'ambariambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['error', 'in', 'kafkakafka', 'connection', 'issueskafka']),\n",
       "       list(['error', 'in', 'opening', 'sparkhistory', 'uierror', 'in', 'opening', 'sparkhistory', 'ui.spark']),\n",
       "       list(['error', 'message', \"'\", 'can', 'not', 'allocate', 'memory', '’', 'from', 'kafka', 'node', 'wn2-mazcac.prd-ebu01.cc.az.rci.rogers.com120041424004372', '-', 'error', 'message', \"'\", 'can', 'not', 'allocate', 'memory', \"'\", 'from', 'kafka', 'node', 'wn2-mazcac.prd-ebu01.cc.az.rci.rogers.comnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['error', 'message', ':', 'existing', 'domain', 'cvsadds.com', 'could', 'not', 'be', 'foundexisting', 'domain', 'cvsadds.com', 'could', 'not', 'be', 'foundcreate', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['error', 'rescaling', 'cluster', 'failed', 'to', 'scale', 'the', 'hdinsight', 'cluster', 'twoprocesshistorical.symptom', ':', 'when', 'scaling', 'up', 'this', 'action', 'failsissue', 'with', 'scaling', 'up']),\n",
       "       list(['error', 'when', 'creating', 'a', 'new', 'hdi', 'clustererror', 'when', 'creating', 'a', 'new', 'hdi', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['error', 'when', 'executing', 'spark', 'job', ':', 'org.apache.hadoop.hive.ql.exec.movetask120040221002365', '-', 'error', 'when', 'executing', 'spark', 'job', ':', 'org.apache.hadoop.hive.ql.exec.movetask', 'hive']),\n",
       "       list(['error', 'while', 'accessing', 'hive', 'tables', 'from', 'odbc', 'connection', 'from', 'excelerror', 'while', 'accessing', 'hive', 'tables', 'from', 'odbc', 'connection', 'from', 'excelodbc', 'or', 'jdbc']),\n",
       "       list(['error', 'while', 'create', 'spark', 'jupyter', 'notebookerror', 'while', 'create', 'spark', 'jupyter', 'notebook.notebooks']),\n",
       "       list(['error', 'while', 'performing', 'schema', 'merge', 'of', '2', 'json/parquet', 'files120041524003479', '-', 'error', 'while', 'performing', 'schema', 'merge', 'of', '2', 'json/parquet', 'files', 'hdinsight', 'service', '.', 'error', ':', 'failed', 'to', 'merge', 'incompatible', 'data', 'types', 'stringtype', 'and', 'arraytype', '(', 'stringtype', ',', 'true', ')', 'hive']),\n",
       "       list(['error', 'while', 'unzip', 'and', 'copying', 'data', 'using', 'hadoop', 'commandrror', 'while', 'unzip', 'and', 'copying', 'data', 'using', 'hadoop', 'commandwasb', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['exception', 'operation', 'failed', 'in', 'logs', 'and', 'log', 'files', 'stay', 'inprogressissue', ':', 'exception', '(', '403', ')', 'operation', 'failed', 'in', 'logs', 'and', 'log', 'files', 'stay', 'inprogress', 'in', 'spark2', 'events', 'logs.spark']),\n",
       "       list(['exception', 'seen', 'while', 'running', 'spark', 'job', '(', 'spark', '2.4', ',', 'hdi', '4.0', ')', 'below', 'error', 'while', 'running', 'your', 'spark', 'job.java.lang.illegalaccesserror', ':', 'class', 'org.apache.hadoop.hdfs.web.hftpfilesystem', 'can', 'not', 'access', 'its', 'superinterface', 'org.apache.hadoop.hdfs.web.tokenaspect', '$', 'tokenmanagementdelegatorspark']),\n",
       "       list(['expired', 'password', 'for', 'aad', 'admin', 'user', 'for', 'hd', 'insights', 'with', 'espexpired', 'password', 'for', 'aad', 'admin', 'user', 'for', 'hd', 'insights', 'with', 'espambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['exporting', 'hdi', 'cluster', 'tez', 'view', 'logsadvisoryhadoop']),\n",
       "       list(['external', 'metastore', 'requires', 'public', 'access', 'to', 'be', 'enabledexternal', 'metastore', 'requires', 'public', 'access', 'to', 'be', 'enabledcreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['facing', 'multiple', 'issues', 'on', 'the', 'clusterunable', 'to', 'access', 'ambari', 'uinode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['fail', 'hdinsight', '“', 'error', 'deleting', 'cluster', ':', 'hdi001prd', '{', \"'code\", \"'\", ':', \"'conflict\", \"'\", ',', \"'message\", \"'\", ':', \"'exception\", 'of', 'type', \"'microsoft.clusterservices.rdfeprovider.resourcetypes.models.rdferesourcehandlerexception\", \"'\", 'was', 'thrown', '.', \"'\", '}', '”', '.issue', 'with', 'autoscaling']),\n",
       "       list(['fail', 'to', 'deserialize', 'avro', 'files', 'with', 'spark', 'sqlfail', 'to', 'deserialize', 'avro', 'files', 'with', 'spark', 'sqlspark']),\n",
       "       list(['failed', 'during', 'upscalescaling', 'failedissue', 'with', 'scaling', 'up']),\n",
       "       list(['failed', 'to', 'connect', 'to', 'the', 'cluster', 'to', 'submit', 'jobsfailed', 'to', 'connect', 'to', 'the', 'cluster', 'to', 'submit', 'jobsspark']),\n",
       "       list(['failed', 'to', 'enable/disable', 'auto-scalefailed', 'to', 'enable/disable', 'auto-scaleissue', 'with', 'autoscaling']),\n",
       "       list(['failed', 'to', 'submit', 'jobfailed', 'to', 'submit', 'job.hive']),\n",
       "       list(['failed', 'to', 'submit', 'spark', 'job', '-', 'bad', 'gateway', 'errorlivy', 'folders', 'are', 'missing', ',', 'spark']),\n",
       "       list(['failed', 'to', 'submit', 'spark', 'jobfailed', 'to', 'submit', 'spark', 'jobspark']),\n",
       "       list(['failing', 'to', 'create', 'hdifailed', 'to', 'validate', 'storage', 'blob', 'account.create', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['false', 'alert', 'of', 'dead', 'regionfalse', 'alert', 'of', 'dead', 'regionhbase']),\n",
       "       list(['fd', 'qa', '-', 'kpq101llapfdqawus201', '-', 'node', 'unresponsive', 'fd', 'qa', '-', 'kpq101llapfdqawus201', '-', 'node', 'unresponsivenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['fd', 'qa', ':', 'kp05hbasefdhdiqausc01', ':', 'ambari', 'agent', 'heartbeat', 'issue', 'on', 'ed21ambari', 'agent', 'heartbeat', 'issue', 'on', 'ed21unable', 'to', 'ssh']),\n",
       "       list(['fd', 'qa', ':', 'kpphv806llapfdqausc01', ':', 'analyze', 'and', 'querying', 'from', 'hive', 'view', 'failing', 'for', 'table', 'insght_mtrc', '&', 'insght_prdctn.fd', 'qa', ':', 'kpphv806llapfdqausc01', ':', 'analyze', 'and', 'querying', 'from', 'hive', 'view', 'failing', 'for', 'table', 'insght_mtrc', '&', 'insght_prdctn.interactive', 'query']),\n",
       "       list(['fd', 'qa', ':', 'kpq049llapfdqawus201', ':', 'commands', 'failing', 'on', 'llap', 'hdi', '4.0', 'zeppelin', 'ui', 'with', 'error', \"'unable\", 'to', 'read', 'hiveserver2', 'configs', 'from', 'zookeeper', \"'\", 'commands', 'failing', 'on', 'llap', 'hdi', '4.0', 'zeppelin', 'ui', 'with', 'error', \"'unable\", 'to', 'read', 'hiveserver2', 'configs', 'from', \"zookeeper'notebooks\"]),\n",
       "       list(['fd', 'sand', ':', 'kps024llapfdsbwus401', ':', ':', 'the', 'his', 'service', 'is', 'stopped', 'in', 'the', 'cluster.unable', 'to', 'restart', 'the', 'llap', 'serviceinteractive', 'query']),\n",
       "       list(['fd', 'sand', ':', 'sandbox', 'cluster', ':', 'kpphv704llapncapfdsbusc01', ':', 'cluster', 'scaled', 'up', 'however', 'softwares', 'are', 'pending', 'statefd', 'sand', ':', 'sandbox', 'cluster', ':', 'kpphv704llapncapfdsbusc01', ':', 'cluster', 'scaled', 'up', 'however', 'softwares', 'are', 'pending', 'statehive']),\n",
       "       list(['fd', 'sandbox', '-', 'kps023sparkespfdsbwus401', '-', 'accessing', 'adls', 'container', 'from', 'secure', 'clusterkps023sparkespfdsbwus401', '-', 'accessing', 'adls', 'container', 'from', 'secure', 'clusteradls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['fd', 'sandbox', '-', 'kps025sparkfdsbwus401', '-', 'hive', 'connections', 'downfd', 'sandbox', '-', 'kps025sparkfdsbwus401', '-', 'hive', 'connections', 'downbeeline']),\n",
       "       list(['fd', 'sandbox', '-', 'kps027sparkespfdsbwus201', '-', 'hiveserver2', 'tasksunable', 'to', 'start', 'llapinteractive', 'query']),\n",
       "       list(['fd', 'sandbox', '-', 'kps071hbasefdsbwus201', '-', 'hive', 'connections', 'not', 'availablefd', 'sandbox', '-', 'kps071hbasefdsbwus201', '-', 'hive', 'connections', 'not', 'availablebeeline']),\n",
       "       list(['fd', 'sandbox', '-', 'kps106llapfdsbwus201', '-', 'cluster', 'not', 'coming', 'upcould', 'not', 'login', 'to', 'cluster', 'with', 'service', 'accounts', ',', 'create', 'failure', '-', 'other']),\n",
       "       list(['fd', 'sandbox', ':', 'kps023sparkespfdsbwus401', ':', 'hive', 'service', 'interactive', 'downhive', 'interactive', 'down', 'issues', 'on', 'spark', 'hdi', '4.0', 'esp', 'clustersinteractive', 'query']),\n",
       "       list(['fd', 'sb', 'hdi4', ':', 'kps023sparkespfdsbwus401', ':', 'not', 'able', 'to', 'access', 'to', 'edge', 'node', '10.10.196.223', 'issue', ':', 'edgenode1', 'not', 'accessable', 'to', 'ssh', '.', 'spark']),\n",
       "       list(['filesystem', 'is', 'in', 'read-only', 'mode', 'symptom', ':', 'edgenode', 'was', 'not', 'coming', 'upnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['folders', 'in', 'head', 'node', 'disappearedfolders', 'in', 'head', 'node', 'disappearedspark']),\n",
       "       list(['frequent', 'hive', 'metastore', 'alerts', 'and', 'the', 'service', 'issuehivemetastore', 'alert', 'on', 'hn0', 'host', 'and', 'unable', 'to', 'estabish', 'communication', 'with', 'port', '9083interactive', 'query']),\n",
       "       list(['gateway-related', 'errors', 'have', 'caused', 'some', 'spark', 'jobs', 'to', 'not', 'be', 'submittedgateway-related', 'errors', 'have', 'caused', 'some', 'spark', 'jobs', 'to', 'not', 'be', 'submittedspark']),\n",
       "       list(['general', 'question', 'related', 'to', 'the', 'hdi', 'esp', 'clustersgeneral', 'question', 'about', 'ldap', 'certificate', 'renewalspark']),\n",
       "       list(['getting', '502.3', '-', 'bad', 'gateway', 'errorslegetting', '502.3', '-', 'bad', 'gateway', 'errorsspark']),\n",
       "       list(['getting', 'a', 'bunch', 'of', 'heartbeat', 'alertgetting', 'a', 'bunch', 'of', 'heartbeat', 'alertkafka']),\n",
       "       list(['getting', 'an', \"'internal\", 'error', \"'\", 'every', 'time', 'i', 'try', 'to', 'deploy', 'this', 'cluster', '.', 'i', 'have', 'tried', 'deleting', 'the', 'cluster', 'and', 'redeploying', 'it', ',', 'with', 'no', 'luck.getting', 'an', \"'internal\", 'error', \"'\", 'every', 'time', 'i', 'try', 'to', 'deploy', 'this', 'cluster', '.', 'i', 'have', 'tried', 'deleting', 'the', 'cluster', 'and', 'redeploying', 'it', ',', 'with', 'no', 'luck.spark']),\n",
       "       list(['getting', 'an', 'exception', 'while', 'trying', 'to', 'submit', 'a', 'pigscript', 'to', 'the', 'hdinsight', 'clustergetting', 'an', 'exception', 'while', 'trying', 'to', 'submit', 'a', 'pigscript', 'to', 'the', 'hdinsight', 'clustermapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['getting', 'error', 'in', 'hive', 'meta', 'store', 'loguser', \"wasn't\", 'authorized', 'to', 'submits', 'jobshive']),\n",
       "       list(['getting', 'error', 'while', 'accessing', 'gen2', 'account', 'from', 'hdi4.0getting', 'error', 'while', 'accessing', 'gen2', 'account', 'from', 'hdi4.0hdinsight', 'sdk']),\n",
       "       list(['getting', 'error', 'while', 'running', 'oozie', 'jobgetting', 'error', 'while', 'running', 'oozie', 'jobmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['getting', 'list', 'of', 'hosts', 'to', 'be', 'customized', 'is', 'empty', 'getting', 'list', 'of', 'hosts', 'to', 'be', 'customized', 'is', 'emptykafka']),\n",
       "       list(['guidance', 'on', 'installing', 'kafka', 'connectguidance', 'on', 'installing', 'kafka', 'connectkafka']),\n",
       "       list(['hadoop', 'space', '/usr/hdp', 'is', 'getting', 'full', 'based', 'on', 'troubleshoot', 'we', 'found', 'that', 'we', 'are', 'hitting', 'bug', 'https', ':', '//issues.apache.org/jira/browse/hive-20824', 'as', 'the', 'support', 'for', 'the', '“', 'spark', '2.1', ',', '2.2', '&', 'kafka', '1.0', 'support', 'will', 'expire', 'on', 'june', '30', ',', '2020.', '”', 'is', 'going', 'to', 'expire', 'soon', '.', 'after', 'discussing', 'with', 'your', 'internal', 'team', 'you', 'have', 'planned', 'to', 'upgrade', 'talend', 'etl', 'which', 'is', 'depend', 'of', 'spark', 'version', 'to', 'higher', 'version', 'so', 'that', 'going', 'forward', 'it', 'will', 'support', 'higher', 'version', 'of', 'spark', '.', 'as', 'a', 'workaround', ',', 'you', 'will', 'be', 'writing', 'the', 'script', 'which', 'will', 'monitor', 'the', '‘', '/usr/hdp', '’', 'disk', 'space', 'and', 'restart', 'the', 'hiveserver2', 'service', 'when', 'no', 'jobs', 'are', 'running', 'to', 'release', 'the', 'files', 'marked', 'for', 'deletion', 'because', 'of', 'bug', 'because', 'roll', 'back', 'of', 'fix', 'to', 'lower', 'version', 'will', 'take', 'sometime', '(', 'fixing', ',', 'testing', ',', 'validation', 'would', 'take', 'more', 'time', ')', '.hive']),\n",
       "       list(['having', 'issue', 'to', 'create', 'hdi', 'cluster', 'in', 'east', 'asiahaving', 'issue', 'to', 'create', 'hdi', 'cluster', 'in', 'east', 'asia.create', 'failure', '-', 'other']),\n",
       "       list(['having', 'issues', 'creating', 'clustergetting', 'a', '``', 'deployment', 'error', \"''\", 'and', 'a', 'gateway', 'timeout', 'error', 'create', 'failure', '-', 'other']),\n",
       "       list(['having', 'problem', 'with', 'long', 'running', 'queries', ',', 'up', 'scaling', 'taking', 'long', 'time', 'and', 'unable', 'to', 'open', 'hive', 'view', ',', 'tez', 'view', 'giving', 'error.monitoring', 'cluster', 'we', 'can', 'observe', 'jobs', 'that', 'consume', 'all', 'resources', 'and', 'take', 'a', 'long', 'time', 'to', 'complete.hive']),\n",
       "       list(['hbase', ':', 'data', 'load', 'and', 'container', 'allocation', 'issueshbase', ':', 'data', 'load', 'and', 'container', 'allocation', 'issueshbase']),\n",
       "       list(['hbase', ':', 'retriesexhaustedwithdetailsexception', ';', 'cdc', 'is', 'failinghbase', ':', 'retriesexhaustedwithdetailsexception', ';', 'cdc', 'is', 'failinghbase']),\n",
       "       list(['hbase', 'cluster', 'getting', 'restarted', 'during', 'region', 'node', 'scaleup', 'activitycomplete', 'hbase', 'cluster', 'gets', 'restarted', 'during', 'scaleup', 'and', 'scale', 'down', 'activity.issue', 'with', 'scaling', 'up']),\n",
       "       list(['hbase', 'import', 'are', 'failing', 'with', 'multiple', 'exceptionshbase', 'import', 'are', 'failing', 'with', 'multiple', 'exceptionshbase']),\n",
       "       list(['hbase', 'service', 'downhbase', 'service', 'downhbase']),\n",
       "       list(['hbase', 'traces', 'in', 'the', 'logs', 'and', 'service', 'not', 'enabledhbase', 'traces', 'in', 'the', 'logs', 'and', 'service', 'not', 'enabledhbase']),\n",
       "       list(['hdfs', 'access', 'not', 'working', 'on', 'hdinsight', 'with', 'esp', 'activecannot', 'run', 'hdfs', 'cmdsadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['hdfs', 'fs', 'is', 'filling', 'up', 'faster', 'than', 'it', 'shouldambari', 'is', 'reporting', 'that', 'the', 'hdfs', 'file', 'system', 'is', '45', '%', 'full', 'but', 'the', 'processed', 'data', 'is', 'only', 'about', '8tb', 'even', 'when', 'the', 'blob', 'storagethat', 'is', 'connected', 'is', 'no', 'where', 'near', 'its', '5pb*', 'capacityhbase']),\n",
       "       list(['hdi', '4', ':', 'hbase', 'sclae', 'is', 'not', 'working', 'user', 'need', 'guidance', 'in', 'scaleup/scaledownissue', 'with', 'scaling', 'up']),\n",
       "       list(['hdi', '4.0', ':', 'hdi', '4', 'hive', 'warehouse', 'connector', 'issueserror', 'in', 'acquiring', 'locks', 'with', 'hwcspark']),\n",
       "       list(['hdi', '4.0', ':', 'spark', ':', 'scale', 'up', 'operation', 'is', 'failing', 'through', 'portalitlehdi', '4.0', ':', 'spark', ':', 'scale', 'up', 'operation', 'is', 'failing', 'through', 'portalissue', 'with', 'scaling', 'up']),\n",
       "       list(['hdi', 'ambari', 'is', 'trying', 'to', 'start', 'for', 'every', 'minute', 'perf', 'issues', 'with', 'ambari', 'dbhadoop']),\n",
       "       list(['hdi', 'deploy', 'with', 'azure', 'cli', 'and', 'gen2', 'adlshdi', 'deploy', 'with', 'azure', 'cli', 'and', 'gen2', 'adlscreate', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['hdi', 'is', 'getting', 'provisioned', 'with', 'incorrect', 'size', 'worker', 'nodes120042222002739', 'hdi', 'is', 'getting', 'provisioned', 'with', 'incorrect', 'size', 'worker', 'nodescreate', 'failure', '-', 'other']),\n",
       "       list(['hdi', 'kafka', 'worker', 'node', 'hostnames', 'not', 'resolvablethe', 'names', 'of', 'the', 'workers', 'where', 'kafka', 'brokers', 'were', 'hosted', 'were', 'not', 'able', 'to', 'be', 'resolved', 'by', 'customer', \"'s\", 'custom', 'dns', 'serverkafka']),\n",
       "       list(['hdi', 'services', 'not', 'coming', 'uphdi', 'services', 'not', 'coming', 'upinteractive', 'query']),\n",
       "       list(['hdi', 'serviuce', 'pathing', 'and', 'updatinghdi', 'service', 'pathing', 'and', 'updatingnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['hdi', 'spark', 'is', 'down', 'and', 'hn0', 'is', 'unresponsivehdi', 'spark', 'is', 'down', 'and', 'hn0', 'is', 'unresponsive.spark']),\n",
       "       list(['hdi', 'versionissue', ':', 'hdi', 'version', ':', 'we', 'deployed', 'two', 'new', 'spark', 'hdinsight', 'clusters', 'recently', ',', 'one', '(', 'hdi3.6', 'and', 'spark2.3', ')', 'was', 'created', 'on', '12/23/2020', 'in', 'our', 'daily', 'subscriptioin', 'and', 'other', '(', 'hdi3.6', 'and', 'spark2.3', ')', 'was', 'created', 'on', '12/26/2020', 'in', 'our', 'staging', 'subscription', '.', 'in', 'these', 'two', 'new', 'clusters', ',', 'the', 'spark', 'version', 'is', '2.3.2.', 'but', 'when', 'we', 'deployed', 'same', 'spark', 'hdinsight', 'cluster', 'in', 'our', 'production', 'subscription', 'on', '01/16/2020', ',', 'we', 'found', 'that', 'spark', 'version', 'was', 'changed', 'back', 'to', '2.3.0.', 'since', 'we', 'have', 'already', 'made', 'change', 'to', 'support', 'spark', '2.3.2', 'on', 'daily', 'and', 'staging', ',', 'we', 'wondered', 'if', 'this', 'could', 'affect', 'our', 'spark', 'apps', 'on', 'production', 'cluster', '.', 'checking', 'azure', 'hdinsight', 'release', 'history', ',', 'there', 'are', 'two', 'recent', 'releases', 'for', 'hdi3.6', 'that', 'were', 'on', '01/09/2020', 'and', '12/17/2019', ',', 'what', 'is', 'change', 'between', 'these', 'two', 'released', '?', 'is', 'spark', 'version', 'change', 'as', 'expected', '?', 'spark']),\n",
       "       list(['hdinsight', '3.6', 'retirementsupport', 'for', 'hdinsight', '3.6', 'expires', '2021-06-30create', 'failure', '-', 'other']),\n",
       "       list(['hdinsight', '4.0', '&', 'juptyerhdinsight', '4.0', '&', 'jupyter.notebooks']),\n",
       "       list(['hdinsight', 'accessing', '{', 'for', 'read', '}', 'adls', 'gen2', 'storagehdinsight', 'accessing', '{', 'for', 'read', '}', 'adls', 'gen2', 'storageadls', 'gen1', ',', 'adls', 'gen2', 'in', 'standard', 'cluster']),\n",
       "       list(['hdinsight', 'cluster', 'appears', 'to', 'be', 'down', 'with', 'multple', 'issues120042024005702', '-', 'hdinsight', 'cluster', 'appears', 'to', 'be', 'down', 'with', 'multiple', 'issues', 'spark']),\n",
       "       list(['hdinsight', 'cluster', 'azure', 'monitor', 'integration', 'not', 'workinghdinsight', 'cluster', 'azure', 'monitor', 'integration', 'not', 'workingazure', 'log', 'analytics', 'integration']),\n",
       "       list(['hdinsight', 'cluster', 'downhdinsight', 'cluster', 'down', '-', 'kafka', 'broker', 'node', 'stopped', 'working', 'and', 'connection', 'failure', 'onto', 'worker', 'nodesnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['hdinsight', 'cluster', 'upgradeadvisorycreate', 'failure', '-', 'other']),\n",
       "       list(['hdinsight', 'firewall', 'ruleshdinsight', 'firewall', 'rulescreate', 'failure', '-', 'other']),\n",
       "       list(['hdinsight', 'kafka', 'having', 'network', 'issue', 'with', 'golden', 'gate', 'application120051124003292', 'hdinsight', 'kafka', 'having', 'network', 'issue', 'with', 'golden', 'gate', 'application', 'hdinsight', 'servicekafka']),\n",
       "       list(['hdinsight', 'script', 'action', 'fails', ',', 'unable', 'to', 'find', 'uri', 'location', ',', 'but', 'the', 'associated', 'managed', 'identity', 'has', 'blob', 'owner', 'on', 'the', 'storage', 'accounthd', 'insight', 'fails', 'to', 'execute', 'script', 'action', 'from', 'a', 'script', 'file', 'in', 'blob', 'storageissues', 'using', 'access', 'control', 'lists', '(', 'acls', ')']),\n",
       "       list(['hdinsight', 'script', 'actioncx', 'received', 'an', '``', 'invalidscriptlocation', \"''\", 'error', 'while', 'trying', 'to', 'submit', 'a', 'custom', 'hdi', 'script', 'actionissues', 'using', 'azure', 'ad', '(', 'rbac', '&', 'oauth', ')']),\n",
       "       list(['hdinsight', 'with', 'premium', 'blov', 'storagelooks', 'like', ',', 'premium', 'storage', 'account', 'is', 'unsupported', 'for', 'the', 'hdi', 'cluster', ',', 'https', ':', '//docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-compare-storage-optionsand', 'i', 'also', 'see', 'the', 'premium', 'storage', 'account', 'is', 'in', 'preview', 'as', 'per', 'the', 'above', 'link', 'and', 'need', 'to', 'check', 'on', 'it', '...', 'create', 'failure', '-', 'other']),\n",
       "       list(['hdinsightwatchdog', 'stopping', 'yarn', 'serviceshdinsightwatchdog', 'stopping', 'yarn', 'serviceshadoop']),\n",
       "       list(['head', 'node', '0', 'is', 'unhealthy', 'with', 'multiple', 'alertshead', 'node', '0', 'is', 'unhealthy', 'with', 'multiple', 'alertshadoop']),\n",
       "       list(['head', 'node', 'connection', 'failure', 'issuefailed', 'to', 'connect', 'to', 'hn0-cmipro.addscummins.com', 'port', '30070', ':', 'connection', 'refused', '000', ')', '.hadoop']),\n",
       "       list(['head', 'node', 'disk', 'space', 'fulldue', 'to', 'low', 'disk', 'space', ',', 'you', 'were', 'not', 'able', 'to', 'execute', 'jupyter', 'jobnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['head', 'node', 'issue', '[', 'org.apache.hadoop.fs.azure.azureexception', ':', 'com.microsoft.azure.storage.storageexception', ':', 'the', 'account', 'being', 'accessed', 'does', 'not', 'support', 'http.', ']', ']', '.', ']', 'hadoop']),\n",
       "       list(['head', 'nodes', 'are', 'in', 'failed', 'state.issue', ':', 'headnodes', 'for', 'the', 'kafka', 'cluster', 'in', 'a', 'bad', 'state', '.', 'hn0', 'was', 'unresponsive', 'and', 'hn1', 'was', 'running', 'fine', 'but', 'ambari-agent', 'was', 'utilizing', 'more', 'cpu.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['head', 'nodes', 'are', 'node', 'accessible', 'and', 'the', 'cluster', 'is', 'hung.head', 'nodes', 'are', 'node', 'accessible', 'and', 'the', 'cluster', 'is', 'hung.hbase']),\n",
       "       list(['head', 'nodes', 'not', 'reachable.hn1', 'not', 'reachable', 'via', 'ssh', '.', 'lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['headnode', 'is', 'downheadnode', 'down', 'node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['headnode0', 'rebooted', 'over', 'the', 'weekend', 'caused', 'bunch', 'of', 'production', 'jobs', 'failed120042724004133', '-', 'headnode0', 'rebooted', 'over', 'the', 'weekend', 'caused', 'bunch', 'of', 'production', 'jobs', 'failed', 'hdinsight', 'serviceadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['health', 'of', 'cluster', '-', 'heartbeat', 'lost', 'on', 'all', 'nodeshealth', 'of', 'cluster', '-', 'heartbeat', 'lost', 'on', 'all', 'nodes.hadoop']),\n",
       "       list(['heartbeat', 'isuses', 'and', 'hosts', 'information', 'missing', 'in', 'ambari', 'uiambari-server', 'stale', 'pid', 'persitinteractive', 'query']),\n",
       "       list(['heartbeat', 'lost', ',', 'headnode', 'downsymptom', ':', 'heart', 'beat', 'lost', 'and', 'head', 'node', '(', 'hn0', ')', 'was', 'un', 'accessible', '.several', 'cluster', 'services', 'have', 'the', 'heartbeat', 'lost', 'issue', 'related', 'to', 'ambari', 'server', 'alert', '(', 'hn0', ')', '.manually', 'not', 'able', 'to', 'restart', 'ambari', 'server', 'and', 'ambari', 'agent.hive']),\n",
       "       list(['heartbeat', 'lost', 'on', 'wn29', '&', 'wn57heartbeat', 'lost', 'on', 'wn29', '&', 'wn57spark']),\n",
       "       list(['heartbeat', 'lostheart', 'beat', 'lost', 'and', 'head', 'node', '(', 'hn0', ')', 'was', 'un', 'accessible', '.several', 'cluster', 'services', 'have', 'the', 'heartbeat', 'lost', 'issue', 'related', 'to', 'ambari', 'server', 'alert', '(', 'hn0', ')', '.hive']),\n",
       "       list(['heartbeat', 'missing', 'on', 'worker', 'node', 'heartbeat', 'missing', 'on', 'worker', 'nodelost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['hi', 'team', ',', 'we', 'have', 'observed', 'one', 'of', 'the', 'data', 'node', 'is', 'missing', 'hearbeat', 'since', '33', 'mins', 'ago', 'as', 'displayed..hi', 'team', ',', 'we', 'have', 'observed', 'one', 'of', 'the', 'data', 'node', 'is', 'missing', 'hearbeat', 'since', '33', 'mins', 'ago', 'as', 'displayed..node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['high', 'inserts', 'in', 'hbase', 'symptom', ':', 'hbase', 'showing', 'high', 'writes/ingestion.hbase']),\n",
       "       list(['high', 'memory', 'and', 'cpu', 'usage', 'performance', 'issue', 'spark']),\n",
       "       list(['high', 'read', 'and', 'write', 'latencies', 'in', 'hbaseone', 'sample', 'row', 'data', 'in', 'hbase', 'table', ':', 'hbase', '(', 'main', ')', ':001:0', '>', 'get', \"'queue-sc-slow\", \"'\", ',', \"'b06\", ':', 'p0:1584489168424', ':', 'urn', ':', 'aaid', ':', 'sc', ':', \"us:1a9bb0ea-4d6a-4ce4-9396-f698c9134213:18:1584489167309'column\", 'cellevd', ':', 'd', 'timestamp=1584489168426', ',', 'value=', '{', '``', 'id', \"''\", ':', \"''\", 'urn', ':', 'aaid', ':', 'sc', ':', 'us:1a9bb0ea-4d6a-4ce4-9396-f', '698c9134213', \"''\", ',', \"''\", 'timestamp', \"''\", ':', \"''\", '1584489168425', \"''\", ',', \"''\", 'application_name', \"''\", ':', \"''\", 'creative_cloud', \"''\", ',', \"''\", 'event_type', \"''\", ':', \"''\", 'pubs_mapping_upsert', \"''\", ',', \"''\", 'event_tracking_id', \"''\", ':', \"''\", '800b3daf-80c0-41c5-9180-67d2683c3680.1-3-us-1-2412349f-a473-4dc0-a83b-fd5e464e0fff', \"''\", ',', \"''\", 'ordering_date', \"''\", ':0', ',', \"''\", 'ordering_version', \"''\", ':1584489167309', ',', \"''\", 'priority', \"''\", ':0', ',', \"''\", 'payload', \"''\", ':', '{', '``', 'asset_id', \"''\", ':', \"''\", 'urn', ':', 'aaid', ':', 'sc', ':', 'us:1a9bb0ea-4d6a-4ce4-9396-698c9134213', \"''\", ',', \"''\", 'owner_id', \"''\", ':', \"''\", '07d48be156ba2a237f000101adobeid', \"''\", ',', \"''\", 'owner_type', \"''\", ':', \"''\", 'usm', \"''\", ',', \"''\", 'target', \"''\", ':', '{', '``', 'asset_id', \"''\", ':', \"''\", 'urn', ':', 'aaid', ':', 'sc', ':', 'us:1a9bb0ea-4d6a-4ce4-9396-698c9134213', \"''\", ',', \"''\", 'owner_id', \"''\", ':', \"''\", '07d48be156ba2a237f000101', '@', 'adobeid', \"''\", ',', \"''\", 'region', \"''\", ':', \"''\", 'us', \"''\", '}', '}', '}', 'status', ':', 'd', 'timestamp=1584489168988', ',', 'value=success', 'server', 'timestamps', 'after', 'initial', 'write', ':', '“', 'columnfamily', ':', 'evd', '”', 'cellv1', '1584489168426', '-', 'march', '17', ',', '2020', '11:52:48.426', 'pm', '“', 'columnfamily', ':', 'status', '”', 'cellv1', '1584489168426', '-', 'march', '17', ',', '2020', '11:52:48.426', 'pm', '(', 'inferred', 'based', 'on', 'evd', 'timestamp', '-', 'cellv1', ',', 'since', 'actual', 'data', 'is', 'missing', ')', 'after', 'the', 'processor', 'scans', 'the', 'data', 'and', 'marks', 'status', 'as', 'done', ':', '“', 'columnfamily', ':', 'status', '”', 'cellv2', '1584489168988', '-', 'tuesday', ',', 'march', '17', ',', '2020', '11:52:48.988', 'pm', 'below', 'are', 'the', 'client', 'log', 'used', 'for', 'investigation', 'corresponding', 'to', 'this', 'rowkey', ':', 'after', 'successful', 'write:3/17/20', '4:52:48.432', 'pm', '2020-03-17', '23:52:48.432', 'gmt', 'info', 'rid=senw81uruvafpazorqr9qj7semc6czlt', 'cid=na', '3178', '--', '-', '[', 'ool-2-thread-13', ']', 'c.a.i.writer.hbasemultiplytablewriter', ':', 'success', 'for', 'row', 'key', ':', 'b06', ':', 'p0:1584489168424', ':', 'urn', ':', 'aaid', ':', 'sc', ':', 'us:1a9bb0ea-4d6a-4ce4-9396-f698c9134213:18:1584489167309', 'first', 'rangescan', 'that', 'did', 'not', 'find', 'the', 'row', ':', '2020-03-17', '23:52:48.614', 'c.a.i.s.hbasescanner', '[', 'info', ']', 'hbasescanner', ':', ':', 'scan', 'result', 'count:0', ',', 'countnewrows:0', ',', 'sourcesize:0', ',', 'table', ':', 'queue-sc-slow', ',', 'prefix', ':', 'b06', ',', 'starttime:1584489120672', '(', 'march', '17', ',', '2020', '11:52:00.672', 'pm', ')', ',', 'endtime:1584489168606', '(', 'march', '17', ',', '2020', '11:52:48.606', 'pm', ')', ',', 'consumer', ':', 'sharedcloud_slowsecond', 'rangescan', 'that', 'did', 'not', 'find', 'the', 'row:2020-03-17', '23:52:49.270', 'c.a.i.s.hbasescanner', '[', 'info', ']', 'hbasescanner', ':', ':', 'scan', 'result', 'count:0', ',', 'countnewrows:0', ',', 'sourcesize:0', ',', 'table', ':', 'queue-sc-slow', ',', 'prefix', ':', 'b06', ',', 'starttime:1584489120678', '(', 'march', '17', ',', '2020', '11:52:00.678', 'pm', ')', ',', 'endtime:1584489169262', ',', 'consumer', ':', 'sharedcloud_slow', '(', 'march', '17', ',', '2020', '11:52:49.26', 'pm', ')', 'after', 'few', 'range', 'scans', 'that', 'did', 'not', 'find', 'the', 'row', ',', 'here', 'is', 'the', 'range', 'scan', 'that', 'finds', 'the', 'row:2020-03-17', '23:53:11.995', 'c.a.i.s.hbasescanner', '[', 'info', ']', 'hbasescanner', ':', ':', 'scan', 'result', 'count:1', ',', 'countnewrows:1', ',', 'sourcesize:1', ',', 'table', ':', 'queue-sc-slow', ',', 'prefix', ':', 'b06', ',', 'starttime:1584489166182', '(', 'march', '17', ',', '2020', '11:52:46.182', 'pm', ')', ',', 'endtime:1584489191984', '(', 'march', '17', ',', '2020', '11:53:11.984', 'pm', ')', ',', 'consumer', ':', 'sharedcloud_slow.below', 'is', 'the', 'data', 'returned', 'from', 'rangescan:2020-03-17', '23:53:11.995', 'c.a.i.s.hbasescanner', '[', 'info', ']', 'hbasescanner', ':', ':tuple', ':', 'b06', ':', 'p0:1584489168424', ':', 'urn', ':', 'aaid', ':', 'sc', ':', 'us:1a9bb0ea-4d6a-4ce4-9396-f698c9134213:18:1584489167309', ',', 'event_type', ':', 'pubs_mapping_upsert', ',', 'application_name', ':', 'null', ',', 'h_timestamp:1584489168426', 'asset_name', ':', 'null', 'adding', 'to', 'queuehbase']),\n",
       "       list(['high', 'wal', 'sync', 'latencies', 'in', 'stagingwal', 'latency', 'went', 'above', '50', 'secs', 'for', 'a', 'minute', '2020-05-12', '22:23:02,165', 'info', '[', 'sync.4', ']', 'wal.fshlog', ':', 'slow', 'sync', 'cost', ':', '69104', 'ms', ',', 'current', 'pipeline', ':', '[', 'datanodeinfowithstorage', '[', '10.67.6.144:30010', ',', 'ds-80163b21-d86b-4abc-83f4-cc22e24aeb04', ',', 'disk', ']', ',', 'datanodeinfowithstorage', '[', '10.67.6.161:30010', ',', 'ds-92dd28df-6a69-45b4-9210-1e7a7717cd41', ',', 'disk', ']', ',', 'datanodeinfowithstorage', '[', '10.67.6.159:30010', ',', 'ds-101927c6-3d03-42de-8290-a714d9ac0025', ',', 'disk', ']', ']', 'root', 'cause', ':', 'hbase']),\n",
       "       list(['history', 'of', 'ramthey', 'did', \"n't\", 'know', 'how', 'to', 'check', 'the', 'metrics', 'for', 'cluster', 'nodes.spark']),\n",
       "       list(['hive', 'and', 'mapreduce', 'services', 'are', 'down', 'from', 'all', 'the', 'clusters', 'hive', 'and', 'mapreduce', 'services', 'are', 'down', 'from', 'all', 'the', 'clustershive']),\n",
       "       list(['hive', 'ats', 'check', 'failhive', 'ats', 'check', 'failhive', 'view']),\n",
       "       list(['hive', 'connectivity', 'to', 'hdinsight', 'cluster', 'is', 'randomunsupported', 'cluster', 'hdi', '3.5hive', 'view']),\n",
       "       list(['hive', 'error', 'while', 'accessing', 'data', 'from', 'qlik', '120051822000128', '-', 'hive', 'error', 'while', 'accessing', 'data', 'from', 'qlikodbc', 'or', 'jdbc']),\n",
       "       list(['hive', 'etl', 'taking', 'longer', 'time120052723001910', '-', 'hive', 'etl', 'taking', 'longer', 'timehive']),\n",
       "       list(['hive', 'export', 'operations', 'are', 'failing', 'with', 'distscp', 'errorhive', 'export', 'operations', 'are', 'failing', 'with', 'distscp', 'errorinteractive', 'query']),\n",
       "       list(['hive', 'going', 'down', 'frequentlycluster', 'service', 'instability', 'due', 'to', 'local', 'disk', 'filling', 'on', 'both', 'headnodesspark']),\n",
       "       list(['hive', 'insert', 'failure', '-', 'memory', 'heap', 'issue', 'hive', 'insert', 'failure', '-', 'memory', 'heap', 'issuehive']),\n",
       "       list(['hive', 'instability', 'on', 'interactive', 'hive', 'clusteroriginal', 'issue', ':', 'connection', 'failed', 'on', 'host', 'hn0-dsjd4l.azure.mvtdevdesjardins.com:10001', '(', 'traceback', '(', 'most', 'recent', 'call', 'last', ')', ':', 'file', \"'/var/lib/ambari-agent/cache/stacks/hdp/3.0/services/hive/package/alerts/alert_hive_interactive_thrift_port.py\", \"'\", ',', 'line', '210', ',', 'in', 'execute', 'ldap_password=ldap_password', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/libraries/functions/hive_check.py\", \"'\", ',', 'line', '84', ',', 'in', 'check_thrift_port_sasl', 'timeout_kill_strategy=terminatestrategy.kill_process_tree', ',', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/base.py\", \"'\", ',', 'line', '166', ',', 'in', '__init__', 'self.env.run', '(', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/environment.py\", \"'\", ',', 'line', '160', ',', 'in', 'run', 'self.run_action', '(', 'resource', ',', 'action', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/environment.py\", \"'\", ',', 'line', '124', ',', 'in', 'run_action', 'provider_action', '(', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py\", \"'\", ',', 'line', '263', ',', 'in', 'action_run', 'returns=self.resource.returns', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/shell.py\", \"'\", ',', 'line', '72', ',', 'in', 'inner', 'result', '=', 'function', '(', 'command', ',', '**kwargs', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/shell.py\", \"'\", ',', 'line', '102', ',', 'in', 'checked_call', 'tries=tries', ',', 'try_sleep=try_sleep', ',', 'timeout_kill_strategy=timeout_kill_strategy', ',', 'returns=returns', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/shell.py\", \"'\", ',', 'line', '150', ',', 'in', '_call_wrapper', 'result', '=', '_call', '(', 'command', ',', '**kwargs_copy', ')', 'file', \"'/usr/lib/ambari-agent/lib/resource_management/core/shell.py\", \"'\", ',', 'line', '308', ',', 'in', '_call', 'raise', 'executetimeoutexception', '(', 'err_msg', ')', 'executetimeoutexception', ':', 'execution', 'of', \"'ambari-sudo.sh\", 'su', 'ambari-qa', '-l', '-s', '/bin/bash', '-c', \"'export\", 'path=/usr/local/sbin', ':', '/usr/local/bin', ':', '/usr/sbin', ':', '/usr/bin', ':', '/sbin', ':', '/bin', ':', '/usr/games', ':', '/usr/local/games', ':', '/var/lib/ambari-agent', ':', '/bin/', ':', '/usr/bin/', ':', '/usr/lib/hive/bin/', ':', '/usr/sbin/', ';', 'beeline', '-n', 'hive', '-u', '``', \"'\", \"''\", 'jdbc', ':', 'hive2', ':', '//hn0-dsjd4l.azure.mvtdevdesjardins.com:10001/', ';', 'transportmode=http', ';', 'httppath=cliservice', ';', 'principal=hive/_host', '@', 'azure.mvtdevdesjardins.com', \"''\", \"'\", \"''\", '-e', '``', \"'\", \"''\", ';', \"'\", \"''\", \"''\", '2', '&', '1', '|', 'awk', '``', \"'\", \"''\", '{', 'print', '}', \"'\", \"''\", \"''\", '|', 'grep', '-i', '-e', '``', \"'\", \"''\", 'connected', 'to', ':', \"'\", \"''\", \"''\", '-e', '``', \"'\", \"''\", 'transaction', 'isolation', ':', \"'\", \"''\", \"'\", \"''\", \"'\", 'was', 'killed', 'due', 'timeout', 'after', '60', 'seconds', ')', ';', '===========================================================interactive', 'query']),\n",
       "       list(['hive', 'interactiveser2', 'is', 'downhive', 'interactiveser2', 'is', 'downhive']),\n",
       "       list(['hive', 'jobs', 'running', 'indefinitelyhive', 'jobs', 'running', 'indefinitelyhive']),\n",
       "       list(['hive', 'major', 'compaction', 'does', 'not', 'delete', 'delta', 'files', 'hive', 'major', 'compaction', 'does', 'not', 'delete', 'delta', 'fileshive']),\n",
       "       list(['hive', 'metastore', 'is', 'failing', 'with', 'schema', 'tool', 'failed', 'message2020-05-07', '17:54:16,710', 'main', 'trace', 'using', 'default', 'systemclock', 'for', 'timestamps', '.', 'initializing', 'the', 'schema', 'to', ':', '3.1.0', 'metastore', 'connection', 'url', ':', 'jdbc', ':', 'sqlserver', ':', '//nextgenanalyticssqlsvr.database.windows.net', ';', 'database=hdi4_metastore_db', ';', 'encrypt=true', ';', 'trustservercertificate=true', ';', 'create=false', ';', 'logintimeout=300', 'metastore', 'connection', 'driver', ':', 'com.microsoft.sqlserver.jdbc.sqlserverdriver', 'metastore', 'connection', 'user', ':', 'nextgenadmin', 'starting', 'metastore', 'schema', 'initialization', 'to', '3.1.0', 'initialization', 'script', 'hive-schema-3.1.0.mssql.sql', 'connecting', 'to', 'jdbc', ':', 'sqlserver', ':', '//nextgenanalyticssqlsvr.database.windows.net', ';', 'database=hdi4_metastore_db', ';', 'encrypt=true', ';', 'trustservercertificate=true', ';', 'create=false', ';', 'logintimeout=300', 'connected', 'to', ':', 'microsoft', 'sql', 'server', '(', 'version', '12.00.2000', ')', 'beeline']),\n",
       "       list(['hive', 'metastore', 'issueshive', 'queries', 'involving', 'tables', 'having', 'multiple', 'partitions', '(', '>', '~250', ')', 'sometime', 'failinteractive', 'query']),\n",
       "       list(['hive', 'odbc', 'connection', 'failing', 'symptom', ':', 'when', 'run', 'a', 'query', 'for', 'more', 'then', '4.5mil', 'records', 'sql', 'error', 'occurs', ':', 'qvx_unexpected_end_of_data', ':', 'sql', '#', '#', 'f', '-', 'sqlstate', ':', 's1000', ',', 'errorcode', ':', '35', ',', 'errormsg', ':', '[', 'microsoft', ']', '[', 'hardy', ']', '(', '35', ')', 'error', 'from', 'server', ':', 'error', 'code', ':', \"'0\", \"'\", 'error', 'message', ':', \"'invalid\", 'operationhandle', ':', 'operationhandle', '[', 'optype=execute_statement', ',', 'gethandleidentifier', '(', ')', '=f7d18e77-6b8c-4546-bfde-c003433d1f5c', ']', \"'.odbc\", 'or', 'jdbc']),\n",
       "       list(['hive', 'permission', 'error', 'on', 'adls', 'while', 'creating', 'the', 'external', 'tables', 'hive', 'create', 'table', 'statements', 'were', 'failing', 'on', 'existing', 'folders\\\\files', 'with', 'error', 'message', '“', 'hiveaccesscontrolexception', \"''\", '.', 'while', 'you', 'were', 'able', 'to', 'create', 'tables', 'on', 'new', 'directories.hive']),\n",
       "       list(['hive', 'queries', 'failing', 'in', 'zeppelin', 'cx', 'internal', 'too', 'issueambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['hive', 'queries', 'sporadically', 'fail', 'after', 'executing', 'a', 'number', 'of', 'tasks', 'with', 'queries', 'failing', 'with', 'digest-md5', 'errorwe', 'see', 'this', 'behavior', 'because', 'of', 'zk', 'high', 'loadsinteractive', 'query']),\n",
       "       list(['hive', 'query', 'console', 'hanginghive', 'query', 'console', 'hanginginteractive', 'query']),\n",
       "       list(['hive', 'query', 'is', 'not', 'working', 'through', 'ambarihive', 'view', 'logs', 'show', 'deadline', 'passed', 'exceptions', 'like', 'java.util.concurrent.timeoutexception', ':', 'deadline', 'passedhive']),\n",
       "       list(['hive', 'query', 'issueunable', 'to', 'run', 'hive', 'query', 'through', 'jdbc', 'connnection', 'with', 'third', 'party', 'tooluser', 'is', 'getting', 'org.apache.thrift.transport.ttransportexception', ':', 'http', 'response', 'code', ':', '502hive']),\n",
       "       list(['hive', 'query', 'performance', 'issuehive', 'count', '(', '*', ')', 'query', 'not', 'working.hive']),\n",
       "       list(['hive', 'serveice', 'frequent', 'outagehive', 'service', 'frequent', 'outageinteractive', 'query']),\n",
       "       list(['hive', 'server', '2', 'is', 'not', 'runninghiveserver2', 'interactive', 'did', 'not', 'finish', 'intialiaztionhive']),\n",
       "       list(['hive', 'service', 'is', 'down', '.', 'restart', 'operation', 'unresponsive.hs2i', 'failed', 'to', 'startinteractive', 'query']),\n",
       "       list(['hive', 'service', 'is', 'downhive', 'service', 'is', 'downhadoop']),\n",
       "       list(['hive', 'service', 'unable', 'to', 'starthive', 'services', 'unable', 'to', 'starthadoop']),\n",
       "       list(['hive', 'table', 'location', 'fails', 'with', 'read', 'permission', 'errorhive', 'table', 'location', 'fails', 'with', 'read', 'permission', 'errorhive']),\n",
       "       list(['hive', 'table', 'lockhive', 'tables', 'were', 'acquiring', 'locks', 'and', 'not', 'clearing', 'before', 'the', 'next', 'load', 'which', 'resulted', 'in', 'table', 'failurehive']),\n",
       "       list(['hive', 'tables', 'access', 'issue', 'from', 'pysparkhive', 'tables', 'access', 'issue', 'from', 'pysparkspark']),\n",
       "       list(['hive', 'view', 'is', 'not', 'accessible', 'from', 'ambarihive', 'view', 'is', 'not', 'accessible', 'from', 'ambarihive', 'view']),\n",
       "       list(['hiveinteractive', 'service', 'and', 'zookeeper', 'cancelled', 'key', 'exceptions', 'issueserror', '[', 'commitprocessor:1', ':', 'nioservercnxn', '@', '178', ']', '-', 'unexpected', 'exception', ':', 'java.nio.channels.cancelledkeyexceptioninteractive', 'query']),\n",
       "       list(['hiveserver2', 'interactive', 'critical', 'alerths2i', 'goes', 'down', 'and', 'see', 'a', 'hiveserver2', 'interactive', 'critical', 'alert', 'on', 'ambari', 'uihive']),\n",
       "       list(['hiveserver2interactive', 'process', 'is', 'not', 'stableconnection', 'failed', 'on', 'host', 'hn0-dsjd4l.azure.mvtdevdesjardins.com:10001', '(', 'traceback', '(', 'most', 'recent', 'call', 'last', ')', ':', 'file', '``', '/var/lib/ambari-agent/cache/stacks/hdp/3.0/services/hive/package/alerts/alert_hive_interactive_thrift_port.py', \"''\", ',', 'line', '210', ',', 'in', 'execute', 'ldap_password=ldap_password', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/libraries/functions/hive_check.py', \"''\", ',', 'line', '84', ',', 'in', 'check_thrift_port_sasl', 'timeout_kill_strategy=terminatestrategy.kill_process_tree', ',', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/base.py', \"''\", ',', 'line', '166', ',', 'in', '__init__', 'self.env.run', '(', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/environment.py', \"''\", ',', 'line', '160', ',', 'in', 'run', 'self.run_action', '(', 'resource', ',', 'action', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/environment.py', \"''\", ',', 'line', '124', ',', 'in', 'run_action', 'provider_action', '(', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py', \"''\", ',', 'line', '263', ',', 'in', 'action_run', 'returns=self.resource.returns', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/shell.py', \"''\", ',', 'line', '72', ',', 'in', 'inner', 'result', '=', 'function', '(', 'command', ',', '**kwargs', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/shell.py', \"''\", ',', 'line', '102', ',', 'in', 'checked_call', 'tries=tries', ',', 'try_sleep=try_sleep', ',', 'timeout_kill_strategy=timeout_kill_strategy', ',', 'returns=returns', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/shell.py', \"''\", ',', 'line', '150', ',', 'in', '_call_wrapper', 'result', '=', '_call', '(', 'command', ',', '**kwargs_copy', ')', 'file', '``', '/usr/lib/ambari-agent/lib/resource_management/core/shell.py', \"''\", ',', 'line', '308', ',', 'in', '_call', 'raise', 'executetimeoutexception', '(', 'err_msg', ')', 'executetimeoutexception', ':', 'execution', 'of', \"'ambari-sudo.sh\", 'su', 'ambari-qa', '-l', '-s', '/bin/bash', '-c', \"'export\", 'path=/usr/local/sbin', ':', '/usr/local/bin', ':', '/usr/sbin', ':', '/usr/bin', ':', '/sbin', ':', '/bin', ':', '/usr/games', ':', '/usr/local/games', ':', '/var/lib/ambari-agent', ':', '/bin/', ':', '/usr/bin/', ':', '/usr/lib/hive/bin/', ':', '/usr/sbin/', ';', 'beeline', '-n', 'hive', '-u', \"'\", \"''\", \"'\", \"''\", \"'jdbc\", ':', 'hive2', ':', '//hn0-dsjd4l.azure.mvtdevdesjardins.com:10001/', ';', 'transportmode=http', ';', 'httppath=cliservice', ';', 'principal=hive/_host', '@', 'azure.mvtdevdesjardins.com', \"'\", \"''\", \"'\", \"''\", \"'\", '-e', \"'\", \"''\", \"'\", \"''\", \"'\", ';', \"'\", \"''\", \"'\", \"''\", \"'\", '2', '>', '&', '1', '|', 'awk', \"'\", \"''\", \"'\", \"''\", \"'\", '{', 'print', '}', \"'\", \"''\", \"'\", \"''\", \"'\", '|', 'grep', '-i', '-e', \"'\", \"''\", \"'\", \"''\", \"'connected\", 'to', ':', \"'\", \"''\", \"'\", \"''\", \"'\", '-e', \"'\", \"''\", \"'\", \"''\", \"'transaction\", 'isolation', ':', \"'\", \"''\", \"'\", \"''\", \"'\", \"''\", 'was', 'killed', 'due', 'timeout', 'after', '60', 'seconds', ')', 'interactive', 'query']),\n",
       "       list(['hn0', 'can', 'not', 'start', 'servicesservices', 'were', 'stopped', 'after', 'a', 'reboot', 'of', 'hn0', '.', 'as', 'client', 'spark']),\n",
       "       list(['hn0', 'vm', 'not', 'respondinghn0', 'vm', 'not', 'respondingnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['hn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.internal.cloudapp.net', 'is', 'downhn0-omeaci.geragnkkulle1co3drhsk3ky0b.hx.internal.cloudapp.net', 'is', 'downhbase']),\n",
       "       list(['hostname', 'resolution', 'failedhostname', 'resolution', 'failedspark']),\n",
       "       list(['how', 'to', 'clear', 'last', 'checkpoint', 'service', 'alert', 'on', 'namenodethe', 'hdinsight', 'cluster', 'etl-a-cas-hdi-spark-sec', 'is', 'currently', 'suffering', 'from', 'hdfs', 'checkpoint', 'error.spark']),\n",
       "       list(['how', 'to', 'connect', 'hd', 'insight', 'cluster', 'to', 'paas', 'sql', 'databaseunable', 'to', 'connect', 'to', 'a', 'sql', 'db', 'to', 'hdiodbc', 'or', 'jdbc', 'connecting', 'to', 'standard', 'cluster']),\n",
       "       list(['how', 'to', 'connect', 'using', 'ssl', 'port', 'from', 'beeline', 'to', 'hivehow', 'to', 'connect', 'using', 'ssl', 'port', 'from', 'beeline', 'to', 'hive.beeline']),\n",
       "       list(['how', 'to', 'enable', 'impala', 'in', 'hdinsight', 'cluster', '?', 'client', 'wanted', 'to', 'install', 'apache', 'impala', 'in', 'a', 'hdinsight', 'cluster.hdinsight', 'workload', 'monitoring', 'solution']),\n",
       "       list(['how', 'to', 'prevent', 'hmaster', 'process', 'run', '?', 'we', 'found', 'hmaster', 'process', 'uses', 'lot', 'of', 'cputhe', 'other', 'jobs', 'are', 'taking', 'more', 'time', 'than', 'usual', 'spark']),\n",
       "       list(['how', 'to', 'schedule', 'creation', 'of', 'clusterguidance', 'regarding', 'automating', 'cluster', 'creation/deletion', 'and', 'permission', 'issues', 'for', 'automation', 'accountcreate', 'failure', '-', 'other']),\n",
       "       list(['how', 'to', 'update', 'hive', 'from', '1.2', 'to', '1.3customer', 'wanted', 'to', 'update', 'hive', 'from', 'version', '1.2', 'to', '1.3', 'within', 'their', 'hdi', 'cluster.issue', 'with', 'autoscaling']),\n",
       "       list(['hs2i', 'service', 'is', 'not', 'startinghs2i', 'is', 'downinteractive', 'query']),\n",
       "       list(['hs2i', 'services', 'are', 'down.abfs', 'delegation', 'token', 'issuezk', 'is', 'unstable', 'and', 'high', 'loads', 'on', 'zk', 'hostsinteractive', 'query']),\n",
       "       list(['hs2i', 'services', 'are', 'downhs2i', 'services', 'are', 'down', 'after', 'scale', 'upinteractive', 'query']),\n",
       "       list(['http', 'error', '502.3', '-', 'bad', 'gateway', 'when', 'checking', 'logcan', 'access', 'to', 'application', 'logsspark']),\n",
       "       list(['http', 'messages', 'unable', 'to', 'process', ',', 'connector', 'giving', 'time', 'outmessages', 'not', 'pushed', 'by', 'kafka', 'to', 'splunk', 'clusterhbase']),\n",
       "       list(['huge', 'amount', 'of', 'logs', 'are', 'present', 'in', 'the', \"'/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir\", \"'\", 'due', 'to', 'this', 'header', 'node', 'running', 'out', 'of', 'storagelarge', 'amount', 'of', 'logs', 'being', 'stored', 'in', 'hadoop_java_io_tmpdirspark']),\n",
       "       list(['i', 'am', 'trying', 'to', 'host', 'the', 'above', 'cluster', 'as', 'premium', 'tier', 'clusteri', 'am', 'trying', 'to', 'host', 'the', 'above', 'cluster', 'as', 'premium', 'tier', 'clustercreate', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['i', 'ca', \"n't\", 'see', 'the', 'performance', 'logs', 'of', 'hdinsight', 'nodes', 'through', 'log', 'analytics', 'queries.ca', \"n't\", 'see', 'the', 'performance', 'logs', 'of', 'hdinsight', 'nodes', 'through', 'log', 'analytics', 'queries.hdinsight', 'workload', 'monitoring', 'solution']),\n",
       "       list(['i', 'ca', \"n't\", 'use', 'my', 'academic/lab', 'subscription', 'for', '(', 'georgia', 'tech', 'course', 'cse6242', ')', 'i', 'ca', \"n't\", 'use', 'my', 'academic/lab', 'subscription', 'for', '(', 'georgia', 'tech', 'course', 'cse6242', ')', 'issues', 'signing', 'in', 'or', 'accessing', 'my', 'subscriptions']),\n",
       "       list(['i', 'can', 'not', 'connect', 'remotely', 'to', 'this', 'cluster', 'through', 'powerbi', 'or', 'a', 'jupyter', 'notebooki', 'can', 'not', 'connect', 'remotely', 'to', 'this', 'cluster', 'through', 'powerbi', 'or', 'a', 'jupyter', 'notebookodbc', 'or', 'jdbc', 'connecting', 'to', 'standard', 'cluster']),\n",
       "       list(['i', 'can', 'not', 'read', 'the', 'logs120051221001901', 'can', 'not', 'read', 'spark', 'container', 'logs', 'in', 'the', 'yarn', 'uispark']),\n",
       "       list(['i', 'want', 'to', 'stop', 'my', 'hd', 'insight', 'cluster', 'for', 'some', 'time.120071624005592-', 'i', 'want', 'to', 'stop', 'my', 'hd', 'insight', 'cluster', 'for', 'some', 'time.issue', 'with', 'scaling', 'down']),\n",
       "       list(['implement', 'posix', 'recursively', 'in', 'gen2', 'using', 'storage', 'explorer', 'or', 'any', 'hdfs', 'commandscannot', 'assign', 'aclsadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['importing', 'snapshotunable', 'to', 'export', 'hbase', 'snapshot', 'from', 'on-prem', 'cluster', 'to', 'hdi', 'clusterhdinsight', 'sdk']),\n",
       "       list(['inc04140006', ':', 'the', 'cluster', 'is', 'down', 'we', 'have', 'all', 'jobs', 'failinginc04140006', ':', 'the', 'cluster', 'is', 'down', 'we', 'have', 'all', 'jobs', 'failingspark']),\n",
       "       list(['inconsistent', 'processing', 'among', 'partitionsinconsistent', 'processing', 'among', 'partitionskafka']),\n",
       "       list(['indisponibilidad', 'del', 'cluster', 'por', 'un', 'lapso', 'de', 'tiempoindisponibilidad', 'del', 'cluster', 'por', 'un', 'lapso', 'de', 'tiempohbase']),\n",
       "       list(['informatica', 'bdm', 'jobs', 'failed', 'on', 'azure', 'clusterinformatica', 'bdm', 'jobs', 'failed', 'on', 'azure', 'cluster.hive']),\n",
       "       list(['install', 'python', 'packages', 'for', 'all', 'worker', 'nodes/05/01', '00:23:56', 'info', 'blockmanagermasterendpoint', ':', 'registering', 'block', 'manager', 'wn0-askaka.kld1xygk2rgutj3nnqhysmnatd.bx.internal.cloudapp.net:39293', 'with', '1458.6', 'mb', 'ram', ',', 'blockmanagerid', '(', '1', ',', 'wn0-askaka.kld1xygk2rgutj3nnqhysmnatd.bx.internal.cloudapp.net', ',', '39293', ',', 'none', ')', '20/05/01', '00:23:56', 'info', 'blockmanagermasterendpoint', ':', 'registering', 'block', 'manager', 'wn0-askaka.kld1xygk2rgutj3nnqhysmnatd.bx.internal.cloudapp.net:34409', 'with', '1458.6', 'mb', 'ram', ',', 'blockmanagerid', '(', '2', ',', 'wn0-askaka.kld1xygk2rgutj3nnqhysmnatd.bx.internal.cloudapp.net', ',', '34409', ',', 'none', ')', '20/05/01', '00:23:56', 'info', 'sparkentries', ':', 'created', 'spark', 'session', '(', 'with', 'hive', 'support', ')', '.', '20/05/01', '00:24:03', 'warn', 'session', ':', 'fail', 'to', 'start', 'interpreter', 'pyspark', 'java.io.ioexception', ':', 'can', 'not', 'run', 'program', '``', '/usr/bin/anaconda/envs/py35new/bin/python', \"''\", ':', 'error=2', ',', 'no', 'such', 'file', 'or', 'directory', 'at', 'java.lang.processbuilder.start', '(', 'processbuilder.java:1048', ')', 'at', 'org.apache.livy.repl.pythoninterpreter', '$', '.apply', '(', 'pythoninterpreter.scala:75', ')', 'at', 'org.apache.livy.repl.session.liftedtree1', '$', '1', '(', 'session.scala:106', ')', 'at', 'org.apache.livy.repl.session.interpreter', '(', 'session.scala:98', ')', 'at', 'org.apache.livy.repl.session.org', '$', 'apache', '$', 'livy', '$', 'repl', '$', 'session', '$', '$', 'setjobgroup', '(', 'session.scala:353', ')', 'at', 'org.apache.livy.repl.session', '$', '$', 'anonfun', '$', 'execute', '$', '1.apply', '$', 'mcv', '$', 'sp', '(', 'session.scala:164', ')', 'at', 'org.apache.livy.repl.session', '$', '$', 'anonfun', '$', 'execute', '$', '1.apply', '(', 'session.scala:163', ')', 'at', 'org.apache.livy.repl.session', '$', '$', 'anonfun', '$', 'execute', '$', '1.apply', '(', 'session.scala:163', ')', 'at', 'scala.concurrent.impl.future', '$', 'promisecompletingrunnable.liftedtree1', '$', '1', '(', 'future.scala:24', ')', 'at', 'scala.concurrent.impl.future', '$', 'promisecompletingrunnable.run', '(', 'future.scala:24', ')', 'at', 'java.util.concurrent.threadpoolexecutor.runworker', '(', 'threadpoolexecutor.java:1149', ')', 'at', 'java.util.concurrent.threadpoolexecutor', '$', 'worker.run', '(', 'threadpoolexecutor.java:624', ')', 'at', 'java.lang.thread.run', '(', 'thread.java:748', ')', 'caused', 'by', ':', 'java.io.ioexception', ':', 'error=2', ',', 'no', 'such', 'file', 'or', 'directory', 'at', 'java.lang.unixprocess.forkandexec', '(', 'native', 'method', ')', 'at', 'java.lang.unixprocess.', '<', 'init', '>', '(', 'unixprocess.java:247', ')', 'at', 'java.lang.processimpl.start', '(', 'processimpl.java:134', ')', 'spark']),\n",
       "       list(['intermittent', 'error', 'connecting', 'adlsintermittent', 'error', 'connecting', 'adlsadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['internal', 'server', 'error', 'occurred', 'while', 'processing', 'the', 'request', '.', 'please', 'retry', 'the', 'request', 'or', 'contact', 'support.cluster', 'creation', 'failcreate', 'failure', '-', 'other']),\n",
       "       list(['io', 'cache', 'not', 'installed', 'and', 'not', 'an', 'available', 'option', 'in', \"'stack\", 'and', \"versions'unable\", 'to', 'find', 'io', 'cache', 'in', 'the', 'clusterspark']),\n",
       "       list(['issue', ':', 'we', 'lost', 'sshuser', 'password', '.', 'could', 'you', 'please', 'reset', 'sshuser', 'password', 'for', 'hdi', 'cluster', \"'s\", 'head', 'nodes.issue', ':', 'we', 'lost', 'sshuser', 'password', '.', 'could', 'you', 'please', 'reset', 'sshuser', 'password', 'for', 'hdi', 'cluster', \"'s\", 'head', 'nodes.ambari', 'in', 'standard', 'cluster']),\n",
       "       list(['issue', 'in', 'bpudlit4itprod', 'hdi', 'cluster.issue', 'in', 'bpudlit4itprod', 'hdi', 'cluster.hadoop']),\n",
       "       list(['issue', 'with', 'hive', 'while', 'storing', 'streaming', 'dataissue', 'with', 'hive', 'while', 'storing', 'streaming', 'datahive']),\n",
       "       list(['issues', 'containercustom', 'built', 'slow', 'listing', 'of', 'filesmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['issues', 'while', 'scaling', 'the', 'clusterchecked', 'script', 'actions', 'in', 'scale', 'up', 'workflow', 'and', 'see', 'one', 'script', 'is', 'in', 'progress', 'for', 'over', '2', 'hourslet', 'clustername', '=', '``', 'ana02spark36datahubpr01', \"''\", ';', 'let', 'starttime', '=', 'datetime', '(', '2020-06-07', '00:00:00', ')', ';', 'let', 'endtime', '=', 'datetime', '(', '2020-06-07', '00:00:00', ')', ';', 'cluster', '(', '``', 'hdinsight', \"''\", ')', '.database', '(', '``', 'hdinsight', \"''\", ')', '.logentry', '|', 'where', 'clusterdnsname', '=~', 'trim', '(', '``', '``', ',', 'clustername', ')', '|', 'where', 'precisetimestamp', '>', '=', 'starttime//|', 'where', 'precisetimestamp', '<', '=', 'endtime//|', 'where', 'precisetimestamp', 'between', '(', 'starttime..endtime', ')', '|', 'where', 'class', 'contains', '``', 'customizescaleuphostsactivity', \"''\", '|', 'project', 'precisetimestamp', ',', 'tracelevel', ',', 'class', ',', 'details', ',', 'exceptiontype', ',', 'exceptionmessage', '|', 'order', 'by', 'precisetimestamp', 'desc', 'issue', 'with', 'scaling', 'down']),\n",
       "       list(['issues', 'with', 'cluster', 'creation', 'issues', 'with', 'cluster', 'creationcreate', 'failure', '-', 'other']),\n",
       "       list(['issues', 'with', 'services', 'after', 'the', 'scale', 'upafter', 'scaledown', 'and', 'both', 'rm', 'goes', 'to', 'standby', 'stateissue', 'with', 'scaling', 'up']),\n",
       "       list(['job', 'started', 'crashing', 'without', 'jar', 'changesjob', 'started', 'crashing', 'without', 'jar', 'changeshive']),\n",
       "       list(['jobs', 'failurejobs', 'failurehive']),\n",
       "       list(['jobs', 'hanging', 'at', '100', '%', 'for', 'hourshive', 'jobs', 'hanging', 'at', '100', '%', 'for', 'hourshive']),\n",
       "       list(['jobs', 'not', 'running', 'on', 'the', 'clusterchecking', 'ambari', 'ui', ',', 'both', 'rm', \"'s\", 'showed', 'in', 'standby.spark']),\n",
       "       list(['jobs', 'running', 'in', 'the', 'cluster', 'are', 'stuck', 'and', 'not', 'able', 'to', 'restart', 'services', 'of', 'the', 'clusterissue', ':', 'unable', 'to', 'start', 'hive', 'metastore', 'service', 'on', 'the', 'cluster.hive']),\n",
       "       list(['jupyter', 'automated', 'fixanswer', 'the', 'followed', 'questions', 'regarding', 'to', 'an', 'improvement', 'on', 'hdinsight', 'spark', 'cluster', 'feture', 'for', 'jupyter', 'service', ':', 'i', 'would', 'like', 'to', 'understand', 'the', 'technological', 'difference', 'between', 'our', 'custom', 'short-term', 'fix', 'and', 'microsoft', \"'s\", 'long-term', 'fix', '.', '(', 'i.e', '.', 'is', 'microsoft', '’', 's', 'fix', 'also', 'executed', 'by', 'powershell', '?', ')', 'what', 'does', 'microsoft', 'recommend', 'to', 'use', '?', 'is', 'it', 'acceptable', 'for', 'selective', 'to', 'use', 'the', 'short-term', 'workaround', 'as', 'the', 'long-term', 'fix', 'since', 'it', 'seems', 'to', 'work', 'better', '?', 'spark']),\n",
       "       list(['jupyter', 'notebooks', 'do', 'not', 'work', 'jupyter', 'notebook', 'do', 'not', 'work.notebooks']),\n",
       "       list(['jupyter', 'service', 'starts', 'then', 'automatically', 'gets', 'stoppedjupyter', 'service', 'starts', 'then', 'automatically', 'gets', 'stoppedhadoop']),\n",
       "       list(['just', 'says', 'resource', 'in', 'error', 'statejust', 'says', 'resource', 'in', 'error', 'statecreate', 'failure', '-', 'other']),\n",
       "       list(['kafka', 'brokers', 'stopped', 'kafka', 'brokers', 'stoppednode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['kafka', 'client', 'is', 'not', 'able', 'to', 'access', 'kafka', 'clusterkafka', 'client', 'is', 'not', 'able', 'to', 'access', 'kafka', 'clusterkafka']),\n",
       "       list(['kafka', 'criticial', 'alert', 'regarding', 'broker', 'nodekafka', 'criticial', 'alert', 'regarding', 'broker', 'nodelost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['kafka', 'hdiinsight', 'failed', 'to', 'deploykafka', 'hdiinsight', 'failed', 'to', 'deploycreate', 'failure', '-', 'other']),\n",
       "       list(['kafka', 'stopped', 'after', 'a', 'perf', 'testkafka', 'stopped', 'after', 'a', 'perf', 'testkafka']),\n",
       "       list(['kerbaroes', 'tgt', 'error', 'for', 'the', 'service', 'account', 'uservm', 'reboots', 'very', 'oftenadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['kerbaroes', 'tgt', 'errors', 'causing', 'production', 'jobs', 'failingkerbaroes', 'tgt', 'errors', 'causing', 'production', 'jobs', 'failingambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['kerberos', 'token', 'not', 'working', 'properlythis', 'cluster', 'is', 'build', 'with', 'the', 'esp', 'package.we', 'have', 'one', 'domain', 'user', 'named', 'as', \"'infoworks-user\", \"'\", '.', 'when', 'we', 'tried', 'klist', 'it', 'shows', 'the', 'ticket', 'is', 'valid', 'but', 'kerberos', 'token', 'got', 'expired', 'and', 'because', 'of', 'the', 'token', 'expired', 'we', 'are', 'unable', 'to', 'login', 'to', 'the', 'hive', 'shell', 'it', 'shows', 'that', 'gss', 'initiate', 'exception', '.', 'but', 'we', 'are', 'able', 'to', 'access', 'the', 'hdfs', 'command', 'with', 'the', 'same', 'user', 'on', 'the', 'same', 'cluster', 'and', 'we', 'have', 'other', '2', 'to', '3', 'different', 'esp', 'enabled', 'cluster', 'with', 'the', 'same', 'domain', 'and', 'we', 'observe', 'that', 'the', 'same', 'user', 'is', 'working', 'fine', 'on', 'the', 'other', 'cluster', 'there', 'are', 'no', 'issue', 'with', 'the', 'kerberos', 'token', '.', 'we', 'are', 'only', 'observing', 'kerberos', 'token', 'failure', 'issue', 'on', 'prdiwsecure', 'cluster.create', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['kp10tntncapllapnsprdsup01', ':', 'hive', 'service', 'is', 'downkp10tntncapllapnsprdsup01', ':', 'hive', 'service', 'is', 'downinteractive', 'query']),\n",
       "       list(['kp10tntncapllapnsprdsup01', ':', 'o', 'domain', 'controller', 'available', 'to', 'service', 'the', 'request', 'for', 'realm', 'kpaaddsprod.onmicrosoft.comno', 'domain', 'controller', 'available', 'to', 'service', 'the', 'request', 'for', 'realm', 'kpaaddsprod.onmicrosoft.com', 'exceptionsinteractive', 'query']),\n",
       "       list(['kp10tntncapllapnsprdsup01', ':', 'queries', 'are', 'not', 'returing', 'the', 'resultshivellap', 'is', 'down', 'and', 'queries', 'are', 'not', 'returning', 'resultsinteractive', 'query']),\n",
       "       list(['kpph18llapprdsupusc01', ':', 'llap', 'appliation', 'is', 'downunable', 'to', 'start', 'hive', 'interactiveinteractive', 'query']),\n",
       "       list(['kpps80sparkprdsupwus201', ':', 'gateway', 'errrorkpps80sparkprdsupwus201', ':', 'gateway', 'errrorspark']),\n",
       "       list(['kpps80sparkprdsupwus201', ':', 'seeing', 'numerous', '“', 'java.sql.sqlexception', ':', 'connection', 'is', 'closed', '”', 'errors', 'in', 'hivemetatorekpps80sparkprdsupwus201', ':', 'seeing', 'numerous', '“', 'java.sql.sqlexception', ':', 'connection', 'is', 'closed', '”', 'errors', 'in', 'hivemetatorespark']),\n",
       "       list(['kpq044sparkespfdqawus201', ':', 'token', 'refresh', 'issue', 'on', 'the', 'hdi', 'spark', 'secure', 'clustertoken', 'refresh', 'issue', 'on', 'the', 'hdi', 'spark', 'secure', 'clusterspark']),\n",
       "       list(['kpq052llapfdqawus201', ':', 'multiple', 'services', 'are', 'downzk', 'cancelledkey', 'exceptionsinteractive', 'query']),\n",
       "       list(['kps020sparkfdsbwus401', ':', 'multiple', 'components', 'issueskps020sparkfdsbwus401', ':', 'multiple', 'components', 'issuesissue', 'with', 'autoscaling']),\n",
       "       list(['kps024llapfdsbwus401', ':', 'authentication', 'exceptionkps024llapfdsbwus401', ':', 'authentication', 'exceptioninteractive', 'query']),\n",
       "       list(['kps025sparkfdsbwus401-spark-dynamicallocation-not', 'working', 'despite', 'of', 'cluster', 'free', 'with', '800gbkps025sparkfdsbwus401-spark-dynamicallocation-not', 'working', 'despite', 'of', 'cluster', 'free', 'with', '800gbspark']),\n",
       "       list(['latency', 'in', 'pulling', 'the', 'metadata', 'for', 'the', 'querieslatency', 'in', 'pulling', 'the', 'metadata', 'for', 'the', 'queriesinteractive', 'query']),\n",
       "       list(['livy', 'endpoint', 'gives', '502', 'bad', 'gateway', '.', 'i', 'tried', 'restarting', 'livylivy', 'endpoint', 'gives', '502', 'bad', 'gateway', '.', 'i', 'tried', 'restarting', 'livy.spark']),\n",
       "       list(['livy', 'for', 'spark2', 'server', 'went', 'down', 'in', 'middle', 'of', 'processproblem', 'description', ':', 'credit', 'suisse', 'has', 'been', 'experiencing', 'issues', 'with', 'sporadic', 'active', 'head', 'node', 'failover', '(', 'once', 'during', 'the', 'lifespan', 'of', 'hdi', 'cluster', ')', 'due', 'to', 'which', 'you', 'were', 'unable', 'to', 'submit', 'new', 'jobs', 'and', 'query', 'the', 'status', 'of', 'already', 'submitted', 'jobs', 'since', 'they', 'were', 'hardcoded', 'to', 'head', 'node', 'which', 'is', 'now', 'standby', 'head', 'node', 'post', 'the', 'failover', '.', 'node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['livy', 'service', 'down', 'for', 'several', 'minuteshdfs', 'checkpoint', 'alerts', 'are', 'not', 'cleared', 'when', 'manually', 'saving', 'and', 'livy', 'server', 'intermittently', 'going', 'down', 'are', 'surface', 'symptoms', '.', 'in', 'the', 'leader', 'election', 'logs', 'we', 'see', 'that', 'there', 'are', 'issues', 'selecting', 'a', 'leader', 'sometimes', '.', 'note', 'that', 'since', 'the', 'zookeeper', 'issue', 'is', 'intermittent', 'most', 'of', 'the', 'other', 'issues', 'may', 'also', 'be', 'intermittent.spark']),\n",
       "       list(['llap', 'app', \"'llap0\", \"'\", 'deployment', 'unsuccessful.llap', 'app', \"'llap0\", \"'\", 'deployment', 'unsuccessful.interactive', 'query']),\n",
       "       list(['llap', 'appication', 'is', 'not', 'working', 'since', 'last', '4', 'hourshive', 'jobs', 'was', 'not', 'able', 'to', 'be', 'executed', 'for', '4', 'hoursinteractive', 'query']),\n",
       "       list(['llap', 'cluster', ':', 'scale', 'up', 'issuesllap', 'cluster', ':', 'scale', 'up', 'issuesissue', 'with', 'scaling', 'up']),\n",
       "       list(['looking', 'for', 'cluster', 'usage', 'detailscluster', 'was', 'created', 'with', '200', 'workers', ',', 'and', 'customer', 'wanted', 'to', 'understand', 'the', 'cluster', 'utilization', 'over', 'a', '2', 'month', 'period', '.', 'if', 'possible', ',', 'they', 'wanted', 'to', 'reduce', 'the', 'size', 'of', 'the', 'cluster', 'to', 'realize', 'a', 'cost', 'savings.cluster', 'metrics', 'on', 'azure', 'portal']),\n",
       "       list(['looking', 'to', 'tune', 'mdsd', 'process', 'on', 'edge', 'nodeapps', 'are', 'having', 'issues', ',', 'like', 'hanging', ',', 'crashing', ',', 'needing', 'to', 'be', 'restarted', 'on', 'a', 'regular', 'basis.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['lsv2', 'series', 'vms', 'are', 'not', 'an', 'option', 'for', 'hadoop', 'clusterslsv2', 'series', 'vms', 'are', 'not', 'an', 'option', 'for', 'hadoop', 'clusterscreate', 'failure', '-', 'other']),\n",
       "       list(['mapreduce', 'activity', 'is', 'hangedunable', 'to', 'run', 'map', 'reduce', 'jobshdinsight', '(', 'hive', ',', 'mapreduce', ',', 'pig', ',', 'spark', ',', 'streaming', ')']),\n",
       "       list(['mdh', 'distribuition', ':', 'problem', 'with', 'deploying', 'the', 'esp', 'clustersmdh', 'distribuition', ':', 'problem', 'with', 'deploying', 'the', 'esp', 'clusterscreate', 'failure', '-', 'other']),\n",
       "       list(['memory', 'and', 'vcores', 'available', 'in', 'a', 'cluster', 'for', 'spark', 'jobs', 'in', 'yarnmemory', 'and', 'vcores', 'available', 'in', 'a', 'cluster', 'for', 'spark', 'jobs', 'in', 'yarnspark']),\n",
       "       list(['mfa', 'is', 'not', 'working', 'for', 'users', 'user', 'was', 'able', 'to', 'login', 'to', 'ambari', 'with/without', 'mfaambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['mi', 'failing', 'to', 'connect', 'to', 'adlsgen2', 'during', 'hdinsight', 'deploymentmi', 'failing', 'to', 'connect', 'to', 'adlsgen2', 'during', 'hdinsight', 'deployment', 'hdinsight', 'servicecreate', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['micro-sgementationadvisory', 'information', 'question', 'for', 'routing', 'hdinsight', 'traffic', 'from', 'nodes', 'to', 'a', 'firewall', 'in', 'micro-segment', 'networkcreate', 'failure', '-', 'other']),\n",
       "       list(['missing', 'hbase', 'metricsmissing', 'hbase', 'metricshbase']),\n",
       "       list(['mr', 'jobs/pipeline', 'running', 'slowly', 'intermittently120032124000601', '-', 'mr', 'jobs/pipeline', 'running', 'slowly', 'intermittentlynode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['msi', 'certification', 'renew', 'workflow', 'failedmsi', 'certification', 'renew', 'workflow', 'failedambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['multiple', 'services', 'like', 'hs2i', ',', 'tez', 'am', \"'s\", 'are', 'downmultiple', 'services', 'like', 'hs2i', ',', 'tez', 'am', \"'s\", 'are', 'downinteractive', 'query']),\n",
       "       list(['my', 'webjobs', 'are', 'failing', 'to', 'submit', 'spark', 'jobsmy', 'webjobs', 'are', 'failing', 'to', 'submit', 'spark', 'jobsspark']),\n",
       "       list(['name', 'node', 'checkpoint', 'errors', 'in', 'hdi', 'clustername', 'node', 'checkpoint', 'alerts', 'in', 'ambarihadoop']),\n",
       "       list(['name', 'nodes', 'unreachable', 'while', 'on', 'autoscalename', 'nodes', 'for', 'the', 'cluster', 'went', 'down', '.', 'we', 'restarted', 'the', 'hdfs', 'services', ',', 'name', 'nodes', 'were', 'up', 'and', 'then', 'went', 'down', 'again', 'after', 'health', 'check', '.', ';', 'increase', 'in', 'load', '?', '-', 'more', 'data', 'to', 'be', 'processed', ';', 'lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['namenode', 'last', 'checkpointnamenode', 'last', 'checkpoint', 'hdinsight', 'servicehbase']),\n",
       "       list(['nan']),\n",
       "       list(['need', 'confirmation', 'for', 'the', 'change', 'in', 'property', 'in', 'coresite.xmlneed', 'confirmation', 'for', 'the', 'change', 'in', 'property', 'in', 'coresite.xmlodbc', 'or', 'jdbc']),\n",
       "       list(['need', 'details', 'from', 'old', 'ticket', '#', '119030723003161perforamance', 'issues', 'during', 'large', 'data', 'transfers', 'spark']),\n",
       "       list(['need', 'help', 'adding', 'application', 'id', 'to', 'kafka', 'zookeeper', 'acls', 'to', 'allow', 'it', 'to', 'create', 'topicsadvisorykafka']),\n",
       "       list(['need', 'help', 'adding', 'disks', 'to', 'hdinsight', 'kafka', 'nodesneed', 'help', 'adding', 'disks', 'to', 'hdinsight', 'kafka', 'nodes', 'kafka']),\n",
       "       list(['need', 'help', 'on', 'deploying', 'hdinisight', 'clusterneed', 'help', 'on', 'deploying', 'hdinisight', 'clustercreate', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['need', 'help', 'with', 'vulnerability', 'questionsgeneral', 'questionhive']),\n",
       "       list(['need', 'ip', 'v6', 'disabled', 'on', 'gateway', 'nodesunable', 'to', 'log', 'into', 'cluster', 'created', 'in', 'newer', 'vnets.ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['need', 'performance', 'tunning', 'in', 'hdi', 'clustercustomer', 'wanted', 'to', 'know', 'if', 'their', 'cluster', 'could', 'be', 'scaled', 'downhive']),\n",
       "       list(['need', 'to', 'access', 'the', 'hdi', 'cluster', 'through', 'wvd', '(', 'windows', 'virtual', 'desktop', ')', 'need', 'to', 'access', 'the', 'hdi', 'cluster', 'through', 'wvd', '(', 'windows', 'virtual', 'desktop', ')', 'create', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['need', 'to', 'cjhange', 'vm', 'family', 'for', 'my', 'worker', 'nodeunable', 'to', 'scale', 'the', 'clusterissue', 'with', 'scaling', 'up']),\n",
       "       list(['need', 'to', 'determine', 'need', 'for', 'ssh11', 'rule', '(', 'telnet', ')', 'need', 'to', 'determine', 'need', 'for', 'ssh11', 'rule', '(', 'telnet', ')', 'configure', 'load', 'balancer']),\n",
       "       list(['need', 'to', 'migrate', 'hdinsight', 'resources', 'from', 'one', 'subscription', 'to', 'anotherneed', 'to', 'migrate', 'hdinsight', 'resources', 'from', 'one', 'subscription', 'to', 'anothercreate', 'failure', '-', 'other']),\n",
       "       list(['need', 'to', 'raise', 'core', 'limitneed', 'to', 'raise', 'core', 'limitcreate', 'failure', 'due', 'to', 'quota', 'limit']),\n",
       "       list(['new', 'cluster', 'build', 'with', 'external', 'metastores', 'for', 'hive', ',', 'oozie', ',', 'and', 'ranger', 'fails', 'on', 'authentication', 'to', 'the', 'metastore', 'using', 'correct', 'credentialsnew', 'cluster', 'build', 'with', 'external', 'metastores', 'for', 'hive', ',', 'oozie', ',', 'and', 'ranger', 'fails', 'on', 'authentication', 'to', 'the', 'metastore', 'using', 'correct', 'credentialscreate', 'failure', '-', 'other']),\n",
       "       list(['newly', 'added', 'ad', 'group', 'is', 'not', 'showing', 'in', 'rangernewly', 'added', 'ad', 'group', 'is', 'not', 'showing', 'in', 'rangerspark']),\n",
       "       list(['newly', 'created', 'cluster', 'is', 'brokennewly', 'created', 'cluster', 'is', 'broken.create', 'failure', '-', 'other']),\n",
       "       list(['no', 'able', 'to', 'connect', 'to', 'hive', 'odbc', 'driver', 'from', 'rs06ue2dipws17.https', ':', '//supportability.visualstudio.com/azurehdinsight/_wiki/wikis/azurehdinsight/315085/hive-2.6.7-odbc-driver-fails-to-connect-to-hadoop-clusterodbc', 'or', 'jdbc', 'connecting', 'to', 'standard', 'cluster']),\n",
       "       list(['no', 'active', 'rm', 'on', 'cluster120060821007094', 'no', 'active', 'rm', 'on', 'cluster', 'hdinsight', 'servicehadoop']),\n",
       "       list(['no', 'module', 'named', \"'azure.storage'no\", 'module', 'named', 'azure.storagenotebooks']),\n",
       "       list(['node', 'hn0-dev-sp', 'unresponsiveheadnodes', 'unresponsiveunable', 'to', 'ssh']),\n",
       "       list(['node', 'manager', 'unhealthy', 'warnings', 'in', 'ambarimapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['node', 'unrespondingunable', 'to', 'ssh', 'and', 'node', 'is', 'unresponsivenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['node', 'wn0-pro-ka', '(', '192.168.15.12', ')', 'is', 'unreachable', ',', 'no', 'ssh', 'acces', ',', 'no', 'ping', 'response', ',', 'ambari', 'shows', 'node', 'as', 'heartbeat', 'lost120042321002261', '-', 'node', 'wn0-pro-ka', '(', '192.168.15.12', ')', 'is', 'unreachable', ',', 'no', 'ssh', 'acces', ',', 'no', 'ping', 'response', ',', 'ambari', 'shows', 'node', 'as', 'heartbeat', 'lost', 'hdinsight', 'servicenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['nodemanager', 'decommissioning', 'is', 'not', 'working', 'through', 'ambari', 'uinodemanager', 'decommissioning', 'is', 'not', 'working', 'through', 'ambari', 'uihbase']),\n",
       "       list(['nodemanager', 'unhealthy1/1', 'local-dirs', 'have', 'errors', ':', '[', '/mnt/resource/hadoop/yarn/local', ':', 'directory', 'is', 'not', 'writable', ':', '/mnt/resource/hadoop/yarn/local', ']', '1/1', 'log-dirs', 'have', 'errors', ':', '[', '/mnt/resource/hadoop/yarn/log', ':', 'can', 'not', 'create', 'directory', ':', '/mnt/resource/hadoop/yarn/log', ']', 'node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['none', 'of', 'azure', 'hdinsight', 'resource', 'is', 'having', 'acceess', 'in', 'its', 'underlying', 'source', '.', 'ie', ':', 'adls', 'gen-1none', 'of', 'azure', 'hdinsight', 'resource', 'is', 'having', 'acceess', 'in', 'its', 'underlying', 'source', '.', 'ie', ':', 'adls', 'gen-1create', 'failure', '-', 'other']),\n",
       "       list(['nonprod', '-', 'kpq041hbasefdqawus201', '-', 'cleanup', 'hbase', 'table', 'in', 'transitionnonprod', '-', 'kpq041hbasefdqawus201', '-', 'cleanup', 'hbase', 'table', 'in', 'transitionhbase']),\n",
       "       list(['nonprod', '-', 'kpq060sparkadfqawus201', '-', 'node', 'not', 'responsive', 'nonprod', '-', 'kpq060sparkadfqawus201', '-', 'node', 'not', 'responsivenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['not', 'able', 'run', 'queries', 'from', 'ambari', '-', 'hive', 'view', ',', 'using', 'hiveserver2', 'interactiveorg.apache.ambari.view.hive20.internal.connectionexception', ':', 'can', 'not', 'open', 'a', 'hive', 'connection', 'with', 'connect', 'string', 'jdbc', ':', 'hive2', ':', '//zk0-kpph13.kpaaddsprod.onmicrosoft.com:2181', ',', 'zk1-kpph13.kpaaddsprod.onmicrosoft.com:2181', ',', 'zk3-kpph13.kpaaddsprod.onmicrosoft.com:2181/', ';', 'servicediscoverymode=zookeeper', ';', 'zookeepernamespace=hiveserver2-hive2', ';', 'hive.server2.proxy.user=admin', 'question', ':', 'interactive', 'query', 'explain', 'plan', 'if', 'availableinteractive', 'query']),\n",
       "       list(['not', 'able', 'to', 'access', 'the', 'server.getting', '502', 'bad', 'gateway', 'error', 'while', 'accessing', 'ambarispark']),\n",
       "       list(['not', 'able', 'to', 'bind', 'the', 'same', 'storage', 'account', 'to', 'the', 'newly', 'created', 'cluster', 'cluster', 'provisioning', 'failed', 'and', 'reported', 'sts', 'would', 'not', 'start', '.', 'while', 'investigating', ',', 'noticed', 'hive', 'metastore', 'service', 'was', 'not', 'upcreate', 'failure', '-', 'other']),\n",
       "       list(['not', 'able', 'to', 'bind', 'the', 'same', 'storage', 'account', 'to', 'the', 'newly', 'created', 'clusterschema', 'upgrade', 'script', 'failed', 'to', 'import', 'com.microsoft.storage.blobservicecreate', 'failure', '-', 'other']),\n",
       "       list(['not', 'able', 'to', 'call', 'oozie', 'rest', 'apinot', 'able', 'to', 'call', 'oozie', 'rest', 'apihadoop']),\n",
       "       list(['not', 'able', 'to', 'choose', 'head', 'node', 'as', 'a2m', 'v2', 'instancenot', 'able', 'to', 'choose', 'head', 'node', 'as', 'a2m', 'v2', 'instancecreate', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['not', 'able', 'to', 'connect', 'qa', 'hdi', 'cluster', 'from', 'odbc', 'but', 'same', 'working', 'fine', 'for', 'dev', 'hdi', 'cluster.not', 'able', 'to', 'connect', 'qa', 'hdi', 'cluster', 'from', 'odbc', 'but', 'same', 'working', 'fine', 'for', 'dev', 'hdi', 'cluster.hive']),\n",
       "       list(['not', 'able', 'to', 'connect', 'the', 'hive', 'odbc', 'driverunable', 'to', 'connect', 'using', 'odbc', 'driver', 'and', 'groups', 'were', 'not', 'syncing.odbc', 'or', 'jdbc', 'connecting', 'to', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['not', 'able', 'to', 'connect', 'to', 'cluster', 'endpoint', 'https', ':', '//clustername-int.azurehdinsight.net', 'from', 'head', 'nodenot', 'able', 'to', 'connect', 'to', 'cluster', 'endpoint', 'https', ':', '//clustername-int.azurehdinsight.net', 'from', 'head', 'node', 'using', 'curllost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['not', 'able', 'to', 'create', 'cluster', 'with', 'express', 'route', 'not', 'able', 'to', 'create', 'cluster', 'with', 'express', 'routecreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['not', 'able', 'to', 'create', 'hdinsight', 'cluster', 'because', 'quota', 'limitssymptom:120060621000707', '-', 'not', 'able', 'to', 'create', 'hdinsight', 'cluster', 'because', 'quota', 'limits', 'issue', ':', 'the', 'hdinsight', 'cluster', 'deployment', 'failed', 'at', '2020-06-06', '12:55:23', 'utc', 'and', 'again', 'at', '2020-06-06', '14:09:24', 'utccreate', 'failure', '-', 'other']),\n",
       "       list(['not', 'able', 'to', 'create', 'topicsnot', 'able', 'to', 'create', 'topicsranger', 'policy', 'auditing']),\n",
       "       list(['not', 'able', 'to', 'fetch', 'logs', 'from', 'tez_viewnot', 'able', 'to', 'fetch', 'logs', 'from', 'tez_viewhbase']),\n",
       "       list(['not', 'able', 'to', 'login', 'user', '(', 'a1002634', '@', 'jci.com', ')', 'in', 'to', 'hdi', 'clusternot', 'able', 'to', 'login', 'user', '(', 'a1002634', '@', 'jci.com', ')', 'in', 'to', 'hdi', 'clusterhive']),\n",
       "       list(['not', 'able', 'to', 'run', 'parallel', 'sqoop', 'jobsissues', 'with', 'running', 'sqoop', 'jobs', 'in', 'parallel.mapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['not', 'able', 'to', 'see', 'lenses.io', 'as', 'an', 'application', 'to', 'addnot', 'able', 'to', 'see', 'lenses.io', 'as', 'an', 'application', 'to', 'addcreate', 'failure', '-', 'other']),\n",
       "       list(['not', 'receiving', 'ambari', 'alertsnot', 'receiving', 'ambari', 'alertshbase']),\n",
       "       list(['not', 'startingdatabricks', 'clusters', 'are', 'failing', 'to', 'start', 'create', 'failure', '-', 'other']),\n",
       "       list(['notservingregionexception', 'while', 'app', 'team', 'running', 'their', 'queries2020-06-05', '13:24:05.951', 'o.a.h.h.c.asyncprocess', '[', 'info', ']', '#', '1', ',', 'table=message_archive_2', ',', 'attempt=31/35', 'failed=17ops', ',', 'last', 'exception', ':', 'org.apache.hadoop.hbase.notservingregionexception', ':', 'org.apache.hadoop.hbase.notservingregionexception', ':', 'region', 'message_archive_2', ',', '\\\\\\\\x02pl6xx_prd15688965362,1591078168659.afedcc36bcf3fb90afdf4bf3c1461555', '.', 'is', 'not', 'online', 'on', '10.2.0.74,16020,1591092483820', 'at', 'org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname', '(', 'hregionserver.java:3077', ')', 'at', 'org.apache.hadoop.hbase.regionserver.rsrpcservices.getregion', '(', 'rsrpcservices.java:1015', ')', 'at', 'org.apache.hadoop.hbase.regionserver.rsrpcservices.multi', '(', 'rsrpcservices.java:2096', ')', 'at', 'org.apache.hadoop.hbase.protobuf.generated.clientprotos', '$', 'clientservice', '$', '2.callblockingmethod', '(', 'clientprotos.java:32393', ')', 'at', 'org.apache.hadoop.hbase.ipc.rpcserver.call', '(', 'rpcserver.java:2150', ')', 'at', 'org.apache.hadoop.hbase.ipc.callrunner.run', '(', 'callrunner.java:112', ')', 'at', 'org.apache.hadoop.hbase.ipc.rpcexecutor', '$', 'handler.run', '(', 'rpcexecutor.java:187', ')', 'at', 'org.apache.hadoop.hbase.ipc.rpcexecutor', '$', 'handler.run', '(', 'rpcexecutor.java:167', ')', 'hbase']),\n",
       "       list(['null', 'vs', 'empty', 'issue', 'in', 'sparknull', 'vs', 'empty', 'issue', 'in', 'sparkspark']),\n",
       "       list(['observing', 'failures', \"'sparkexception\", ':', \"'\", 'in', 'the', 'logs', ',', 'the', 'data', 'load', 'is', 'not', 'happening', 'observing', 'failures', \"'sparkexception\", ':', \"'\", 'in', 'the', 'logs', ',', 'the', 'data', 'load', 'is', 'not', 'happeningspark']),\n",
       "       list(['observing', 'issue', 'while', 'querying', 'hive', \"'failed\", 'to', 'execute', 'tez', \"graph'failed\", 'tez', 'jobshive']),\n",
       "       list(['one', 'data', 'node', 'is', 'deadone', 'data', 'node', 'is', 'deadlost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['one', 'of', 'the', 'nodes', 'lost', 'hearbeatone', 'of', 'the', 'nodes', 'lost', 'hearbeatnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['one', 'of', 'the', 'worker', 'node', 'went', 'down', 'in', 'prodvm', 'inaccessable', 'unable', 'to', 'ssh']),\n",
       "       list(['oneforoneblockfetcher:138', '-', 'failed', 'while', 'starting', 'block', 'fetchesoneforoneblockfetcher:138', '-', 'failed', 'while', 'starting', 'block', 'fetchesspark']),\n",
       "       list(['only', '38', 'out', 'of', '40', 'node', 'managers', 'are', 'activeonly', '38', 'out', 'of', '40', 'node', 'managers', 'are', 'active.spark']),\n",
       "       list(['oom', 'and', 'java', 'heap', 'size', 'issuesvertex', 'failed', ',', 'vertexname=map', '9', ',', 'vertexid=vertex_1587680811081_0480_1_02', ',', 'diagnostics=', '[', 'task', 'failed', ',', 'taskid=task_1587680811081_0480_1_02_000003', ',', 'diagnostics=', '[', 'taskattempt', '0', 'failed', ',', 'info=', '[', 'error', ':', 'error', 'while', 'running', 'task', '(', 'failure', ')', ':', 'java.lang.runtimeexception', ':', 'map', 'operator', 'initialization', 'failedspark']),\n",
       "       list(['oozie', 'failure', 'with', 'keberos', 'authentication', 'errorintermittent', 'issue', ':', 'error', 'abstractcredentialservicecaller', ':', 'connection', 'refused', '(', 'connection', 'refused', ')', 'ambari', 'in', 'standard', 'cluster']),\n",
       "       list(['oozie', 'job', 'can', 'not', 'find', 'kerberos', 'ticketoozie', 'job', 'can', 'not', 'find', 'kerberos', 'ticketmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['oozie', 'job', 'failed', 'at', 'final', 'stage', 'because', 'of', 'kerberos', 'authentication', 'problemoozie', 'job', 'failed', 'at', 'final', 'stage', 'because', 'of', 'kerberos', 'authentication', 'problemambari', 'in', 'standard', 'cluster']),\n",
       "       list(['orc', 'file', 'and', 'azureblobfilesystem', 'api', 'issuefailed', 'to', 'get', 'files', 'with', 'id', ';', 'using', 'regular', 'api', ':', 'only', 'supported', 'for', 'dfs', ';', 'got', 'class', 'org.apache.hadoop.fs.azurebfs.secureazureblobfilesystemfailed', 'to', 'get', 'files', 'with', 'id', ';', 'using', 'regular', 'api', ':', 'only', 'supported', 'for', 'dfs', ';', 'got', 'class', 'org.apache.hadoop.fs.azurebfs.azureblobfilesystembeeline']),\n",
       "       list(['our', 'dns', 'in', 'sao', 'paulo', 'has', 'no', 'route', 'to', 'hdinsight', 'in', 'virginiaadd', 'new', 'dns', 'servers', 'to', 'the', 'vnetkafka']),\n",
       "       list(['out', 'of', 'memory', 'errorsout', 'of', 'memory', 'errors.hive']),\n",
       "       list(['overall', 'cluster', \"'s\", 'performance', 'is', 'extremely', 'slowslower', 'query', 'performancehive']),\n",
       "       list(['performing', 'a', 'simple', \"'insert\", 'into', \"'\", 'in', 'hive', 'db', 'take', '12', 'secondsperforming', 'a', 'simple', \"'insert\", 'into', \"'\", 'in', 'hive', 'db', 'take', '12', 'secondsinteractive', 'query']),\n",
       "       list(['permission', 'error', '(', 'hive', 'user', ')', 'trying', 'to', 'create', 'table', 'from', 'spark', 'dataframe', '.', 'config', 'option', '“', 'run', 'as', 'end', 'user', 'instead', 'of', 'hive', 'user', '”', 'is', 'setpermission', 'error', '(', 'hive', 'user', ')', 'trying', 'to', 'create', 'table', 'from', 'spark', 'dataframe', '.', 'config', 'option', '“', 'run', 'as', 'end', 'user', 'instead', 'of', 'hive', 'user', '”', 'is', 'set.spark']),\n",
       "       list(['pin', 'the', 'hdi', '4', 'spark', 'clusters', 'to', '3.1.2.7-1', 'epin', 'the', 'hdi', '4', 'spark', 'clusters', 'to', '3.1.2.7-1spark']),\n",
       "       list(['pipeline', 'got', 'failed', '(', 'sephoramasterpipeline', ')', 'spark', 'was', 'not', 'able', 'to', 'connect', 'to', 'key', 'vaulthdinsight', '(', 'hive', ',', 'mapreduce', ',', 'pig', ',', 'spark', ',', 'streaming', ')']),\n",
       "       list(['pipeline', 'run', 'failed', '.', 'we', 'see', 'very', 'weird', 'error', '.', '[', 'csat', 'impacting', ']', 'adf', 'jobs', 'were', 'failing', 'intermittentlyhdinsight', 'sdk']),\n",
       "       list(['please', 'increase', 'our', 'cores', 'limit', 'in', 'centralus', 'from', '260', 'to', '300please', 'increase', 'our', 'cores', 'limit', 'in', 'central', 'us', 'from', '260', 'to', '300create', 'failure', 'due', 'to', 'quota', 'limit']),\n",
       "       list(['please', 'upgrade', 'cluster', 'size', 'to', 'additional', '7', 'nodesunable', 'to', 'scale', 'up', '.', 'clustersissue', 'with', 'scaling', 'up']),\n",
       "       list(['poor', 'cluster', 'performance120040224001297', '-', 'poor', 'cluster', 'performance', 'hdinsight', 'service', 'and', 'ascii', 'characters', 'in', 'hbase', 'column', 'nameshbase']),\n",
       "       list(['prddm07', '-', 'user', 'not', 'able', 'to', 'expand', 'the', 'table', 'names', 'some', 'tables', 'do', 'not', 'expand', 'to', 'show', 'the', 'column', 'names', '120060223002752', 'some', 'users', 'unable', 'to', 'run', 'queries', 'from', 'alteryx', 'using', 'hive', 'odbc', 'connectionodbc', 'or', 'jdbc']),\n",
       "       list(['prdsup', '-', 'kp50kafkaadfhdiprdsupusc01', '-', 'unhealthy', 'zookeeper', 'nodeprdsup', '-', 'kp50kafkaadfhdiprdsupusc01', '-', 'unhealthy', 'zookeeper', 'nodenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['prdsup', '-', 'kpp109sparkespprdsupwus201', '-', 'edge', 'node', 'deployment', 'failrekpp109sparkespprdsupwus201', '-', 'edge', 'node', 'deployment', 'failrenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['prdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'cluster', 'scaling', 'incompletecluster', 'did', 'not', 'upscale', '.', 'ran', 'for', '1', 'hr', ',', '30', 'minsissue', 'with', 'scaling', 'up']),\n",
       "       list(['prdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'history', 'server', '(', 'not', 'aggregating', 'comleted', 'jobs', ')', 'prdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'history', 'server', '(', 'not', 'aggregating', 'comleted', 'jobs', ')', 'spark']),\n",
       "       list(['prdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'lock', 'on', 'adls', 'locationsprdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'lock', 'on', 'adls', 'locationsspark']),\n",
       "       list(['prdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'jobs', 'are', 'taking', 'longer', 'time', 'and', 'failingprdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'jobs', 'are', 'taking', 'longer', 'time', 'and', 'failingspark']),\n",
       "       list(['prdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'not', 'able', 'to', 'execute', 'spark', 'query', 'from', 'de', 'wfunable', 'to', 'execute', 'queries', 'and', 'a', 'lot', 'of', 'gc', 'pauses', 'in', 'hivemetastore', 'logsspark']),\n",
       "       list(['prdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'spark', 'service', 'downprdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'spark', 'service', 'downspark']),\n",
       "       list(['prdsup', ':', 'kp08tntncapsparknsprdsup01', ':', 'spark-shell', 'is', 'not', 'workingissue', ':', 'spark-shell', 'does', 'not', 'work', 'when', 'running', 'simple', 'sql', 'commandsspark']),\n",
       "       list(['prdsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'hiveserver2', 'interactive', 'service', 'stoppedprdsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'hiveserver2', 'interactive', 'service', 'stoppedhive']),\n",
       "       list(['prdsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'nodemanager', 'health', '-no', 'more', 'usable', 'spaceprdsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'nodemanager', 'health', '-no', 'more', 'usable', 'spacehive']),\n",
       "       list(['prdsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'there', 'is', 'not', 'a', 'single', 'ncap', 'query', 'run', 'successfully', 'since', '7', 'am', 'today', 'morning', '.', 'let', 'us', 'restart', 'please.unable', 'to', 'execute', 'any', 'queries', 'from', 'hive', 'view', 'and', 'beelinehive']),\n",
       "       list(['prdsup', ':', 'kpph10llapprdsupusc01', ':', 'hive', 'query', 'failinghive', 'queries', 'are', 'failinghive']),\n",
       "       list(['prdsup', ':', 'kpph18llapprdsup', ':', 'noticing', 'other', 'connection', 'see', 'the', 'gateway', 'logs', 'on', 'the', 'cluster', 'to', 'debug', 'an', 'issue', 'from', 'local', 'to', 'hdinsight', '.', 'experience', 'the', 'no', 'domain', 'controller', 'exceptions', 'in', 'gw', 'logsinteractive', 'query']),\n",
       "       list(['prdsup', ':', 'kpph18llapprdsupusc01', '&', 'kp51sparkadfhdiprdsupusc01', ':', 'ddls', 'are', 'not', 'updating', 'prdsup', ':', 'kpph18llapprdsupusc01', '&', 'kp51sparkadfhdiprdsupusc01', ':', 'ddls', 'are', 'not', 'updatinghive']),\n",
       "       list(['prdsup', ':', 'kpph18llapprdsupusc01', ':', 'application', 'can', 'connect', 'to', 'the', 'cluster', 'via', 'odbc', '.', 'the', 'query', 'doesn', '’', 't', 'complete', '.', 'prdsup', ':', 'kpph18llapprdsupusc01', ':', 'application', 'can', 'connect', 'to', 'the', 'cluster', 'via', 'odbc', '.', 'the', 'query', 'doesn', '’', 't', 'complete.interactive', 'query']),\n",
       "       list(['prdsup', ':', 'kpph18llapprdsupusc01', ':', 'hive', 'service2', 'intractive', 'stoppedhive', 'service2', 'intractive', 'stoppedhive']),\n",
       "       list(['prdsup', ':', 'kpph18llapprdsupusc01', ':', 'hiveserver2', 'interactive', 'stoppedprdsup', ':', 'kpph18llapprdsupusc01', ':', 'hiveserver2', 'interactive', 'stoppedhive']),\n",
       "       list(['prdsup', ':', 'kpps83sparkespprdsupwus201', ':', 'most', 'of', 'the', 'services', 'downissue', ':', 'cluster', 'services', 'down', 'including', 'hive', 'interactive', 'service', '.', 'spark']),\n",
       "       list(['prdsup', ';', 'kp10tntncapllapnsprdsup01', ':', 'hiverserver', 'interacive', 'service', 'downhive', 'interactive', 'going', 'down', 'very', 'ofteninteractive', 'query']),\n",
       "       list(['problems', 'to', 'send', 'email', 'alerts', 'in', 'ambariproblems', 'to', 'send', 'email', 'alerts', 'in', 'ambarihadoop']),\n",
       "       list(['problems', 'with', 'the', 'service', \"'mdsd'problems\", 'with', 'the', 'service', \"'mdsd'node\", 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['prod', '-', 'kpp608sparkprodwus201', '-', 'cluster', 'created', 'with', 'wrong', 'zookeeper', 'sku', 'hdinsight', 'for', 'spark', 'cluster', 'type', 'does', 'not', 'support', 'any', 'sku', 'other', 'than', 'standard_a2_v2', 'create', 'failure', '-', 'other']),\n",
       "       list(['prod', ':', 'kp01sparkadfhdiprodusc01', ':', 'hive', 'is', 'downhms', 'and', 'ats', 'heap', 'size', 'issueshive']),\n",
       "       list(['prod', ':', 'kp01sparkadfhdiprodusc01', ':', 'spark2', 'server', 'downspark2', 'history', 'server', 'goes', 'down', 'with', 'out', 'of', 'memory', 'issuespark']),\n",
       "       list(['prod', ':', 'kp02hbaseadfhdiprodusc01', ':', 'region', 'server', 'is', 'not', 'online/notservingregionexception', 'in', 'prod', 'hbase', 'clusterwe', 'identified', 'the', '“', 'error', ':', 'found', 'lingering', 'hfilelink', '”', 'and', 'assume', 'that', ',', 'temporary', 'file', 'left', 'behind', 'because', 'of', 'bulkload', 'operation', 'and', 'move', 'those', 'files', 'out', 'of', 'the', 'region.hbase']),\n",
       "       list(['prod', ':', 'kpph16llapprodusc01', ':', 'hiveserver2', 'interactive', 'stoppedprod', ':', 'kpph16llapprodusc01', ':', 'hiveserver2', 'interactive', 'stoppedhive']),\n",
       "       list(['prod', 'cluster', 'performance', 'low', '+wfh', '+|', 'delay', 'in', 'data', 'loadprod', 'cluster', 'performance', 'low', '+wfh', '+|', 'delay', 'in', 'data', 'loadspark']),\n",
       "       list(['prodsup', ':', 'hive', 'services', '-', 'llap', 'application', 'is', 'downgc', 'pauses', 'in', 'hive', 'logsinteractive', 'query']),\n",
       "       list(['prodsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'hive', 'service', 'down.prodsup', ':', 'kp10tntncapllapnsprdsup01', ':', 'hive', 'service', 'down.interactive', 'query']),\n",
       "       list(['prodsup', 'clusters', ':', 'with', 'the', 'esp', 'service', 'accounts', 'not', 'able', 'to', 'login', 'to', 'ambari', 'portalprodsup', 'clusters', ':', 'with', 'the', 'esp', 'service', 'accounts', 'not', 'able', 'to', 'login', 'to', 'ambari', 'portalinteractive', 'query']),\n",
       "       list(['production', 'hdi', 'cluster', 'ambari', 'ui', 'and', 'metrics', 'missing/slowproduction', 'hdi', 'cluster', 'ambari', 'ui', 'and', 'metrics', 'missing/slowhbase']),\n",
       "       list(['program', 'to', 'load', 'a', 'file', 'to', 'adls', 'and', 'it', 'failed', 'with', 'accesstoken', 'exception', 'unable', 'to', 'run', 'application', 'due', 'to', 'access', 'tokenhive']),\n",
       "       list(['provision', 'edge', 'node', 'admin', 'account', 'for', 'debuggingneed', 'elevated', 'access', 'for', 'security', 'troubleshooting', 'purposesunable', 'to', 'ssh']),\n",
       "       list(['public', 'outgoing', 'ippublic', 'outgoing', 'ip.lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['pyspark', 'and', 'key', 'vault120030224006133', '-', 'customer', 'wanted', 'options', 'for', 'using', 'azure', 'key', 'vault', 'in', 'a', 'pyspark', 'application', 'without', 'hard-coding', 'the', 'required', 'options', '(', 'client_id', ',', 'secret', ',', 'and', 'tenant', ')', 'notebook', 'in', 'standard', 'cluster']),\n",
       "       list(['python', '3.7', 'on', 'hdi', 'clusterneed', 'to', 'have', 'python', '3.7', 'installed', 'on', 'hdi', 'cluster', 'during', 'the', 'creation.can', 'you', 'please', 'check', 'and', 'update', 'the', 'backend', 'scripts', 'or', 'provide', 'the', 'officical', 'ppa', 'to', 'install', 'the', 'same.create', 'failure', '-', 'other']),\n",
       "       list(['qa', ':', 'kp05hbasefdhdiqausc01', ':', 'not', 'able', 'to', 'access', 'node', 'unable', 'to', 'ssh', 'in', 'to', 'wn0unable', 'to', 'ssh']),\n",
       "       list(['qa', ':', 'kp05hbasefdhdiqausc01', ':', 'not', 'able', 'to', 'run', 'the', 'queriesqa', ':', 'kp05hbasefdhdiqausc01', ':', 'not', 'able', 'to', 'run', 'the', 'querieshbase']),\n",
       "       list(['qa', ':', 'llap', 'application', '(', 'slider', 'app', ')', 'is', 'down.qa', ':', 'llap', 'application', '(', 'slider', 'app', ')', 'is', 'down.interactive', 'query']),\n",
       "       list(['qa', 'cluster', 'not', 'auto-scaling', '-', 'hdi4qarsiappqa', 'cluster', 'not', 'auto-scaling', '-', 'hdi4qarsiappissue', 'with', 'scaling', 'up']),\n",
       "       list(['qa-ambari', 'metrics', 'collector', 'failes', 'to', 'connect120030922002819', '-', 'qa-ambari', 'metrics', 'collector', 'fails', 'to', 'connecthadoop']),\n",
       "       list(['queries', 'are', 'not', 'completing', '.', 'zookeper', 'service', 'down.ambari', 'website', 'down/unresponsive', '.', 'qlik', 'odbc', 'servers', 'unable', 'to', 'make', 'connections', 'to', 'cluster', \"'s\", 'thrift', 'endoint', '.', 'zookeeper', 'unhealthy', 'and', 'unable', 'to', 'form', 'a', 'quorumhive']),\n",
       "       list(['queries', 'get', 'to', '100', '%', 'than', 'hang', 'for', '15+', 'hoursqueries', 'taking', 'longer', 'than', 'usual', 'to', 'execute', '.', 'some', 'queries', 'reach', '100', '%', 'then', 'hang', 'for', '15+', 'hours.hive']),\n",
       "       list(['query', 'delta', 'lake', 'format', 'from', 'llapcustomer', 'advisory', 'question.odbc', 'or', 'jdbc']),\n",
       "       list(['query', 'exectuion', 'failures', ':', 'query', 'running', 'through', 'beeline', ',', 'but', 'not', 'through', 'vs', 'code', 'or', 'zeppelinquery', 'exectuion', 'failures', ':', 'query', 'running', 'through', 'beeline', ',', 'but', 'not', 'through', 'vs', 'code', 'or', 'zeppelininteractive', 'query']),\n",
       "       list(['query', 'hanginghive', 'query', 'hanging', 'hive']),\n",
       "       list(['query', 'is', 'running', 'really', 'slow', 'also', 'not', 'able', 'to', 'see', 'all', 'the', 'dags', 'in', 'tez', 'viewquery', 'is', 'running', 'really', 'slow', 'also', 'not', 'able', 'to', 'see', 'all', 'the', 'dags', 'in', 'tez', 'viewhive']),\n",
       "       list(['query', 'performance', 'issue.cluster', 'performance', 'degradation.interactive', 'query']),\n",
       "       list(['query', 'slownessquery', 'slownessinteractive', 'query']),\n",
       "       list(['question', 'about', 'jmx', 'rmi', 'on', 'kafka', 'clusterguidancekafka']),\n",
       "       list(['ranger', 'hdfs', 'setupranger', 'hdfs', 'setupadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['ranger', 'permission', 'issues', '.', 'user', 'is', 'not', 'able', 'to', 'view', 'the', 'permitted', 'tables', 'through', 'beeline', 'ranger', 'permission', 'issuesranger', 'policy', 'enforcement']),\n",
       "       list(['ranger', 'rest', 'api', 'for', 'row', 'level', 'fillterranger', 'rest', 'api', 'for', 'row', 'level', 'fillterhive']),\n",
       "       list(['read', 'latencies', 'went', 'very', 'high', 'when', 'reindexing', 'is', 'startedread', 'latencies', 'went', 'very', 'high', 'when', 'reindexing', 'is', 'startedhbase']),\n",
       "       list(['reason', 'for', 'hive', 'service', 'downhive', 'services', 'unavailable', '.', 'hive']),\n",
       "       list(['reboot', 'worker', 'node', '26worker', 'node', '26', 'is', 'unresponsive.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['receiving', 'failedtoconnectwithclusterthroughgatewayerrorcode', 'on', 'deploymentcluster', 'creation', 'failing', 'with', ':', 'failedtoconnectwithclusterthroughgatewayerrorcodecreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['recommendations', 'on', 'making', 'wasb', 'hbase', 'write', 'readycustomer', 'said', 'they', 'are', 'using', 'their', 'on', 'hbase', 'cluster', '(', 'non', 'hdinsight', ')', 'but', 'have', 'seen', 'a', 'hbase', 'cluster', 'created', '(', 'also', 'deleted', ')', '.so', 'we', 'tried', 'to', 'help', 'our', 'best', 'on', 'this', ',', 'though', 'we', 'understood', 'customer', 'is', 'tricking', 'us', '.', 'we', 'asked', 'the', 'cluster', 'details', 'to', 'help', 'further', 'and', 'she', 'said', 'that', ',', 'do', \"n't\", 'have', 'any', 'and', 'asked', 'to', 'close', 'the', 'casehbase']),\n",
       "       list(['regionservers', 'were', 'rebooted', 'with', 'out', 'a', 'notice', '5', 'region', 'servers', 'in', 'prod', 'and', 'stage', 'were', 'rebooted', 'with', 'out', 'a', 'notification', 'to', 'customers', '.', 'was', 'there', 'any', 'outage', 'or', 'known', 'maintenance', 'on', 'hdi', 'cluster', 'from', 'you', 'end', '?', 'prod', 'cluster', ':', 'adobesearchhbaseprodva7', 'nodes', 'rebooted', ':', 'wn6', ',', 'wn2', ',', 'wn7', 'hbase']),\n",
       "       list(['removing', 'executor', '15', 'because', 'it', 'has', 'been', 'idle', 'for', '60', 'secondsevery', 'time', ',', 'after', '5-6', 'mins', 'of', 'job', 'kicked', 'off', ',', 'the', 'executors', 'reduced', 'from', '45', 'to', '2.', 'spark']),\n",
       "       list(['repeated', 'errors', 'connecting', 'to', 'sqlrepeated', 'errors', 'connecting', 'to', 'sqlspark']),\n",
       "       list(['repo.hortonworks', 'suddenly', 'unloaded', 'some', 'of', 'the', 'jar', 'versions', 'used', 'in', 'our', 'application', '.', 'storm', ',', 'phoenix', 'and', 'hbase', 'jarsrepo.hortonworks', 'suddenly', 'unloaded', 'some', 'of', 'the', 'jar', 'versions', 'used', 'in', 'our', 'application', '.', 'storm', ',', 'phoenix', 'and', 'hbase', 'jarsstorm']),\n",
       "       list(['request', 'body', 'too', 'large', 'error', 'with', 'the', 'spark', 'errors', \"''\", 'the', 'uncommitted', 'block', 'count', 'can', 'not', 'exceed', 'the', 'maximum', 'limit', 'of', '100,000', 'blocks.request', 'body', 'too', 'large', 'error', 'with', 'the', 'spark', 'errorsspark']),\n",
       "       list(['required', 'instruction', 'to', 'create', 'workload', 'managercustomer', 'requested', 'instructions', 'to', 'setup', 'the', 'hive', 'workload', 'management', 'featurehive']),\n",
       "       list(['resource', 'health', 'is', 'going', 'down', 'every', 'day', 'since', 'last', 'few', 'daysresource', 'health', 'is', 'going', 'down', 'every', 'day', 'since', 'last', 'few', 'dayshive']),\n",
       "       list(['resource', 'manager', 'downwe', 'determined', 'that', 'the', 'hdinsight', 'cluster', 'sensei-dsnpalpha-hdi', 'has', 'a', 'wasb', 'storage', 'account', 'with', 'secure', 'transfer', 'enabled', 'breaking', 'the', 'cluster', '.', 'we', 'would', 'recommend', 'you', 'recreate', 'the', 'cluster', 'and', 'enable', 'secure', 'transfer', 'while', 'deploy', 'the', 'new', 'cluster', 'or', 'if', 'you', 'want', 'to', 'fix', 'this', 'cluster', 'without', 'secure', 'transfer', ',', 'please', 'disable', 'the', 'secure', 'transfer', 'on', 'storage', 'account.hadoop']),\n",
       "       list(['resource', 'manager', 'ui', 'not', 'operationalboth', 'rm', 'standby', 'and', 'unable', 'to', 'access', 'rm', 'uihadoop']),\n",
       "       list(['resource', 'managers', 'stuck', 'in', 'standby', 'stateresource', 'managers', 'stuck', 'in', 'standby', 'statehadoop']),\n",
       "       list(['resource', 'manangers', 'are', 'stopping', 'due', 'to', ':', 'connection', 'failed', 'to', 'http', ':', '//hn0-perfwu.z5pipfv3tdyuxorx05ypbu513e.xx.internal.cloudapp.net:8088', '120030321002006', '-', 'resource', 'managers', 'are', 'stopping', 'due', 'to', ':', 'connection', 'failed', 'to', 'http', ':', '//hn0-perfwu.z5pipfv3tdyuxorx05ypbu513e.xx.internal.cloudapp.net:8088', 'hdinsight', 'servicenode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['resource', 'mangers', 'are', 'downboth', 'rm', 'goes', 'to', 'standbyhadoop']),\n",
       "       list(['rm', 'not', 'foundrm', 'not', 'foundissue', 'with', 'scaling', 'up']),\n",
       "       list(['root', 'file', 'system', 'are', 'getting', 'filled', 'with', 'credential', 'service', 'logs.root', 'file', 'system', 'are', 'getting', 'filled', 'with', 'credential', 'service', 'logs.spark']),\n",
       "       list(['s020', 'data', 'storage', 'errorcx', 'received', 's020', 'data', 'storage', 'error', 'while', 'trying', 'to', 'execute', 'a', 'hive', 'query', 'from', 'ambari', 'hive', 'viewhive']),\n",
       "       list(['s360', 'error', 'which', 'was', 'due', 'to', 'tls', 'is', 'not', '1.2', 'for', 'current', 'clusters360', 'error', 'which', 'was', 'due', 'to', 'tls', 'is', 'not', '1.2', 'for', 'current', 'clusterspark']),\n",
       "       list(['sandbox', '-', 'kps050sparkadfsbwus201', '-', 'oozie', 'ca', \"n't\", 'run', 'spark', 'submitsandbox', '-', 'kps050sparkadfsbwus201', '-', 'oozie', 'ca', \"n't\", 'run', 'spark', 'submitspark']),\n",
       "       list(['sandbox', '-', 'kps126llaphpbiosbwus201securehadoopwaitforoucontainercreationactivitytimedout', 'error', 'while', 'cluster', 'creationcreate', 'failure', '-', 'other']),\n",
       "       list(['saprk', 'clusters', ':', 'admin', 'acl', 'issueall', 'users', 'unable', 'to', 'see', 'application', 'logsspark']),\n",
       "       list(['scale', 'down', 'two', 'nodes', 'in', 'hdi', 'hbase', 'clusterscale', 'down', 'two', 'nodes', 'in', 'hdi', 'hbase', 'clusterissue', 'with', 'scaling', 'down']),\n",
       "       list(['scale', 'up', 'cluster', 'failed', 'and', 'cluster', 'is', 'not', 'in', 'running', 'state.cluster', 'stuck', 'in', 'accepting', 'state', 'issue', 'with', 'scaling', 'up']),\n",
       "       list(['scale', 'up', 'cluster', 'failed', 'and', 'cluster', 'is', 'not', 'in', 'running', 'state.scale', 'up', 'cluster', 'failed', 'and', 'cluster', 'is', 'not', 'in', 'running', 'state.issue', 'with', 'scaling', 'up']),\n",
       "       list(['scale', 'up', 'failedscaling', 'failissue', 'with', 'scaling', 'up']),\n",
       "       list(['scale', 'up', 'issues', 'on', 'different', 'cluster', 'types', '(', 'hbase', ',', 'spark', ')', 'within', 'the', 'same', 'resource', 'group', 'through', 'azure', 'portalcan', '’', 't', 'simultaneously', 'scale', 'up', 'the', 'clusters', 'in', 'a', 'same', 'resource', 'group', 'within', 'the', 'same', 'subscription', '.', 'the', 'scale', 'up', 'or', 'scale', 'down', 'activities', 'has', 'be', 'done', 'in', 'a', 'serial', 'manner.issue', 'with', 'scaling', 'up']),\n",
       "       list(['scaling', 'failingscalign', 'failign', 'when', 'scaling', 'up', '+200', 'or', '+100', 'nodes', 'issue', 'with', 'scaling', 'down']),\n",
       "       list(['scaling', 'operation', 'over', '5', 'hours120061821006718', '-', 'scaling', 'operation', 'over', '5', 'hoursissue', 'with', 'scaling', 'up']),\n",
       "       list(['scaling', 'up', 'failed120060824010500', '-', 'scaling', 'up', 'failedissue', 'with', 'scaling', 'up']),\n",
       "       list(['scaling', 'up', 'to', '500', 'nodes', 'failed', ',', 'leaving', 'us', 'with', '488.', 'then', '4', 'nodes', 'have', 'died.scaling', 'up', 'to', '500', 'nodes', 'failed', ',', 'leaving', 'us', 'with', '488.', 'then', '4', 'nodes', 'have', 'diedissue', 'with', 'scaling', 'up']),\n",
       "       list(['schedule', 'auto', 'restart', 'of', 'worker', 'nodescustomer', 'wanted', 'to', 'understand', 'if', 'we', 'have', 'role', 'restart', 'on', 'hdinsightnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['script', 'action', 'is', 'failing', 'on', 'hdi', 'cluster', '-', 'cmiprodllapdjfailed', 'to', 'validate', 'script', 'action', 'at', 'uri', 'https', ':', '//cmiprdhdiscripts.blob.core.windows.net/prd/llapprd01djenterpriseprd/llapprd01djenterpriseprd.sh', '?', 'sv=2019-02-02', '&', 'ss=bqtf', '&', 'srt=sco', '&', 'sp=rwdlacup', '&', 'se=2020-04-27t05:41:19z', '&', 'sig=bj9vg4ozl5', '%', '2bxhnfsbdrc0wfl20i5fwfnul2v', '%', '2f7nh4ci', '%', '3d', '&', '_=1587937288360', '.', 'exception', 'message', ':', 'script', 'uri', 'can', 'not', 'be', 'retrieved', 'correctly', '.', 'http', 'status', 'code', ':', 'forbidden.create', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['scriptactionunable', 'to', 'add', 'new', 'edge', 'nodelost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['security', 'concern', ':', 'zeppelin', 'notebook', 'ui', 'is', 'accessible', 'from', 'all', 'networksecurity', 'concern', ':', 'zeppelin', 'notebook', 'ui', 'is', 'accessible', 'from', 'all', 'networknotebook', 'in', 'standard', 'cluster']),\n",
       "       list(['see', 'attached', 'pdf', 'for', 'full', 'descriptiongrafana', 'dashboard', 'showing', 'different', 'metrics', 'or', 'not', 'provide', 'any', 'metrics', 'at', 'allhbase']),\n",
       "       list(['seeing', '-mv', ':', 'fatal', 'internal', 'error', 'java.lang.nullpointerexception.fatal', 'internal', 'error', 'java.lang.nullpointerexception.azure', 'storage', 'in', 'standard', 'cluster']),\n",
       "       list(['seeing', 'alerts', 'in', 'ambari', 'dashboard', '-', 'hdfs', 'and', 'mapreduceseeing', 'alerts', 'in', 'ambari', 'dashboard', '-', 'hdfs', 'and', 'mapreducehadoop']),\n",
       "       list(['seeing', 'alerts', 'on', 'cluster', 'head', 'node', 'dashaboard', 'seeing', 'alerts', 'on', 'cluster', 'head', 'node', 'dashaboardspark']),\n",
       "       list(['seeing', 'alerts', 'on', 'hdfs', 'name', 'node', 'last', 'checkinseeing', 'alerts', 'on', 'hdfs', 'name', 'node', 'last', 'checkinhadoop']),\n",
       "       list(['seeing', 'heartbeat', 'errors', 'for', 'spark', 'cluster', 'there', 'are', '14', 'stale', 'alerts', 'from', '14', 'host', '(', 's', ')', ':', 'wn106-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn107-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn108-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn109-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn110-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn111-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn112-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn114-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn115-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn116-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn117-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn118-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn120-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ',', 'wn122-prddfs.meuoib51lmqezo25oxzugd5q2g.dx.internal.cloudapp.net', '[', 'ambari', 'agent', 'heartbeat', '(', '46d', '9h', '23m', ')', ']', ';', 'spark']),\n",
       "       list(['server', 'not', 'found', 'in', 'kerberos', 'database1', '.', 'server', 'not', 'found', 'in', 'kerberos', 'database2', '.', 'jdbc', 'connectivity', 'error3', '.', 'spark-sumit', 'run', 'errorinteractive', 'query']),\n",
       "       list(['service', 'principal', 'authenticationfollowing', 'deploying', 'in', 'log', 'entry', 'we', 'can', 'see', 'the', 'internal', 'error', 'for', 'the', 'error', 'that', 'was', 'thrown', 'to', 'the', 'customer', '.', 'external', 'error', '-', '``', 'failed', 'to', 'connect', 'to', 'aad', 'to', 'validate', 'the', 'service', 'principal', '.', \"''\", 'internal', 'error', '-', '``', 'invalid', 'provider', 'type', 'specified', \"''\", 'create', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['service', 'stuck', 'registering/unregisteringcustomer', 'is', 'unable', 'to', 'unregister', 'the', 'resource', 'provider', '``', 'microsoft.hdinsight', \"''\", 'create', 'failure', '-', 'other']),\n",
       "       list(['services', 'fail', 'to', 'start', 'on', 'hn1', 'and', 'cluster', 'creation', 'failsservices', 'fail', 'to', 'start', 'on', 'hn1', 'and', 'cluster', 'creation', 'failscreate', 'failure', '-', 'other']),\n",
       "       list(['serviço', 'indisponívelcustomer', 'reported', 'that', 'their', 'hdi', 'cluster', 'was', 'unavailable.spark']),\n",
       "       list(['setfacl', ':', 'fatal', 'internal', 'error', '.', 'java.lang.unsupportedoperationexception', ':', 'secure', 'does', \"n't\", 'support', 'setaclsetfacl', ':', 'fatal', 'internal', 'error', '.', 'java.lang.unsupportedoperationexception', ':', 'secure', 'does', \"n't\", 'support', 'setaclazure', 'storage', 'in', 'standard', 'cluster']),\n",
       "       list(['shop', 'site', 'data', 'processor', 'spark', 'job', 'running', 'for', 'long', 'after', 'increase', 'in', 'the', 'executor', 'size', 'to', '4gb', ',', 'no', 'longer', 'see', 'any', 'outofdirectmemoryerror', 'exceptions', '.', 'the', 'issue', 'is', 'related', 'to', 'customer', \"'s\", 'business', 'logic', '.', 'they', 'added', 'a', 'new', 'dataset', 'to', 'their', 'script', 'and', 'left', 'joining', 'that', 'with', 'existing', 'dataset', '.', 'this', 'is', 'causing', 'the', 'performance', 'issue', 'and', 'making', 'their', 'pipeline', 'run', 'from', '2', 'hours', 'to', '9+', 'hoursspark']),\n",
       "       list(['similar', 'to', 'this', 'case', '120030224003288', '[', 'csat', 'impacting', ']', 'queries', 'was', 'taking', 'to', 'much', 'time', 'to', 'retrieve', 'data.on', 'a', 'hdi', '4.0', 'and', 'adl', 'gen2', 'as', 'a', 'primary', 'storage', 'account', ',', 'client', 'was', 'taking', 'to', 'much', 'to', 'get', 'the', 'result', 'from', 'simple', 'querieshive']),\n",
       "       list(['simple', 'query', 'not', 'workingsimple', 'query', 'not', 'workinghive']),\n",
       "       list(['since', '9am', 'est', ',', 'a', 'keberos', 'error', ',', 'and', 'not', 'accessible', 'to', 'the', 'jobs', 'anymore', '.', '2020-06-19', '15:26:35.528', '|', 'main', '|', 'warn', '|', 'org.apache.http.impl.auth.httpauthenticator:203', '|', 'negotiate', 'authentication', 'error', ':', 'invalid', 'name', 'provided', '(', 'mechanism', 'level', ':', 'krbexception', ':', 'can', 'not', 'locate', 'default', 'realm', ')', ';', 'ambari', 'in', 'standard', 'cluster']),\n",
       "       list(['slow', 'performance', 'slow', 'performance.spark']),\n",
       "       list(['slowness', 'in', 'processing', 'jobs', 'in', 'hdinsights', 'cluster', 'to', 'storageslowness', 'in', 'processing', 'jobs', 'in', 'hdinsights', 'cluster', 'to', 'storagelow', 'throughput']),\n",
       "       list(['sluggish', 'response', 'from', 'application', 'running', 'on', 'edge', 'node.sluggish', 'response', 'from', 'application', 'running', 'on', 'edge', 'node.node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['smbd', 'services', 'confusing', 'the', 'qualys', 'scan', '.', 'do', 'you', 'see', 'any', 'potential', 'issue', 'with', 'hdinisght', 'cluster', 'or', 'services', 'if', 'we', 'stop', 'the', 'service', '?', 'smbd', 'services', 'confusing', 'the', 'qualys', 'scan', '.', 'do', 'you', 'see', 'any', 'potential', 'issue', 'with', 'hdinisght', 'cluster', 'or', 'services', 'if', 'we', 'stop', 'the', 'service', '?', 'spark']),\n",
       "       list(['some', 'pipelines', 'are', 'failing', 'with', 'no', 'applicatoin', 'id', 'assignedout', 'of', 'memory', 'exceptions', 'in', 'the', 'livy', 'logs', 'and', 'application', \"'s\", 'not', 'being', 'created', 'from', 'the', 'service', 'logs', 'due', 'to', 'timeout.hdinsight', '(', 'hive', ',', 'mapreduce', ',', 'pig', ',', 'spark', ',', 'streaming', ')']),\n",
       "       list(['some', 'queries', 'are', 'not', 'working', 'via', 'zeppelinunable', 'to', 'run', 'few', 'queries', 'through', 'zeppelin/sqllinehbase']),\n",
       "       list(['spark', 'applications', 'failing', 'in', 'cluster', 'modespark', 'applications', 'failing', 'in', 'cluster', 'modespark']),\n",
       "       list(['spark', 'cluster', 'creation', 'failingspark', 'cluster', 'creation', 'failingcreate', 'failure', '-', 'other']),\n",
       "       list(['spark', 'cluster', 'is', 'down', ',', 'no', 'jobs', 'runningcx', 'worked', 'with', 'another', 'engineer', 'on', 'a', 'similar', 'issue', 'and', 'this', 'was', 'the', 'resolution', 'for', 'that', 'issue.spark']),\n",
       "       list(['spark', 'cluster', 'is', 'intermittently', 'not', 'accessible', 'with', 'unauthorized', 'accedd', 'spark', 'cluster', 'is', 'intermittently', 'not', 'accessible', 'with', 'unauthorized', 'accessambari', 'in', 'standard', 'cluster']),\n",
       "       list(['spark', 'history', 'server', 'takes', 'too', 'much', 'of', 'cpu', 'consumtpioncustomer', 'wanted', 'the', 'root', 'cause', 'and', 'to', 'know', 'what', 'steps', 'to', 'take', 'before', 'killing', 'the', 'spark', 'history', 'serverspark']),\n",
       "       list(['spark', 'hive', 'warehouse', 'connector', 'error', 'nullpointerexception', 'when', 'running', 'a', 'hive', 'query', 'via', 'the', 'hwc', 'executequery', '(', ')', 'methodnotebooks']),\n",
       "       list(['spark', 'in', 'yarn', 'mode', 'wo', \"n't\", 'accept', 'jar', 'local', 'arguments', 'for', 'files', 'on', 'adlsspark', 'in', 'yarn', 'mode', 'wo', \"n't\", 'accept', 'jar', 'local', 'arguments', 'for', 'files', 'on', 'adlsspark']),\n",
       "       list(['spark', 'job', 'failed', 'after', 'successful', 'executionspark', 'job', 'failed', 'after', 'successful', 'executionspark']),\n",
       "       list(['spark', 'job', 'failingspark', 'job', 'failingspark']),\n",
       "       list(['spark', 'jobs', 'are', 'failing', 'job', 'not', 'getting', 'submitted', 'to', 'hdinsight', 'from', 'adfspark']),\n",
       "       list(['spark', 'on', 'hive', 'config', 'is', 'not', 'enforcing', 'ranger', 'permissions', 'while', 'trying', 'to', 'access', 'a', 'hive', 'table', 'from', 'spark', 'shellunable', 'to', 'honor', 'ranger', 'policies', 'when', 'using', '--', 'conf', 'spark.hadoop.metastore.catalog.default=hiveranger', 'policy', 'enforcement']),\n",
       "       list(['spark', 'pipelines', 'are', 'failingcustomer', 'was', 'experencing', 'multiple', 'spark', 'jobs', 'triggered', 'from', 'adf', 'not', 'working', '(', 'timed', 'out', ')', 'over', 'night.from', 'adf', ',', 'and', 'together', 'with', 'icm', 'team', ',', 'we', 'checked', 'the', 'trace', 'logs', 'for', 'some', 'of', 'the', 'timed', 'out', 'run', 'ids', ',', 'but', 'the', 'logs', 'were', 'inconclusive', ',', 'as', 'it', 'seems', 'it', 'was', 'expected', 'for', 'the', 'jobs', 'to', 'be', 'timed', 'out', 'after', '1h.spark']),\n",
       "       list(['spark', 'process', 'consume', 'lots', 'of', 'cpuobservation', 'in', 'logs', 'from', 'another', 'engineerspark']),\n",
       "       list(['spark', 'sqlcontext', 'not', 'able', 'to', 'access', 'hive', 'post', 'upgrade', 'from', 'spark', '2.2', 'to', 'spark', '2.3spark', 'sqlcontext', 'not', 'able', 'to', 'access', 'hive', 'post', 'upgrade', 'from', 'spark', '2.2', 'to', 'spark', '2.3.spark']),\n",
       "       list(['spark', 'thrift', 'server', 'connection', 'issueconnection', 'between', 'power', 'bi', 'and', 'hdinsight', 'spark', 'cluster', 'is', 'failing', ',', 'powerbi', 'is', 'attempting', 'to', 'connect', 'to', 'hn1', 'instead', 'of', 'hn0', '(', 'which', 'is', 'the', 'default', ')', '.', 'throwing', 'the', 'error', 'obdc', ':', 'error', '[', 'hy000', ']', '[', 'microsoft', ']', '[', 'driversupport', ']', '(', '1170', ')', 'issue', '2', ':', 'hiveview', 'failure', 'cx', 'wanted', 'to', 'be', 'keep', 'opening', 'icm', 'for', 'further', 'principal', 'root', 'causeodbc', 'or', 'jdbc']),\n",
       "       list(['spark', 'thrift', 'server', 'is', 'not', 'starting', '.', 'it', 'is', 'trying', 'to', 'connect', 'to', 'hive', 'for', 'very', 'long', 'time', 'and', 'getting', 'failedwhen', 'trying', 'to', 'restart', 'spark', 'from', 'ambari', 'it', 'kept', 'saying', 'it', 'was', 'not', 'able', 'to', 'connect', 'to', 'jdbc', 'urlspark']),\n",
       "       list(['spark', 'unable', 'to', 'read/write', 'to', 'hive', 'transactional', 'tables120042221002138', '-', 'spark', 'unable', 'to', 'read/write', 'to', 'hive', 'transactional', 'tables', 'hdinsight', 'servicespark']),\n",
       "       list(['spark-shell', 'is', 'not', 'working', 'on', 'integration', 'servers', 'fro', 'qa', 'and', 'perf', 'envsspark-shell', 'is', 'not', 'working', 'on', 'integration', 'servers', 'fro', 'qa', 'and', 'perf', 'envsspark']),\n",
       "       list(['spark-sql', 'errorunable', 'to', 'launch', 'the', 'saprk-sql', 'sessionspark']),\n",
       "       list(['spark2', 'history', 'server', 'ui', '-', 'out', 'of', 'memorythis', 'issue', 'is', 'often', 'caused', 'by', 'a', 'lack', 'of', 'resources', 'when', 'opening', 'large', 'spark-event', 'files', '.', 'the', 'spark', 'heap', 'size', 'is', 'set', 'to', '1', 'gb', 'by', 'default', ',', 'but', 'large', 'spark', 'event', 'files', 'may', 'require', 'more', 'than', 'this.spark']),\n",
       "       list(['spark2', 'is', 'not', 'startingall', 'job', 'failures', 'that', 'was', 'submitted', 'to', 'the', 'clusterspark']),\n",
       "       list(['spark2', 'thrift', 'server', 'connection', 'lost', 'to', 'headnode', '0spark2', 'thrift', 'server', 'connection', 'lost', 'to', 'headnode', '0spark']),\n",
       "       list(['spark2', 'thrift', 'servers', 'went', 'down', 'and', 'are', 'not', 'starting', 'upspark2', 'thrift', 'servers', 'went', 'down', 'and', 'are', 'not', 'starting', 'upspark']),\n",
       "       list(['sql', 'connectivity', 'issues', 'to', 'metastoreintermittently', 'sql', 'db', 'connectivity', 'issues', 'to', 'metastoreinteractive', 'query']),\n",
       "       list(['sqoop', 'as', 'parquet', 'not', 'working', 'on', 'hdinsightimport', 'command', 'failing', 'when', 'the', 'argument', '``', '--', 'as-parquetfiles', \"''\", 'is', 'on', ',', 'getting', 'the', 'followed', 'error', ':', 'org.kitesdk.data.datasetnotfoundexception', ':', 'unknown', 'dataset', 'uri', 'pattern', ':', 'dataset', ':', 'adl', ':', '//deventanalyticsdatalake.azuredatalakestore.net/raw/codebreaker/error_code_lookupcheck', 'that', 'jars', 'for', 'adl', 'datasets', 'are', 'on', 'the', 'classpathmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['sqoop', 'directory', 'version', 'changecustomer', 'deploying', 'new', 'cluster', 'each', 'day', 'and', 'by', 'design', ',', 'new', 'hdi', 'clusters', 'will', 'pull', 'the', 'latest', 'hdi', 'image', 'for', 'that', 'version', '.', 'so', 'while', 'the', 'major', 'version', 'is', '3.6', 'for', 'this', 'customer', ',', 'the', 'minor', 'version', 'may', 'change', 'with', 'each', 'new', 'deployment', 'as', 'new', 'bug', 'fixes', 'and', 'other', 'improvements', 'are', 'pulled', 'into', 'the', 'new', 'image.create', 'failure', '-', 'other']),\n",
       "       list(['sqoop', 'job', 'failuresqoop', 'job', 'failuremapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['subscription', 'got', 'disabled', 'due', 'to', 'non', 'usagesubscription', 'got', 'disabled', 'due', 'to', 'non', 'usageazure', 'global', '(', 'portal.azure.com', ')']),\n",
       "       list(['system', 'corruptedserver', 'is', 'not', 'booting', ',', 'basically', 'all', 'the', 'services', 'were', 'failing', 'to', 'start', 'and', 'at', 'some', 'point', 'was', 'crashing', '.', '(', 'still', 'need', 'to', 'take', 'a', 'screenshot', 'for', 'this', '.', ')', 'unable', 'to', 'ssh']),\n",
       "       list(['team', 'needs', 'scala', 'runtime', 'environment', 'for', 'kafka', 'cluster', 'to', 'run', 'scala', 'jar.advisoryspark']),\n",
       "       list(['technical', 'doubt', 'about', 'the', 'virtual', 'machines', 'timequestions', 'about', 'the', 'virtual', 'machine', 'time', '.', 'node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['technical', 'doubtadvisoryissue', 'with', 'scaling', 'down']),\n",
       "       list(['test', 'testtesthadoop']),\n",
       "       list(['test', 'ticket', ',', 'please', 'closetest', 'ticketspark']),\n",
       "       list(['testtest', 'ticketcreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['testtest', 'ticketkafka']),\n",
       "       list(['the', 'auto', 'scale', 'is', 'still', 'in', 'progress', 'since', 'fridaycluster', 'stuck', 'in', 'error', 'stateissue', 'with', 'autoscaling']),\n",
       "       list(['the', 'autoscaling', 'didnt', 'finish', 'and', 'it', 'is', 'stuck', 'in', 'therethe', 'autoscaling', 'didnt', 'finish', 'and', 'it', 'is', 'stuck', 'in', 'thereissue', 'with', 'autoscaling']),\n",
       "       list(['the', 'credentials', 'provided', 'for', 'the', 'microsoftazureconsumptioninsights', 'source', 'are', 'invalid', '.', '(', 'source', 'at', '7306177', '.', ')', 'credentials', 'provided', 'for', 'the', 'microsoftazureconsumptioninsights', 'source', 'are', 'invaliddownload', 'consumption', 'data', 'by', 'using', 'billing', 'api']),\n",
       "       list(['the', 'name', 'node', 'process', 'is', 'frequently', 'getting', 'restarted', '&', 'gracefully', 'shutting', 'down', 'on', 'insights', 'production', 'processing', 'clusternamenode', 'restarting', 'frequently', '/', 'services', 'unhealthynode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['the', 'number', 'of', 'nodes', 'used', 'by', 'hive', \"'s\", 'llap', 'config', 'issuethe', 'number', 'of', 'nodes', 'used', 'by', 'hive', \"'s\", 'llap', 'config', 'issueinteractive', 'query']),\n",
       "       list(['the', 'previous', 'problem', 'has', 'reoccured', ',', 'previous', 'support', 'id', ':', '120070723000968followed', 'error', ':', 'uncaught', 'error', 'in', 'kafka', 'producer', 'i/o', 'thread', ':', 'java.util.concurrentmodificationexception', ':', 'nullkafka']),\n",
       "       list(['the', 'query', 'is', 'being', 'automatically', 'killed', 'with', 'an', 'exception', 'in', 'prod', 'dm04', 'clusterthe', 'query', 'is', 'being', 'automatically', 'killed', 'with', 'an', 'exception', 'in', 'prod', 'dm04', 'clusterhive']),\n",
       "       list(['there', 'is', 'a', 'service', '360', 'action', 'item', 'that', 'needs', 'the', 'cluster', \"'s\", 'gateway', 'vms', 'to', 'explicitly', 'disable', 'sslv2', 'and', 'sslv3there', 'is', 'a', 'service', '360', 'action', 'item', 'that', 'needs', 'the', 'cluster', \"'s\", 'gateway', 'vms', 'to', 'explicitly', 'disable', 'sslv2', 'and', 'sslv3', 'spark']),\n",
       "       list(['timeline', 'servcies', 'issues', 'in', 'the', 'hdi', 'clusterstimeline', 'service', 'version', '2.0', 'issuesranger', 'policy', 'auditing']),\n",
       "       list(['timeoutexception', '-', 'while', 'running', 'hive', 'query', 'in', 'prod', 'dm05query', 'runs', 'into', 'a', 'vertex', 'error.hive']),\n",
       "       list(['tls', '1.2', 'enforcementquestions', 'regarding', 'tls', '1.2', ':', '-', 'adf', 'to', 'hdinsight', ':', 'do', 'apache', 'livy', 'api', 'calls', 'to', 'https', ':', '//sigi03spark.azurehdinsight.net', 'using', 'port', '443', 'use', 'tls', '1.2', 'by', 'default', '?', '-', 'hdinsight', 'pyspark', 'to', 'gen1', 'adls', ':', 'does', 'the', 'adl', ':', '//', 'address', 'use', 'tls', '1.2', 'by', 'default', '?', '-', 'hdinsight', 'pyspark', 'to', 'azure', 'sql', 'via', 'jdbc', ':', 'is', 'it', 'tls', '1.2', 'by', 'default', '?', '-', 'azure', 'storage', 'explorer', 'client', 'to', 'gen1', 'adls', ':', 'which', 'version', '(', 's', ')', 'of', 'the', 'storage', 'explorer', 'client', 'uses', 'tls', '1.2', 'by', 'default', '?', '-', 'jdbc/odbc', ':', 'do', 'our', 'on-premise', 'application', 'calls', 'to', 'hdinsight', 'hive', 'use', 'tls', '1.2', 'by', 'default', '?', 'we', 'are', 'using', 'the', \"'cloudera\", 'odbc', 'driver', 'for', 'apache', 'hive', \"'\", 'driver', ',', 'version', '2.6.4.1004', 'on', 'a', 'windows', 'server', '2016', 'vm.odbc', 'or', 'jdbc']),\n",
       "       list(['token', 'doesnt', 'exists', 'in', 'token', 'managercustomer', 'mention', ':', 'znode', 'that', 'stores', 'the', 'credential', 'cache', 'has', 'found', 'to', 'be', 'missing.error', 'org.apache.hadoop.security.token.secretmanager', '$', 'invalidtoken', ':', 'token', '(', 'adls', 'delegation', 'token', '632137', 'for', 'hive', ')', 'ca', \"n't\", 'be', 'found', 'in', 'cacheat', 'org.apache.hadoop.security.token.delegation.abstractdelegationtokensecretmanager.checktoken', '(', 'abstractdelegationtokensecretmanager.java:410', ')', 'at', 'org.apache.hadoop.security.token.delegation.abstractdelegationtokensecretmanager.retrievepassword', '(', 'abstractdelegationtokensecretmanager.java:422', ')', 'at', 'org.apache.hadoop.security.token.delegation.abstractdelegationtokensecretmanager.verifytoken', '(', 'abstractdelegationtokensecretmanager.java:448', ')', 'at', 'com.microsoft.azure.datalake.store.security.common.authenticationhelper.getugifromtoken', '(', 'unknown', 'source', ')', 'at', 'com.microsoft.azure.datalake.store.security.common.authenticationhelper.getugi', '(', 'unknown', 'source', ')', 'at', 'com.microsoft.azure.datalake.store.security.resources.userprovider.getvalue', '(', 'unknown', 'source', ')', 'at', 'com.microsoft.azure.datalake.store.security.resources.userprovider.getvalue', '(', 'unknown', 'source', ')', 'at', 'com.sun.jersey.server.impl.inject.injectablevaluesprovider.getinjectablevalues', '(', 'injectablevaluesprovider.java:46', ')', 'at', 'com.sun.jersey.server.impl.model.method.dispatch.abstractresourcemethoddispatchprovider', '$', 'entityparamininvoker.getparams', '(', 'abstractresourcemethoddispatchprovider.java:153', ')', 'at', 'com.sun.jersey.server.impl.model.method.dispatch.abstractresourcemethoddispatchprovider', '$', 'responseoutinvoker._dispatch', '(', 'abstractresourcemethoddispatchprovider.java:203', ')', 'at', 'com.sun.jersey.server.impl.model.method.dispatch.resourcejavamethoddispatcher.dispatch', '(', 'resourcejavamethoddispatcher.java:75', ')', 'at', 'com.sun.jersey.server.impl.uri.rules.httpmethodrule.accept', '(', 'httpmethodrule.java:288', ')', 'at', 'com.sun.jersey.server.impl.uri.rules.resourceclassrule.accept', '(', 'resourceclassrule.java:108', ')', 'at', 'com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept', '(', 'righthandpathrule.java:147', ')', 'at', 'com.sun.jersey.server.impl.uri.rules.rootresourceclassesrule.accept', '(', 'rootresourceclassesrule.java:84', ')', 'at', 'com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest', '(', 'webapplicationimpl.java:1469', ')', 'at', 'com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest', '(', 'webapplicationimpl.java:1400', ')', 'at', 'com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest', '(', 'webapplicationimpl.java:1349', ')', 'at', 'com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest', '(', 'webapplicationimpl.java:1339', ')', 'at', 'com.sun.jersey.spi.container.servlet.webcomponent.service', '(', 'webcomponent.java:416', ')', 'at', 'com.sun.jersey.spi.container.servlet.servletcontainer.service', '(', 'servletcontainer.java:537', ')', 'at', 'com.sun.jersey.spi.container.servlet.servletcontainer.service', '(', 'servletcontainer.java:699', ')', 'at', 'javax.servlet.http.httpservlet.service', '(', 'httpservlet.java:820', ')', 'at', 'org.mortbay.jetty.servlet.servletholder.handle', '(', 'servletholder.java:511', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler', '$', 'cachedchain.dofilter', '(', 'servlethandler.java:1221', ')', 'at', 'com.microsoft.azure.datalake.store.security.filters.authfilter.dofilter', '(', 'unknown', 'source', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler', '$', 'cachedchain.dofilter', '(', 'servlethandler.java:1212', ')', 'at', 'org.apache.hadoop.http.httpserver2', '$', 'quotinginputfilter.dofilter', '(', 'httpserver2.java:1426', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler', '$', 'cachedchain.dofilter', '(', 'servlethandler.java:1212', ')', 'at', 'org.apache.hadoop.http.nocachefilter.dofilter', '(', 'nocachefilter.java:45', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler', '$', 'cachedchain.dofilter', '(', 'servlethandler.java:1212', ')', 'at', 'org.apache.hadoop.http.nocachefilter.dofilter', '(', 'nocachefilter.java:45', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler', '$', 'cachedchain.dofilter', '(', 'servlethandler.java:1212', ')', 'at', 'org.mortbay.jetty.servlet.servlethandler.handle', '(', 'servlethandler.java:399', ')', 'at', 'org.mortbay.jetty.security.securityhandler.handle', '(', 'securityhandler.java:216', ')', 'at', 'org.mortbay.jetty.servlet.sessionhandler.handle', '(', 'sessionhandler.java:182', ')', 'at', 'org.mortbay.jetty.handler.contexthandler.handle', '(', 'contexthandler.java:766', ')', 'at', 'org.mortbay.jetty.webapp.webappcontext.handle', '(', 'webappcontext.java:450', ')', 'at', 'org.mortbay.jetty.handler.contexthandlercollection.handle', '(', 'contexthandlercollection.java:230', ')', 'at', 'org.mortbay.jetty.handler.handlerwrapper.handle', '(', 'handlerwrapper.java:152', ')', 'at', 'org.mortbay.jetty.server.handle', '(', 'server.java:326', ')', 'at', 'org.mortbay.jetty.httpconnection.handlerequest', '(', 'httpconnection.java:542', ')', 'at', 'org.mortbay.jetty.httpconnection', '$', 'requesthandler.headercomplete', '(', 'httpconnection.java:928', ')', 'at', 'org.mortbay.jetty.httpparser.parsenext', '(', 'httpparser.java:549', ')', 'at', 'org.mortbay.jetty.httpparser.parseavailable', '(', 'httpparser.java:212', ')', 'at', 'org.mortbay.jetty.httpconnection.handle', '(', 'httpconnection.java:404', ')', 'at', 'org.mortbay.io.nio.selectchannelendpoint.run', '(', 'selectchannelendpoint.java:410', ')', 'at', 'org.mortbay.thread.queuedthreadpool', '$', 'poolthread.run', '(', 'queuedthreadpool.java:582', ')', 'adls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['topology_script.py', 'is', 'built', 'on', 'python2', 'and', 'python2', 'reached', 'eol', 'on', 'jan', '1', ',', '2020topology_script.py', 'is', 'built', 'on', 'python2', 'and', 'python2', 'reached', 'eol', 'on', 'jan', '1', ',', '2020cx', 'runs', 'application', 'on', 'edge', 'node', 'which', 'are', 'built', 'on', 'python3', 'and', 'the', 'application', 'fails', 'with', 'errorsmapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['transaction', 'aborted', 'error', '-', 'even', 'after', 'cleanup', '-', 'lockexceptiontransaction', 'aborted', 'error', '-', 'even', 'after', 'cleanup', '-', 'lockexceptionhbase']),\n",
       "       list(['transient', 'errors', 'in', 'deployment', '.', 'script', 'actions', 'not', 'getting', 'submitted', '.', 'operation', 'state', 'is', 'conflict.transient', 'errors', 'in', 'deployment', '.', 'script', 'actions', 'not', 'getting', 'submitted', '.', 'operation', 'state', 'is', 'conflict.hadoop']),\n",
       "       list(['trouble', 'to', 'deploy', 'presto', 'app', 'in', 'hdinsight', 'both', 'with', 'terraforms', 'and', 'from', 'azure', 'portal', '.', 'ca', \"n't\", 'deploy', 'presto', ';', 'attached', 'an', 'image', 'with', 'more', 'detailscreate', 'failure', '-', 'other']),\n",
       "       list(['trying', 'to', 'create', 'a', 'new', 'cluster', 'with', 'adls', 'gen', '1', 'as', 'secondary', 'storage', 'unable', 'to', 'create', 'a', '4.0', 'hdi', 'cluster', 'in', 'the', 'portal', 'using', 'blob', 'storage', 'as', 'primary', 'storage', 'and', 'adls', 'gen', '1', 'as', 'secondary', 'storagecreate', 'failure', '-', 'other']),\n",
       "       list(['ubable', 'to', 'login', 'to', 'ambariubable', 'to', 'login', 'to', 'ambariambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['ubuntu', '18.0.4', 'for', 'hdinsight', 'nodesubuntu', '18.0.4', 'for', 'hdinsight', 'nodescreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'execute', 'script', 'actionthe', 'script', 'was', 'failing', 'with', 'object', 'not', 'found', 'message', 'due', 'to', 'az.hdinsight', 'module', 'being', 'missing', 'from', 'automation', 'workspacecreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'start', 'metric', 'collector', 'and', 'lost', '1wn', '(', 'wn71-hdi01p', ')', 'ambari', 'metrics', 'unable', 'to', 'starthadoop']),\n",
       "       list(['unable', 'ti', 'publish', 'messages', 'from', 'kafka', 'to', 'spark', 'unable', 'ti', 'publish', 'messages', 'from', 'kafka', 'to', 'sparkkafka']),\n",
       "       list(['unable', 'to', 'access', 'beeline', 'unable', 'to', 'access', 'beelineinteractive', 'query']),\n",
       "       list(['unable', 'to', 'add', 'an', 'edge', 'nodesymptom', ':', 'when', 'trying', 'to', 'add', 'an', 'edgenode', 'failure', 'to', 'point', 'to', 'artifact', 'locationkafka']),\n",
       "       list(['unable', 'to', 'add', 'l8s', 'v2', 'type', 'machines', 'as', 'edge', 'node', 'in', 'hdinsight', 'clusterbackground', ':', 'the', 'support', 'request', 'was', 'created', 'by', 'cvs', 'on', 'may', '28th', 'as', 'they', 'attempted', 'to', 'use', 'an', 'l8', 'vm', 'as', 'an', 'edge', 'node', 'for', 'hdinsight', ',', 'and', 'were', 'unable', 'to', 'because', 'l8', 'vms', 'are', 'not', 'a', 'supported', 'edge', 'node', 'type', 'for', 'hdinsight', '.', 'the', 'main', 'purpose', 'of', 'the', 'edge', 'node', 'is', 'to', 'run', 'redpoint.investigation', 'completed', ':', 'after', 'investigating', 'cvs', '’', 's', 'requirements', 'with', 'redpoint', ',', 'it', 'was', 'determined', 'that', 'a', 'standard', 'ds14', 'v2', 'vm', ',', 'which', 'is', 'a', 'supported', 'edge', 'node', 'sku', ',', 'does', 'meet', 'their', 'ram', 'and', 'core', 'needs', 'but', 'did', 'not', 'meet', 'their', '2', 'tb', 'storage', 'requirement.feature', 'requests', 'made', 'to', 'the', 'hdinsight', 'product', 'group', ':', 'a', 'feature', 'request', 'was', 'made', 'to', 'the', 'product', 'group', 'to', 'enable', 'cvs', 'to', 'mount', 'additional', 'ssds', 'to', 'hdi', 'edge', 'nodes', 'to', 'meet', 'the', '2', 'tb', 'storage', 'requirement', '.', 'this', 'feature', 'request', 'was', 'discussed', 'by', 'the', 'product', 'group', ',', 'and', 'because', 'edge', 'nodes', 'are', 'provisioned', 'to', 'and', 'managed', 'by', 'an', 'hdinsight', 'internal', 'subscription', ',', 'there', 'is', 'no', 'ability', 'for', 'the', 'customers', 'to', 'mount', 'their', 'own', 'ssds', 'to', 'these', 'hdinsight', 'managed', 'edge', 'nodes', '–', 'simply', 'put', ',', 'the', 'plumbing', 'does', 'not', 'exist', 'to', 'support', 'this', 'request', '.', 'the', 'engineering', 'requirements', 'to', 'enable', 'this', 'functionality', 'are', 'not', 'trivial', 'and', 'would', 'require', 'extensive', 'investment', 'from', 'the', 'product', 'group.alternatively', ',', 'during', 'recent', 'conversations', ',', 'it', 'was', 'suggested', 'that', 'an', 'l16', 'vm', 'would', 'meet', 'the', 'ram', ',', 'core', ',', 'and', 'storage', 'needs', 'for', 'cvs', '’', 's', 'use', 'of', 'redpoint', ',', 'however', 'l16', 'is', 'also', 'not', 'a', 'supported', 'edge', 'node', 'type', 'for', 'hdinsight.neither', 'of', 'these', 'feature', 'requests', 'are', 'on', 'the', 'roadmap', 'at', 'this', 'time', ',', 'and', 'the', 'earliest', 'time', 'we', 'could', 'start', 'investigating', 'implementing', 'either', 'ask', 'would', 'be', 'cy2021.in', 'an', 'effort', 'to', 'provide', 'guidance', 'to', 'cvs', 'about', 'how', 'they', 'could', 'achieve', 'their', 'requirements', 'in', 'the', 'near', 'future', ',', 'members', 'of', 'the', 'hdinsight', 'product', 'group', ',', 'account', 'team', ',', 'and', 'support', 'team', 'held', 'a', 'series', 'of', 'meetings', 'with', 'redpoint', 'to', 'learn', 'about', 'its', 'application', '’', 's', 'requirements', ',', 'as', 'cvs', 'is', 'specifically', 'using', 'the', 'edge', 'node', 'to', 'run', 'redpoint.recommended', 'path', ':', 'through', 'the', 'collaboration', 'with', 'redpoint', ',', 'we', '’', 've', 'determined', 'that', 'the', 'best', 'path', 'forward', 'is', 'for', 'cvs', 'to', 'deploy', 'a', 'storage', 'optimized', 'virtual', 'machine', 'that', 'meets', 'the', 'capacity', 'needs', '(', 'redpoint', 'indicated', 'an', 'l16s', 'or', 'higher', 'vm', 'sku', 'would', 'work', 'best', ')', ',', 'add', 'the', 'vm', 'to', 'the', 'vnet', 'that', 'the', 'hdinsight', 'cluster', 'is', 'in', ',', 'then', 'use', 'redpoints', 'utility', 'to', 'grab', 'the', 'necessary', 'files', 'as', 'a', 'configuration', 'from', 'one', 'worker', 'node', 'and', 'download', 'them', 'to', 'the', 'new', 'vm.redpoint', '’', 's', 'data', 'manager', 'would', 'then', 'be', 'pointed', 'to', 'the', 'download', 'directory', 'as', 'part', 'of', 'the', 'hadoop', 'connection', 'configuration', '.', 'in', 'short', ',', 'redpoint', 'will', 'install', 'and', 'configure', 'the', 'hadoop', 'dependencies', 'through', 'data', 'manager', 'from', 'an', 'hdinsight', 'worker', 'node.this', 'is', 'the', 'closest', 'way', 'to', 'mimic', 'an', 'hdinsight', 'edge', 'node', ',', 'while', 'running', 'redpoint', ',', 'and', 'achieving', 'maximum', 'flexibility', 'with', 'the', 'vm', 'shape.this', 'new', 'vm', 'would', 'not', 'be', 'part', 'of', 'the', 'hdinsight', 'cluster', ',', 'nor', 'would', 'it', 'be', 'supported', 'by', 'the', 'hdinsight', 'support', 'staff', 'as', 'it', 'is', 'not', 'an', 'hdinsight', 'managed', 'resource', ',', 'we', 'are', 'simply', 'providing', 'guidance', 'for', 'an', 'alternative', 'solution.create', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['unable', 'to', 'add', 'new', 'edge', 'nodeunable', 'to', 'add', 'new', 'edge', 'nodeambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['unable', 'to', 'add', 'script', 'action', 'to', 'cluster', 'via', 'powershellwe', 'are', 'seeing', 'an', 'increased', 'occurrence', 'of', 'the', 'internal', 'hive', 'metastore', 'going', 'down', 'in', 'our', 'main', 'cluster', '.', 'this', 'week', 'alone', 'it', 'happened', '6', 'times.when', 'it', 'happens', ',', 'the', 'cluster', 'is', 'unusable', 'and', 'all', 'jobs', 'do', \"n't\", 'run', '.', 'so', 'far', 'only', 'a', 'cluster', 'restart', 'fixes', 'the', 'problem.mapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['unable', 'to', 'bring', 'up', 'head', 'node', 'and', 'edge', 'node.unable', 'to', 'bring', 'up', 'headnode', 'and', 'edgenodeunable', 'to', 'ssh']),\n",
       "       list(['unable', 'to', 'change', 'cluster', 'sizeunable', 'to', 'change', 'cluster', 'sizeissue', 'with', 'scaling', 'up']),\n",
       "       list(['unable', 'to', 'change', 'group', 'ownership', 'of', 'files', 'in', 'gen2', 'storage', 'account.unable', 'to', 'change', 'group', 'ownership', 'of', 'files', 'in', 'gen2', 'storage', 'account.adls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['unable', 'to', 'change', 'the', 'ownership', 'of', 'files', 'in', 'adls', 'storage', 'using', 'hdfs', 'commands.unable', 'to', 'change', 'the', 'ownership', 'of', 'files', 'in', 'adls', 'storage', 'using', 'hdfs', 'commands.hive']),\n",
       "       list(['unable', 'to', 'conect', 'to', 'ambari', '.', '120041622002775', '-', 'unable', 'to', 'connect', 'to', 'ambari.ambari', 'in', 'standard', 'cluster']),\n",
       "       list(['unable', 'to', 'connect', 'from', 'hdinsightca', \"n't\", 'authenticate', 'ambari', 'database', 'while', 'provisioning', 'hdiazure', 'portal']),\n",
       "       list(['unable', 'to', 'connect', 'from', 'on', 'premise', 'to', 'azure', 'hive', 'in', 'provided', 'resource120051423002522', '-', 'unable', 'to', 'connect', 'to', 'beeline', 'from', 'on', 'premise', 'machinehive']),\n",
       "       list(['unable', 'to', 'connect', 'spark-shell', 'to', 'hive', 'connetion', 'to', 'spark', 'shell', 'issuesspark']),\n",
       "       list(['unable', 'to', 'connect', 'to', 'beeline', 'unable', 'to', 'connect', 'to', 'clusterbeeline']),\n",
       "       list(['unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '.', 'please', 'retry', 'laterunable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', '.', 'please', 'retry', 'latercreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['unable', 'to', 'connect', 'to', 'hive', 'service', '.', 'service', 'is', 'downunable', 'to', 'connect', 'to', 'hs2i', 'running', 'on', 'hn1', '.', 'interactive', 'query']),\n",
       "       list(['unable', 'to', 'connect', 'to', 'mysql', 'external', 'metastoredeployment', 'fails', '.', 'create', 'failure', 'with', 'other', 'customization']),\n",
       "       list(['unable', 'to', 'connect', 'to', 'worker', 'nodeunable', 'to', 'connect', 'to', 'worker', 'nodehadoop']),\n",
       "       list(['unable', 'to', 'connect', 'via', 'powerbica', \"n't\", 'connect', 'power', 'bi', 'to', 'hdinsight', 'spark', 'cluster', ',', 'no', 'metrics', 'over', 'ambari', 'ui', ',', 'hn0', 'was', 'complete', 'down.odbc', 'or', 'jdbc', 'connecting', 'to', 'standard', 'cluster']),\n",
       "       list(['unable', 'to', 'create', 'clusterunable', 'to', 'create', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'create', 'clusterunable', 'to', 'create', 'clustercreate', 'failure', 'within', 'a', 'vnet']),\n",
       "       list(['unable', 'to', 'create', 'clusterunable', 'to', 'create', 'on', 'demand', 'hdi', 'clusterambari', 'in', 'standard', 'cluster']),\n",
       "       list(['unable', 'to', 'create', 'external', 'table', 'in', 'hiveunable', 'to', 'create', 'external', 'tablebeeline']),\n",
       "       list(['unable', 'to', 'create', 'hdi', 'cluster', 'with', 'azure', 'datalake', 'gen2unable', 'to', 'create', 'hdi', 'cluster', 'with', 'azure', 'datalake', 'gen2create', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['unable', 'to', 'create', 'hdi', 'clusterunable', 'to', 'create', 'hdi', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'create', 'hdinsight', 'hadoop', 'clusterunable', 'to', 'access', 'the', 'ambari', 'uicreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'create', 'hive', 'functions', 'and', 'assign', 'udfsitleunable', 'to', 'create', 'hive', 'functions', 'and', 'assign', 'udfshive']),\n",
       "       list(['unable', 'to', 'create', 'r', 'studio', 'edge', 'serverwhen', 'creating', 'a', 'new', 'hdi', 'ml', 'r', 'server', 'internal', 'server', 'error', 'was', 'caused.create', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'create', 'spark', 'cluster', 'with', 'adls', 'gen2', 'storage', 'account', 'unable', 'to', 'create', 'spark', 'cluster', 'with', 'adls', 'gen2', 'storage', 'accountcreate', 'failure', 'with', 'azure', 'data', 'lake', 'storage', 'gen2']),\n",
       "       list(['unable', 'to', 'created', 'a', 'hdi', 'cluster', 'with', 'name', ':', 'anahdist01hdoop36redsdv01unable', 'to', 'create', 'a', 'cluster', 'with', 'same', 'name', 'as', 'deleted', 'one.create', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'deploy', 'llap', 'cluster', 'using', 'addsunable', 'to', 'deploy', 'llap', 'cluster', 'using', 'adds.create', 'failure', 'with', 'azure', 'active', 'directory', 'integration']),\n",
       "       list(['unable', 'to', 'drop', 'tables', 'and', 'databases', 'in', 'hiveunable', 'to', 'drop', 'tables', 'and', 'databases', 'in', 'hivehive']),\n",
       "       list(['unable', 'to', 'establish', 'connection', 'to', 'the', 'cluster', 'via', 'odbcunable', 'to', 'establish', 'connection', 'to', 'the', 'cluster', 'via', 'odbcodbc', 'or', 'jdbc', 'connecting', 'to', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['unable', 'to', 'install', 'edgenodeunable', 'to', 'install', 'edgenodehive']),\n",
       "       list(['unable', 'to', 'install', 'hue', 'application', 'to', 'the', 'existing', 'clusterunable', 'to', 'install', 'hue', 'application', 'to', 'the', 'existing', 'clustermapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['unable', 'to', 'kinit', 'with', 'keytab', 'in', 'the', 'esp', 'enabled', 'hdinsight', 'cluster', 'unable', 'to', 'kinit', 'with', 'keytab', 'in', 'the', 'esp', 'enabled', 'hdinsight', 'clusterranger', 'policy', 'auditing']),\n",
       "       list(['unable', 'to', 'launch', 'clusterunable', 'to', 'launch', 'clustercreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'login', 'to', 'ambari', 'using', 'batch', 'accountsunable', 'to', 'login', 'to', 'ambari', 'using', 'batch', 'accountsambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['unable', 'to', 'login', 'to', 'edge', 'node.unable', 'to', 'login', 'to', 'edge', 'node.unable', 'to', 'ssh']),\n",
       "       list(['unable', 'to', 'provision', 'new', 'hdinsight', 'cluster', 'utilizing', 'existing', 'managed', 'sql', 'instanceunable', 'to', 'provision', 'new', 'hdinsight', 'cluster', 'utilizing', 'existing', 'managed', 'sql', 'instancecreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'push', 'messages', 'to', 'kafka', 'topicunable', 'to', 'push', 'messages', 'to', 'kafka', 'topic.kafka']),\n",
       "       list(['unable', 'to', 'query', 'ambari', 'server', 'to', 'retrieve', 'the', 'list', 'of', 'cluster', 'hosts.unable', 'to', 'query', 'ambari', 'server', 'to', 'retrieve', 'the', 'list', 'of', 'cluster', 'hosts.issue', 'with', 'scaling', 'up']),\n",
       "       list(['unable', 'to', 'query', 'storage', 'accountsshuser', '@', 'ed10-anahd1', ':', '~', '$', 'hadoop', 'fs', '-ls', 'wasbs', ':', '//config', '@', 'stor02spark36datahubqa01.blob.core.windows.net/clm_liab2_copy/pickled_objects/*ls', ':', '`', 'wasbs', ':', '//config', '@', 'stor02spark36datahubqa01.blob.core.windows.net/clm_liab2_copy/pickled_objects/*', \"'\", ':', 'no', 'such', 'file', 'or', 'directorysshuser', '@', 'ed10-anahd1', ':', '~', '$', 'azure', 'storage', 'in', 'standard', 'cluster']),\n",
       "       list(['unable', 'to', 'reach', 'repositoryall', 'jobs', 'are', 'stopped', 'due', 'to', 'repo', 'and', 'dependancy', 'not', 'reachable.spark']),\n",
       "       list(['unable', 'to', 'read', 'hiveserver2', 'config', 'from', 'zookeeper200619170348603-oozie-oozi-w', ']', 'action', '[', '0000500-200619170348603-oozie-oozi-w', '@', 'createdailyrawpartitions', ']', 'exception', 'in', 'addtojobconfjava.sql.sqlexception', ':', 'org.apache.hive.jdbc.zookeeperhiveclientexception', ':', 'unable', 'to', 'read', 'hiveserver2', 'configs', 'from', 'zookeeper', 'at', 'org.apache.hive.jdbc.hiveconnection.', '&', 'lt', ';', 'init', '&', 'gt', ';', '(', 'hiveconnection.java:187', ')', 'at', 'org.apache.hive.jdbc.hivedriver.connect', '(', 'hivedriver.java:105', ')', 'at', 'java.sql.drivermanager.getconnection', '(', 'drivermanager.java:664', ')', 'at', 'java.sql.drivermanager.getconnection', '(', 'drivermanager.java:270', ')', 'at', 'org.apache.oozie.action.hadoop.hive2credentials.addtojobconf', '(', 'hive2credentials.java:66', ')', 'at', 'org.apache.oozie.action.hadoop.javaactionexecutor.setcredentialtokens', '(', 'javaactionexecutor.java:1367', ')', 'spark']),\n",
       "       list(['unable', 'to', 'read', 'hiveserver2', 'configs', 'from', 'zookeeperunable', 'to', 'read', 'hiveserver2', 'configs', 'from', 'zookeeperhive']),\n",
       "       list(['unable', 'to', 'restart', \"'cache\", 'metadata', 'server', \"'\", 'on', 'one', 'of', 'the', 'nodeissue', ':', 'io', 'cache', 'not', 'starting', 'on', 'hn1.hadoop']),\n",
       "       list(['unable', 'to', 'restart', 'hive', 'server', '2', 'and', 'hive', 'queries', 'fail', 'unable', 'to', 'restart', 'hive', 'server', '2', 'and', 'hive', 'queries', 'failhive']),\n",
       "       list(['unable', 'to', 'retreive', 'size', 'for', 'large', 'streaming', 'hdfs', 'directories', 'unable', 'to', 'retreive', 'size', 'for', 'large', 'streaming', 'hdfs', 'directoriesadls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['unable', 'to', 'scale', 'the', 'clusterunable', 'to', 'scale', 'the', 'cluster.issue', 'with', 'scaling', 'up']),\n",
       "       list(['unable', 'to', 'scaleup', 'tapcfs', 'clusterwe', 'see', 'the', 'scale', 'up', 'fails', 'with', 'the', 'message', '“', 'unable', 'to', 'connect', 'to', 'cluster', 'management', 'endpoint', 'to', 'perform', 'scaling', 'operation', '.', 'please', 'verify', 'network', 'security', 'rules', 'are', 'not', 'blocking', 'external', 'access', 'to', 'the', 'cluster', 'and', 'that', 'cluster', 'manager', '(', 'ambari', ')', 'ui', 'can', 'be', 'successfully', 'accessed.', '”', 'but', 'this', 'is', 'a', 'standard', 'deployment', 'with', 'no', 'vnet', 'so', 'there', 'is', 'no', 'way', 'for', 'an', 'nsg', 'to', 'cause', 'issues.issue', 'with', 'scaling', 'up']),\n",
       "       list(['unable', 'to', 'setup', 'azure', 'monitorunable', 'to', 'setup', 'azure', 'monitorazure', 'log', 'analytics', 'integration']),\n",
       "       list(['unable', 'to', 'spin', 'up', 'hdinsight', 'cluster', 'using', 'azure', 'template', 'hdinsight', 'serviceunable', 'to', 'spin', 'up', 'hdinsight', 'cluster', 'using', 'azure', 'template', 'hdinsight', 'servicecreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'spin', 'up', 'hdinsight', 'cluster', 'using', 'azure', 'templateunable', 'to', 'spin', 'up', 'hdinsight', 'cluster', 'using', 'azure', 'templatecreate', 'failure', '-', 'other']),\n",
       "       list(['unable', 'to', 'ssh', 'into', 'the', 'cluster', 'from', 'saw120040321001709', '-', 'unable', 'to', 'ssh', 'into', 'the', 'cluster', 'from', 'sawunable', 'to', 'ssh']),\n",
       "       list(['unable', 'to', 'ssh', 'into', 'the', 'clusterunable', 'to', 'ssh', 'into', 'the', 'clusterspark']),\n",
       "       list(['unable', 'to', 'ssh', 'on', 'hn0unable', 'to', 'ssh', 'on', 'hn0lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['unable', 'to', 'start', 'hiveserver2', 'service', 'unable', 'to', 'start', 'hiveserver2', 'serviceinteractive', 'query']),\n",
       "       list(['unable', 'to', 'submit', 'spark', 'app', 'to', 'yarn', 'using', 'oozie.unable', 'to', 'submit', 'spark', 'app', 'to', 'yarn', 'using', 'oozie.mapreduce', ',', 'pig', ',', 'sqoop', 'or', 'oozie']),\n",
       "       list(['unable', 'to', 'sync', 'kafka', 'topics', 'between', 'worker', 'logsunable', 'to', 'sync', 'kafka', 'topics', 'between', 'worker', 'logskafka']),\n",
       "       list(['unable', 'to', 'view', 'logs', 'for', 'in', 'yarn', 'ui', 'for', 'finished', 'spark', 'jobsunable', 'to', 'view', 'logs', 'for', 'in', 'yarn', 'ui', 'for', 'finished', 'spark', 'jobs', ':', 'for', 'finished', 'logs', ',', 'the', 'yarn', 'ui', 'does', 'not', 'show', 'the', 'executor', 'logs', '.', 'spark', 'history', 'server', 'is', 'not', 'refreshed', 'regularly', '.', 'contains', 'logs', 'older', 'than', 'a', 'month.spark']),\n",
       "       list(['unable', 'to', 'view', 'tables', 'on', 'hiveunable', 'to', 'view', 'tables', 'on', 'hivespark']),\n",
       "       list(['unavailable', 'to', 'ssh', 'into', 'the', 'cluster', ':', 'gateway', 'connectivity', 'failureunavailable', 'to', 'ssh', 'into', 'the', 'cluster', ':', 'gateway', 'connectivity', 'failureunable', 'to', 'ssh']),\n",
       "       list(['unified', 'perf', '||', 'azure', 'hdinsight', 'service', '||', 'restricting', 'public', 'network', 'access', 'to', 'ssh', 'services.restricting', 'public', 'network', 'access', 'to', 'ssh', 'services.lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['unknow', 'status', 'from', 'headnode', 'host', '1unknow', 'status', 'from', 'headnode', 'host', '1node', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['unknown', 'host', 'exception', 'host', 'wn6-prd-ts', 'hbase', 'region', 'servers', 'are', 'communicating', 'only', 'with', 'ip', 'addresseshbase']),\n",
       "       list(['unpin', 'request', 'unpin', 'request', 'spark']),\n",
       "       list(['unresponsive', 'nodecannot', 'ssh', 'into', 'the', 'nodeunable', 'to', 'ssh']),\n",
       "       list(['updating', 'errorcluster', 'stuck', 'in', 'updating', 'error', 'state', 'when', 'trying', 'to', 'scale', 'upissue', 'with', 'autoscaling']),\n",
       "       list(['use', 'empty', 'edge', 'nodes', 'on', 'apache', 'hadoop', 'clusters', 'in', 'hdinsightuse', 'empty', 'edge', 'nodes', 'on', 'apache', 'hadoop', 'clusters', 'in', 'hdinsightissue', 'with', 'autoscaling']),\n",
       "       list(['use', 'public', 'ip', 'for', 'kafka', 'worker', 'node', 'for', 'bootstrap', 'serversuse', 'public', 'ip', 'for', 'kafka', 'worker', 'node', 'for', 'bootstrap', 'serverskafka']),\n",
       "       list(['user', 'is', 'not', 'able', 'to', 'run', 'spark', 'job', 'with', 'livy', 'interpreter', 'users', 'are', 'unable', 'to', 'run', 'zeppelin', 'commandsspark']),\n",
       "       list(['user', 'not', 'able', 'to', 'run', 'the', 'query', 'for', 'wms', 'databaseuser', 'was', 'not', 'able', 'to', 'query', 'any', 'tables', 'in', 'wms', 'databasehive']),\n",
       "       list(['users', 'belonging', 'to', 'the', 'esp', 'cluster', 'access', 'group', 'are', 'not', 'able', 'to', 'loginambari', 'and', 'ranger', 'was', 'showing', 'domain', 'user', 'synced', ',', 'kerberos', 'authenticated', 'but', 'still', 'was', 'not', 'able', 'to', 'access', 'ambari', 'with', 'domain', 'user.ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['users', 'not', 'able', 'to', 'see', 'databases', 'in', 'hiveunable', 'to', 'use', 'the', 'group', 'in', 'ranger', 'polociesranger', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['using', '.jar', 'in', 'jupyter', 'notebooks120060924008825', '-', 'using', '.jar', 'in', 'jupyter', 'notebooksnotebooks']),\n",
       "       list(['validation', 'failed120061624002105', '-', 'cluster', 'validation', 'failedcreate', 'failure', '-', 'other']),\n",
       "       list(['various', 'ambari/grafana', 'panels', 'not', 'loadingvarious', 'ambari/grafana', 'panels', 'not', 'loading', '-', 'memory', 'hbase']),\n",
       "       list(['version', 'compatibility', 'issue', 'with', 'clusterversion', 'compatibility', 'issue', 'with', 'clusterspark']),\n",
       "       list(['very', 'high', 'wal', 'slow', 'sync', 'cost', 'caused', 'clients', 'failed', 'to', 'get', 'region', 'server', '.', '[', 'sync.3', ']', 'wal.fshlog', ':', 'slow', 'sync', 'cost', ':', '301090', 'ms', ',', 'current', 'pipeline', ':', '[', ']', 'host', '=', 'wn6-adobeisource', '=', '/var/log/hbase/hbase-hbase-regionserver-wn6-adobei.logsourcetype', '=', 'log4jhbase']),\n",
       "       list(['warn', 'shutdownhookmanager', ':', 'shutdownhook', \"'\", '$', 'anon', '$', '2', \"'\", 'timeout', ',', 'java.util.concurrent.timeoutexception', 'java.util.concurrent.timeoutexceptionsome', 'logs', 'from', 'stderr', ':', 'https', ':', '//dtspwsuwsparkcluster1a.azurehdinsight.net/yarnui/jobhistory/logs/wn7574-dtspws.vorgjpf0h2he3jil1zutnsraea.dx.internal.cloudapp.net/port/30050/container_e602_1588918665250_0013_01_000001/container_e602_1588918665250_0013_01_000001/root/stderr/', '?', 'start=0', '20/05/08', '07:08:34', 'info', 'retryinvocationhandler', ':', 'java.net.connectexception', ':', 'call', 'from', 'wn7574-dtspws/10.20.8.12', 'to', 'hn1-dtspws.vorgjpf0h2he3jil1zutnsraea.dx.internal.cloudapp.net:8030', 'failed', 'on', 'connection', 'exception', ':', 'java.net.connectexception', ':', 'connection', 'refused', ';', 'for', 'more', 'details', 'see', ':', 'http', ':', '//wiki.apache.org/hadoop/connectionrefused', ',', 'while', 'invoking', 'applicationmasterprotocolpbclientimpl.finishapplicationmaster', 'over', 'null', '.', 'retrying', 'after', 'sleeping', 'for', '30000ms.20/05/08', '07:09:02', 'warn', 'shutdownhookmanager', ':', 'shutdownhook', \"'\", '$', 'anon', '$', '2', \"'\", 'timeout', ',', 'java.util.concurrent.timeoutexceptionjava.util.concurrent.timeoutexception', 'at', 'java.util.concurrent.futuretask.get', '(', 'futuretask.java:205', ')', 'at', 'org.apache.hadoop.util.shutdownhookmanager.executeshutdown', '(', 'shutdownhookmanager.java:124', ')', 'at', 'org.apache.hadoop.util.shutdownhookmanager', '$', '1.run', '(', 'shutdownhookmanager.java:95', ')', '20/05/08', '07:09:02', 'error', 'utils', ':', 'uncaught', 'exception', 'in', 'thread', 'shutdown-hook-0java.lang.reflect.undeclaredthrowableexception', 'at', 'com.sun.proxy.', '$', 'proxy24.finishapplicationmaster', '(', 'unknown', 'source', ')', 'at', 'sun.reflect.nativemethodaccessorimpl.invoke0', '(', 'native', 'method', ')', 'at', 'sun.reflect.nativemethodaccessorimpl.invoke', '(', 'nativemethodaccessorimpl.java:62', ')', 'at', 'sun.reflect.delegatingmethodaccessorimpl.invoke', '(', 'delegatingmethodaccessorimpl.java:43', ')', 'ambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['we', 'are', 'experiencing', 'several', 'unrelated', 'errors', 'in', 'activities', 'that', 'try', 'to', 'create', 'hdi', 'clusters.we', 'are', 'experiencing', 'several', 'unrelated', 'errors', 'in', 'activities', 'that', 'try', 'to', 'create', 'hdi', 'clusters.pipeline', 'or', 'trigger', 'run', 'failure']),\n",
       "       list(['we', 'are', 'facing', 'an', 'error', 'for', 'one', 'of', 'the', 'production', 'hdinsight', 'cluster', 'for', 'hive', 'queries.we', 'are', 'facing', 'an', 'error', 'for', 'one', 'of', 'the', 'production', 'hdinsight', 'cluster', 'for', 'hive', 'queries.hive']),\n",
       "       list(['we', 'are', 'getting', 'odbc', 'connection', 'error', 'when', 'connecting', 'to', 'hdi', 'cluster', 'using', 'hive', 'odbc', 'driver', '.', 'same', 'setting', 'are', 'working', 'randomly', '.', 'symptom', ':', 'hive', 'odbc', 'connection', 'failing', 'to', 'connect.hive', 'view']),\n",
       "       list(['we', 'are', 'not', 'able', 'to', 'see', 'the', 'hbase', 'config', 'from', 'the', 'ambari.lingering', 'pid', 'of', 'ambari-server', 'on', 'hn1hbase']),\n",
       "       list(['we', 'are', 'receving', 'an', 'alert', 'for', 'the', 'livy', 'service', 'on', 'this', 'node', 'even', 'though', 'it', 'is', 'in', 'a', 'stopped', 'state', 'and', 'in', 'matineance', 'mode', '.', 'we', 'are', 'receving', 'an', 'alert', 'for', 'the', 'livy', 'service', 'on', 'this', 'node', 'even', 'though', 'it', 'is', 'in', 'a', 'stopped', 'state', 'and', 'in', 'matineance', 'mode.spark']),\n",
       "       list(['we', 'are', 'unable', 'to', 'configure', 'presto', 'in', 'hd', 'insightscannot', 'configure', 'prestomy', 'issue', 'is', 'not', 'listed', 'above']),\n",
       "       list(['we', 'are', 'unable', 'to', 'connect', 'to', 'one', 'of', 'our', 'spark', 'serverwe', 'are', 'unable', 'to', 'connect', 'to', 'one', 'of', 'our', 'spark', 'serverspark']),\n",
       "       list(['we', 'had', 'an', 'azure', 'outage', 'tvw2-vd0', '.', 'hdi', 'cluster', 'is', 'downoutage', 'caused', 'this', 'service', 'to', 'go', 'downhbase']),\n",
       "       list(['we', 'had', 'an', 'outage', 'on', 'our', 'hdinsight', 'cluster', '(', 'message', 'were', 'lost', ')', '-', 'zookeeper', 'node', 'was', 'unreachable', '(', 'zk0we', 'had', 'an', 'outage', 'on', 'our', 'hdinsight', 'cluster', '(', 'message', 'were', 'lost', ')', '-', 'zookeeper', 'node', 'was', 'unreachable', '(', 'zk0kafka']),\n",
       "       list(['we', 'have', 'some', 'general', 'questions', 'surrounding', 'network', 'security', 'of', 'hdinsight', 'clustersclient', 'has', 'some', 'general', 'questions', 'surrounding', 'network', 'security', 'of', 'hdinsight', 'clusters.hbase']),\n",
       "       list(['we', 'need', 'to', 'know', 'what', 'ip', 'addresses', 'to', 'give', 'vendors', 'to', 'whitelist', 'when', 'they', 'are', 'recieving', 'data', 'from', 'out', 'hdi', 'assetswe', 'need', 'to', 'know', 'what', 'ip', 'addresses', 'to', 'give', 'vendors', 'to', 'whitelist', 'when', 'they', 'are', 'recieving', 'data', 'from', 'out', 'hdi', 'assetsambari', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['we', 'want', 'to', 'know', 'is', 'it', 'mandatory', 'that', 'when', 'the', 'java', 'upgrade', 'is', 'done', 'from', 'ms', 'end', ',', 'we', 'have', 'to', 'restart', 'all', 'the', 'hadoop', 'and', 'mosaic', 'services', '?', 'we', 'want', 'to', 'know', 'is', 'it', 'mandatory', 'that', 'when', 'the', 'java', 'upgrade', 'is', 'done', 'from', 'ms', 'end', ',', 'we', 'have', 'to', 'restart', 'all', 'the', 'hadoop', 'and', 'mosaic', 'services', '?', 'spark']),\n",
       "       list(['we', 'want', 'to', 'know', 'which', 'nodes', 'will', 'be', 'deleted', 'when', 'we', 'downscale', 'the', 'clusteradvisory/guidancewe', 'want', 'to', 'know', 'which', 'nodes', 'will', 'be', 'deleted', 'when', 'we', 'downscale', 'the', 'issue', 'with', 'scaling', 'down']),\n",
       "       list(['webhdfs', 'configuration', 'to', 'access', 'sotrage', 'containerwebhdfs', 'configuration', 'to', 'access', 'sotrage', 'containerazure', 'storage', 'in', 'standard', 'cluster']),\n",
       "       list(['websocket', 'connection', 'issues', 'through', 'zeppelinjava', 'heap', 'space', 'exceptions', 'in', 'the', 'loginteractive', 'query']),\n",
       "       list(['west', 'prdsup', '-', 'kpps83sparkespprdsupwus201', '-', 'hive', 'warehouce', 'config', 'failing', 'with', 'spak', 'submit', 'in', 'cluster', 'modehwc', 'spark-submit', 'fails', 'with', 'cluster', 'mode', 'when', 'spark.security.credentials.hiveserver2.enabled=falsehive']),\n",
       "       list(['wn', '7', '&', 'hn', '0', 'services', 'are', 'down', 'in', 'production', 'clusterwn', '7', '&', 'hn', '0', 'services', 'are', 'down', 'in', 'production', 'clusterspark']),\n",
       "       list(['wn2', 'is', 'loosing', 'heartbeat', '.', 'please', 'restart', 'it', 'from', 'backend.wn2', 'is', 'losing', 'heartbeat', '.', 'please', 'restart', 'it', 'from', 'backend.lost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['wn8', 'not', 'responding', 'after', 'rebootunable', 'to', 'access', 'wn8unable', 'to', 'ssh']),\n",
       "       list(['woker', 'node', 'downwn43', 'was', 'downnode', 'unresponsive', 'or', 'sluggish']),\n",
       "       list(['worker', 'node', 'is', 'booting', 'up', 'since', 'quite', 'some', 'time', '.', \"'system\", 'is', 'booting', 'up', '.', 'see', 'pam_nologin', '(', '8', ')', \"'\", 'connection', 'to', 'wn3-sca01.cvshaadds.com', 'closed', 'by', 'remote', 'host', '.', 'connection', 'to', 'wn3-sca01.cvshaadds.com', 'closed.unable', 'to', 'ssh']),\n",
       "       list(['worker', 'node', 'is', 'downunable', 'to', 'sshlost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['worker', 'node', 'wn-12', 'is', 'dead', 'and', 'can', 'not', 'recoverworker', 'node', 'wn-12', 'is', 'dead', 'and', 'can', 'not', 'recover.unable', 'to', 'ssh']),\n",
       "       list(['worker', 'node', 'wn170-ahd649', 'is', 'in', 'decommissoned', 'status120062924004079', '-', 'worker', 'node', 'wn170-ahd649', 'is', 'in', 'decommissoned', 'statusissue', 'with', 'scaling', 'down']),\n",
       "       list(['worker', 'nodes', 'are', 'going', 'off', 'lineworker', 'nodes', 'are', 'going', 'off', 'line', 'and', 'no', 'heartbeat', 'infomation', 'observed.hbase']),\n",
       "       list(['worker', 'nodes', 'downworker', 'nodes', 'downlost', 'network', 'connectivity', 'between', 'nodes']),\n",
       "       list(['worker', 'nodes', 'frequently', 'down', 'for', 'emea', 'prod', ',', 'stg', 'and', 'china', 'stgunable', 'to', 'access', 'worker', 'nodes', 'and', 'ambari', 'showed', 'multiple', 'alertshadoop']),\n",
       "       list(['workernode', '24', 'is', 'down', 'in', 'dev-offprev-hdispark-clusterworkernode', '24', 'is', 'down', 'in', 'dev-offprev-hdispark-clusterunable', 'to', 'ssh']),\n",
       "       list(['working', 'and', 'tested', 'arm', 'template', 'is', 'return', 'error', 'when', 'deployedarm', 'template', 'is', 'return', 'error', 'when', 'deployedcreate', 'failure', '-', 'other']),\n",
       "       list(['yarn', '-', 'application', 'logs', 'not', 'showing', 'in', 'ambari', '.', 'getting', 'http', 'erroryarn', '-', 'application', 'logs', 'not', 'showing', 'in', 'ambari', '.', 'getting', 'http', 'errorhadoop']),\n",
       "       list(['yarn', 'application', 'resources', 'were', 'not', 'allocated', 'to', 'the', 'jobs', 'waiting', 'in', 'que', 'yarn', 'application', 'resources', 'were', 'not', 'allocated', 'to', 'the', 'jobs', 'waiting', 'in', 'quespark']),\n",
       "       list(['yarn', 'is', 'not', 'working', 'as', 'expectedyarn', 'is', \"n't\", 'showing', 'the', 'applications', 'in', 'the', 'ui', '.', 'no', 'information', 'available.spark']),\n",
       "       list(['yarn', 'logs', 'between', '6/1~6/5', 'missingmissing', 'yarn', 'logs', 'in', 'yarn', 'uispark']),\n",
       "       list(['yarn', 'memory', 'usage', '100', '%', 'with', 'jupyter', 'nodeswhen', 'the', 'client', 'launch', 'one', 'jupyter', 'notebook', 'in', 'a', 'hdinsight', 'spark', 'cluster', ',', 'it', 'uses', 'excessive', 'amount', 'of', 'resources', '(', 'yarn', 'memory', ')', '.', 'when', 'they', 'trigger', '4', 'jupyter', 'notebooks', 'yarn', 'memory', 'is', 'already', 'at', '100', '%', 'of', 'its', 'capacity.notebooks']),\n",
       "       list(['yarn', 'portal', 'not', 'working', '.', 'cluster', 'in', 'weird', 'stateyarn', 'portal', 'not', 'working', '.', 'cluster', 'in', 'weird', 'stateissue', 'with', 'autoscaling']),\n",
       "       list(['yarn', 'query', 'failingyarn', 'query', 'failingspark']),\n",
       "       list(['yarn', 'resource', 'manager', 'not', 'workingambari', 'services', 'are', 'not', 'healthy', '.', 'both', 'rm', 'are', 'in', 'standby', 'mode.hadoop']),\n",
       "       list(['yarn', 'services', 'are', 'downyarn', 'resource', 'managers', 'are', 'down', '.', 'the', 'oozie', 'jobs', 'are', 'not', 'being', 'processed.after', 'reviewing', 'the', 'logs', 'we', 'found', ':', '2020-04-08', '07:56:32,598', 'warn', 'ha.activestandbyelector', '-', 'exception', 'handling', 'the', 'winning', 'of', 'electioncausing', 'rmstatestore', 'get', 'stuck', 'in', 'the', 'state', 'fenced', 'and', 'many', 'other', 'errors', ';', '2020-04-08', '07:12:48,261', 'info', 'recovery.zkrmstatestore', '-', 'fencing', 'node', '/rmstore/zkrmstateroot/rm_zk_fencing_lock', 'does', \"n't\", 'exist', 'to', 'delete2020-04-08', '07:55:54,254', 'error', 'recovery.rmstatestore', '-', 'error', 'storing', 'appattempt', ':', 'appattempt_1586329968056_0760_000001\\u200b2020-04-08', '07:55:54,271', 'error', 'recovery.rmstatestore', '-', 'state', 'store', 'operation', 'failed', 'org.apache.hadoop.yarn.server.resourcemanager.recovery.storefencedexception', ':', 'rmstatestore', 'has', 'been', 'fencedhadoop']),\n",
       "       list(['yarn', 'stops', 'after', 'queue', 'config', 'changeyarn', 'stops', 'after', 'queue', 'config', 'changehadoop']),\n",
       "       list(['yarn.resourcemanager.am.max-attemptsthe', 'cx', 'changed', 'this', 'parameter', 'to', '1', ',', 'and', 'their', 'job', 'was', 'not', 're-executed', 'when', 'it', 'failed', 'the', 'first', 'time.spark']),\n",
       "       list(['yarnui', 'not', 'reachableyarnui', 'not', 'reachablehadoop']),\n",
       "       list(['yarnui', 'rest', 'ap', 'returns', 'http', 'error', 'on', 'active', 'leadyarn', 'ha', 'flipover', 'gateway', 'issuesspark']),\n",
       "       list(['zeppelin', 'is', 'not', 'working', 'to', 'access', 'hive', 'tablesunable', 'to', 'access/use', 'hive', 'tables', 'zeppelin', 'ui', 'notebooks']),\n",
       "       list(['zeppelin', 'not', 'startingzeppelin', 'not', 'startingnotebooks']),\n",
       "       list(['zeppelin', 'thrift', 'server', 'connection', 'issuezeppelin', 'thrift', 'server', 'connection', 'issuenotebooks']),\n",
       "       list(['zookeeper', 'instability', 'prevent', 'tables', 'to', 'be', 'created', 'in', 'hbaseunable', 'to', 'run', 'reach', 'hbase', 'cluster', 'from', 'spark', 'cluster.hbase']),\n",
       "       list(['zookeeper', 'logs', 'configuration.in', 'prod', 'cluster', '“', 'prod05rxperso', '”', ',', 'credential', 'service', 'gets', 'restarted', 'every', '6', 'hours', 'multiple', 'times', 'spanning', '2-3', 'minutes', 'of', 'total', 'time', 'and', 'jobs', 'running', 'during', 'this', 'interval', 'are', 'failing', 'due', 'to', 'that.adls', 'gen1', ',', 'adls', 'gen2', 'in', 'cluster', 'with', 'enterprise', 'security', 'package']),\n",
       "       list(['zookeeper', 'node', 'downzookeeper', 'node', 'down', 'and', 'no', 'metric', 'data', 'observed', 'lost', 'network', 'connectivity', 'between', 'nodes'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_titles=np.array(processed_title)\n",
    "np.unique(np_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating DF for all words¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len (processed_Resolution)\n",
    "\n",
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_Resolution[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_size = len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8073"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = [x for x in DF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'vm', 'in', 'question', 'is', 'not', 'hosted', 'on', 'node', 'that', 'was', 'listed', 'during', 'time', 'period', 'of', 'this', 'incident', 'so', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(total_vocab[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating TF-IDF for body, we will consider this as the actual tf-idf as we will add the title weight to this.¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_Resolution[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF-IDF for Title¶\n",
    "# Calculating TF-IDF for Title¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf_title = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_Resolution[i])\n",
    "    words_count = len(tokens + processed_Resolution[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
    "        \n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10174757531634587"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[1,'livy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, ','): 0.01644171398057013,\n",
       " (0, 'account'): 0.059286297501231044,\n",
       " (0, 'adls'): 0.06202672774388121,\n",
       " (0, 'cluster'): 0.015098503111282984,\n",
       " (0, 'enterprise'): 0.061542558869560206,\n",
       " (0, 'error'): 0.039684879053913276,\n",
       " (0, 'for'): 0.022649982010715174,\n",
       " (0, 'gen1'): 0.07561784893133112,\n",
       " (0, 'gen2'): 0.06464499034436205,\n",
       " (0, 'in'): 0.03055678568437151,\n",
       " (0, 'kerbaroes'): 0.11281628000752487,\n",
       " (0, 'oftenadls'): 0.12076657624493985,\n",
       " (0, 'package'): 0.06107005785820608,\n",
       " (0, 'reboots'): 0.11281628000752487,\n",
       " (0, 'security'): 0.05688233040138139,\n",
       " (0, 'service'): 0.03906725813118051,\n",
       " (0, 'tgt'): 0.10717545505748996,\n",
       " (0, 'the'): 0.02554596441600197,\n",
       " (0, 'uservm'): 0.12076657624493985,\n",
       " (0, 'very'): 0.07999321268259015,\n",
       " (0, 'with'): 0.016489713620543463,\n",
       " (1, \"'s\"): 0.03800120211825088,\n",
       " (1, '('): 0.01869574887807232,\n",
       " (1, ')'): 0.01846348129999388,\n",
       " (1, ','): 0.06368562630448683,\n",
       " (1, 'and'): 0.006105589699033559,\n",
       " (1, 'application'): 0.03648038804430071,\n",
       " (1, 'applicatoin'): 0.07796323276572066,\n",
       " (1, 'are'): 0.0198612511432138,\n",
       " (1, 'assignedout'): 0.07796323276572066,\n",
       " (1, 'being'): 0.04501779370946265,\n",
       " (1, 'created'): 0.04209977271437616,\n",
       " (1, 'due'): 0.07067846983558124,\n",
       " (1, 'exceptions'): 0.05426953432392838,\n",
       " (1, 'failing'): 0.07710329314636856,\n",
       " (1, 'from'): 0.017152839794413628,\n",
       " (1, 'hive'): 0.02427555235343274,\n",
       " (1, 'id'): 0.047610128046753956,\n",
       " (1, 'in'): 0.009863266265208526,\n",
       " (1, 'livy'): 0.10174757531634587,\n",
       " (1, 'logs'): 0.06702658100407373,\n",
       " (1, 'mapreduce'): 0.06041520287812712,\n",
       " (1, 'memory'): 0.0919839682071817,\n",
       " (1, 'no'): 0.03277927458241219,\n",
       " (1, 'not'): 0.012229601741793958,\n",
       " (1, 'of'): 0.029500435764862238,\n",
       " (1, 'pig'): 0.0501502634323508,\n",
       " (1, 'pipelines'): 0.07283076304283252,\n",
       " (1, 'service'): 0.02522063499607856,\n",
       " (1, 'some'): 0.03648038804430071,\n",
       " (1, 'spark'): 0.02645656499055851,\n",
       " (1, 'streaming'): 0.05892427837614758,\n",
       " (1, 'the'): 0.023088377965854945,\n",
       " (1, 'timeout.hdinsight'): 0.07796323276572066,\n",
       " (1, 'to'): 0.012435057359679131,\n",
       " (1, 'with'): 0.01064525816009768,\n",
       " (2, ','): 0.11180365506787689,\n",
       " (2, 'ambarimapreduce'): 0.4106063592327955,\n",
       " (2, 'in'): 0.05194653566343157,\n",
       " (2, 'manager'): 0.2679352816663775,\n",
       " (2, 'node'): 0.13492858878330513,\n",
       " (2, 'oozie'): 0.24778322354151525,\n",
       " (2, 'or'): 0.12058783014217545,\n",
       " (2, 'pig'): 0.26412472074371424,\n",
       " (2, 'sqoop'): 0.27197692312080646,\n",
       " (2, 'unhealthy'): 0.2858195474393561,\n",
       " (2, 'warnings'): 0.3643965471954659,\n",
       " (3, 'cluster'): 0.004400135192431041,\n",
       " (3, 'not'): 0.01104158328687683,\n",
       " (3, 'scaleissue'): 0.03519483079138247,\n",
       " (3, 'scaling'): 0.04756637357393415,\n",
       " (3, 'to'): 0.005613540179512294,\n",
       " (3, 'up'): 0.012142027144797358,\n",
       " (3, 'upunable'): 0.03287788731647868,\n",
       " (3, 'with'): 0.01922229473480495,\n",
       " (4, 'nan'): 0.0647849711231359,\n",
       " (5, \"'s\"): 0.0612672442314657,\n",
       " (5, 'cluster'): 0.015714768544396573,\n",
       " (5, 'extremely'): 0.11154996342718342,\n",
       " (5, 'is'): 0.018083588365881347,\n",
       " (5, 'overall'): 0.11742102613028099,\n",
       " (5, 'performance'): 0.14669983800124065,\n",
       " (5, 'performancehive'): 0.1256958242549374,\n",
       " (5, 'query'): 0.04756028555107832,\n",
       " (5, 'slowslower'): 0.1256958242549374,\n",
       " (6, '-'): 0.026437815773347733,\n",
       " (6, '2.1'): 0.17841208799987507,\n",
       " (6, '3.6'): 0.1588122152459592,\n",
       " (6, 'a'): 0.0576100809974458,\n",
       " (6, 'along'): 0.19876175301570864,\n",
       " (6, 'an'): 0.04277150724948387,\n",
       " (6, 'and'): 0.017539694044496405,\n",
       " (6, 'azure'): 0.03962570426527222,\n",
       " (6, 'being'): 0.06466192187359181,\n",
       " (6, 'cluster'): 0.02800086031547026,\n",
       " (6, 'creation'): 0.26721372645119507,\n",
       " (6, 'data'): 0.135425166359174,\n",
       " (6, 'failure'): 0.03473818448077407,\n",
       " (6, 'gen2'): 0.17983060950340715,\n",
       " (6, 'hdi'): 0.08636020925722442,\n",
       " (6, 'hdinsight'): 0.1038263279276569,\n",
       " (6, 'is'): 0.03222166654284313,\n",
       " (6, 'lake'): 0.2161020442448571,\n",
       " (6, 'of'): 0.042373353189529395,\n",
       " (6, 'problem.create'): 0.11198355251803513,\n",
       " (6, 'problem120022523001923'): 0.11198355251803513,\n",
       " (6, 'spark'): 0.07600249579105899,\n",
       " (6, 'storage'): 0.04584562234119177,\n",
       " (6, 'version'): 0.10994913354773757,\n",
       " (6, 'with'): 0.07645230860433788,\n",
       " (7, \"'\"): 0.02791799558505459,\n",
       " (7, \"'at\"): 0.020530317961639778,\n",
       " (7, \"'code\"): 0.018219827359773295,\n",
       " (7, \"'deploymentfailed\"): 0.020530317961639778,\n",
       " (7, \"'message\"): 0.018219827359773295,\n",
       " (7, ','): 0.01397545688348461,\n",
       " (7, '.'): 0.015022421428068254,\n",
       " (7, '//aka.ms/deployoperations'): 0.020530317961639778,\n",
       " (7, ':'): 0.02936602430871386,\n",
       " (7, 'a'): 0.010561848182865064,\n",
       " (7, 'aads'): 0.08212127184655911,\n",
       " (7, 'ambari'): 0.01815289966638249,\n",
       " (7, 'an'): 0.01568288599147742,\n",
       " (7, 'and'): 0.0032156105748243416,\n",
       " (7, 'as'): 0.025815627683213375,\n",
       " (7, 'azure'): 0.014529424897266482,\n",
       " (7, 'cluster'): 0.007700236586754322,\n",
       " (7, 'create'): 0.007557583634604356,\n",
       " (7, 'creation'): 0.012247295795679776,\n",
       " (7, 'data'): 0.00827598238861619,\n",
       " (7, 'deploy'): 0.021257872819481546,\n",
       " (7, 'deployment'): 0.021089087432919613,\n",
       " (7, 'details'): 0.012389161177075763,\n",
       " (7, 'details.2'): 0.020530317961639778,\n",
       " (7, 'error'): 0.006746429439165257,\n",
       " (7, 'esp'): 0.025074667437957086,\n",
       " (7, 'failed'): 0.007992984242661236,\n",
       " (7, 'failing,1'): 0.020530317961639778,\n",
       " (7, 'failure'): 0.00636866715480858,\n",
       " (7, 'for'): 0.02310298165092948,\n",
       " (7, 'gen2'): 0.010989648358541548,\n",
       " (7, 'getting'): 0.010715521364418308,\n",
       " (7, 'hdinsight'): 0.0063449422622457,\n",
       " (7, 'https'): 0.01245757298229028,\n",
       " (7, 'in'): 0.02077861426537263,\n",
       " (7, 'incorrectly'): 0.01917876760127923,\n",
       " (7, 'is'): 0.02067556936499101,\n",
       " (7, 'lake'): 0.013206236037185713,\n",
       " (7, 'least'): 0.018219827359773295,\n",
       " (7, 'list'): 0.013598846156040326,\n",
       " (7, 'message'): 0.012389161177075763,\n",
       " (7, 'of'): 0.011652672127120584,\n",
       " (7, 'one'): 0.009137895682928558,\n",
       " (7, 'operation'): 0.012247295795679776,\n",
       " (7, 'operations'): 0.015165524920192774,\n",
       " (7, 'opposed'): 0.020530317961639778,\n",
       " (7, 'please'): 0.01775058551350302,\n",
       " (7, 'reads'): 0.01917876760127923,\n",
       " (7, 'resource'): 0.010462235007825236,\n",
       " (7, 'see'): 0.008192644714598131,\n",
       " (7, 'showing'): 0.013206236037185713,\n",
       " (7, 'storage'): 0.008405030762551825,\n",
       " (7, 'that'): 0.02191513836722236,\n",
       " (7, 'to'): 0.007367771485609885,\n",
       " (7, 'unable'): 0.007233704473092195,\n",
       " (7, 'usage'): 0.013598846156040326,\n",
       " (7, 'user'): 0.04741515639699012,\n",
       " (7, 'username'): 0.05060483099823824,\n",
       " (7, 'what'): 0.026793528166637746,\n",
       " (7, 'with'): 0.01962275920844672,\n",
       " (7, '{'): 0.01211122248061226,\n",
       " (8, 'nan'): 0.05758664099834302,\n",
       " (9, '-'): 0.09693865783560836,\n",
       " (9, 'clustercreate'): 0.2858195474393561,\n",
       " (9, 'clusterunable'): 0.28087901596244136,\n",
       " (9, 'create'): 0.1511516726920871,\n",
       " (9, 'creating'): 0.24222444961224515,\n",
       " (9, 'erroe'): 0.4106063592327955,\n",
       " (9, 'failure'): 0.1273733430961716,\n",
       " (9, 'hdinsight'): 0.253797690489828,\n",
       " (9, 'other'): 0.1312960430977005,\n",
       " (9, 'to'): 0.016372825523577522,\n",
       " (9, 'while'): 0.18275791365857114,\n",
       " (10, '-'): 0.08553410985494855,\n",
       " (10, 'all'): 0.13336912296360626,\n",
       " (10, 'cluster'): 0.04529550933384895,\n",
       " (10, 'health'): 0.12188801082204907,\n",
       " (10, 'heartbeat'): 0.2330512241856302,\n",
       " (10, 'lost'): 0.22987079940502575,\n",
       " (10, 'nodes.hadoop'): 0.1811498643674098,\n",
       " (10, 'nodeshealth'): 0.1811498643674098,\n",
       " (10, 'of'): 0.06854513015953285,\n",
       " (10, 'on'): 0.058488789092259935,\n",
       " (11, 'autoscaling'): 0.09362633865414711,\n",
       " (11, 'error'): 0.08995239252220343,\n",
       " (11, 'errorcluster'): 0.13686878641093184,\n",
       " (11, 'in'): 0.017315511887810525,\n",
       " (11, 'scale'): 0.06446664112156558,\n",
       " (11, 'state'): 0.06817895411408038,\n",
       " (11, 'stuck'): 0.08570022878884194,\n",
       " (11, 'to'): 0.0218304340314367,\n",
       " (11, 'trying'): 0.08164863863786517,\n",
       " (11, 'updating'): 0.19410381863395015,\n",
       " (11, 'upissue'): 0.12785845067519486,\n",
       " (11, 'when'): 0.05380946156019033,\n",
       " (11, 'with'): 0.018688342103282594,\n",
       " (12, 'nan'): 0.07148686468759824,\n",
       " (13, 'hive'): 0.19177686359211865,\n",
       " (13, 'service'): 0.09962150823451031,\n",
       " (13, 'services'): 0.12413973582924286,\n",
       " (13, 'starthadoop'): 0.28768151401918846,\n",
       " (13, 'starthive'): 0.30795476942459665,\n",
       " (13, 'to'): 0.036838857428049425,\n",
       " (13, 'unable'): 0.21701113419276585,\n",
       " (14, ','): 0.014583085443636115,\n",
       " (14, '.'): 0.010687888763052511,\n",
       " (14, 'a'): 0.018368431622374025,\n",
       " (14, 'account'): 0.07887655232772478,\n",
       " (14, 'account.hadoop'): 0.053557351204277674,\n",
       " (14, 'and'): 0.008388549325628717,\n",
       " (14, 'breaking'): 0.10006313531102207,\n",
       " (14, 'cluster'): 0.06695857901525497,\n",
       " (14, 'deploy'): 0.055455320398647506,\n",
       " (14, 'determined'): 0.10006313531102207,\n",
       " (14, 'disable'): 0.07912447784448404,\n",
       " (14, 'downwe'): 0.053557351204277674,\n",
       " (14, 'enable'): 0.06463910179343876,\n",
       " (14, 'enabled'): 0.06541217592510544,\n",
       " (14, 'fix'): 0.04825574130688972,\n",
       " (14, 'has'): 0.04503587290453153,\n",
       " (14, 'hdinsight'): 0.03310404658562974,\n",
       " (14, 'if'): 0.03533845223385811,\n",
       " (14, 'manager'): 0.03494808021735358,\n",
       " (14, 'new'): 0.03997157699915369,\n",
       " (14, 'on'): 0.017292337644668157,\n",
       " (14, 'or'): 0.03145769481969794,\n",
       " (14, 'please'): 0.04630587525261657,\n",
       " (14, 'recommend'): 0.0578414268597516,\n",
       " (14, 'recreate'): 0.06796180156322501,\n",
       " (14, 'resource'): 0.027292786976935397,\n",
       " (14, 'secure'): 0.288291642987803,\n",
       " (14, 'sensei-dsnpalpha-hdi'): 0.10711470240855535,\n",
       " (14, 'storage'): 0.08770466882662774,\n",
       " (14, 'that'): 0.022867970470145074,\n",
       " (14, 'the'): 0.022658159742888703,\n",
       " (14, 'this'): 0.023074602259479906,\n",
       " (14, 'to'): 0.0042711718757158755,\n",
       " (14, 'transfer'): 0.3802398753343992,\n",
       " (14, 'want'): 0.07095050168368865,\n",
       " (14, 'wasb'): 0.077466909500061,\n",
       " (14, 'we'): 0.04093144800818843,\n",
       " (14, 'while'): 0.047675977476149,\n",
       " (14, 'with'): 0.014625659037351593,\n",
       " (14, 'without'): 0.0631889868553683,\n",
       " (14, 'would'): 0.040529634643527615,\n",
       " (14, 'you'): 0.06224860994869258,\n",
       " (15, 'nan'): 0.34551984599005814,\n",
       " (16, '('): 0.018234125448984114,\n",
       " (16, ')'): 0.018007592872833537,\n",
       " (16, ','): 0.03105657085218802,\n",
       " (16, '.'): 0.0075870815293274,\n",
       " (16, '01/09/2020'): 0.03801910733636995,\n",
       " (16, '01/16/2020'): 0.03801910733636995,\n",
       " (16, '12/17/2019'): 0.03801910733636995,\n",
       " (16, '12/23/2020'): 0.03801910733636995,\n",
       " (16, '12/26/2020'): 0.03801910733636995,\n",
       " (16, '2.3.0.'): 0.03801910733636995,\n",
       " (16, '2.3.2'): 0.03801910733636995,\n",
       " (16, '2.3.2.'): 0.03801910733636995,\n",
       " (16, ':'): 0.016314457949285478,\n",
       " (16, '?'): 0.04220068351316953,\n",
       " (16, 'affect'): 0.03236299170751714,\n",
       " (16, 'already'): 0.022942891068658818,\n",
       " (16, 'and'): 0.01786450319346856,\n",
       " (16, 'apps'): 0.03236299170751714,\n",
       " (16, 'are'): 0.009685424940209197,\n",
       " (16, 'as'): 0.009561343586375323,\n",
       " (16, 'azure'): 0.013453171201172667,\n",
       " (16, 'back'): 0.019374509273750434,\n",
       " (16, 'between'): 0.019526932808258898,\n",
       " (16, 'but'): 0.015813476098169674,\n",
       " (16, 'change'): 0.05076608712738087,\n",
       " (16, 'changed'): 0.02445599266145502,\n",
       " (16, 'checking'): 0.031237549998912487,\n",
       " (16, 'cluster'): 0.009506464921918915,\n",
       " (16, 'clusters'): 0.03384405808492058,\n",
       " (16, 'could'): 0.019683215573594023,\n",
       " (16, 'created'): 0.041060272153527366,\n",
       " (16, 'daily'): 0.07103247259733048,\n",
       " (16, 'deployed'): 0.04345725998613178,\n",
       " (16, 'expected'): 0.028084305407764396,\n",
       " (16, 'for'): 0.007130549892262184,\n",
       " (16, 'found'): 0.016722887917656453,\n",
       " (16, 'have'): 0.010892877986540086,\n",
       " (16, 'hdi'): 0.04397973619580873,\n",
       " (16, 'hdi3.6'): 0.11405732200910985,\n",
       " (16, 'hdinsight'): 0.03524967923469833,\n",
       " (16, 'history'): 0.02518304843711171,\n",
       " (16, 'if'): 0.012542969157079267,\n",
       " (16, 'in'): 0.019239457653122804,\n",
       " (16, 'is'): 0.01640918203570715,\n",
       " (16, 'made'): 0.02480882237651643,\n",
       " (16, 'new'): 0.02837488490680663,\n",
       " (16, 'on'): 0.03682627461364514,\n",
       " (16, 'one'): 0.01692202904246029,\n",
       " (16, 'other'): 0.01215704102756486,\n",
       " (16, 'our'): 0.07309684619608028,\n",
       " (16, 'production'): 0.04824448876401775,\n",
       " (16, 'recent'): 0.030286002592571384,\n",
       " (16, 'recently'): 0.03374042103661721,\n",
       " (16, 'release'): 0.03374042103661721,\n",
       " (16, 'released'): 0.03236299170751714,\n",
       " (16, 'releases'): 0.03551623629866524,\n",
       " (16, 'same'): 0.01744994369331314,\n",
       " (16, 'since'): 0.018799877032602212,\n",
       " (16, 'spark'): 0.10321326588909245,\n",
       " (16, 'spark2.3'): 0.0760382146727399,\n",
       " (16, 'staging'): 0.07103247259733048,\n",
       " (16, 'subscriptioin'): 0.03801910733636995,\n",
       " (16, 'subscription'): 0.045885782137317636,\n",
       " (16, 'support'): 0.018027265039058965,\n",
       " (16, 'that'): 0.01623343582757212,\n",
       " (16, 'the'): 0.0032168992227558034,\n",
       " (16, 'there'): 0.015020999760835062,\n",
       " (16, 'these'): 0.04260549614061386,\n",
       " (16, 'this'): 0.0081900594439512,\n",
       " (16, 'to'): 0.00303200472658843,\n",
       " (16, 'two'): 0.09401778102045569,\n",
       " (16, 'version'): 0.07465681907562427,\n",
       " (16, 'versionissue'): 0.03801910733636995,\n",
       " (16, 'was'): 0.031249196206080816,\n",
       " (16, 'we'): 0.04842712470104599,\n",
       " (16, 'were'): 0.015404529273841273,\n",
       " (16, 'what'): 0.02480882237651643,\n",
       " (16, 'when'): 0.014947072655608422,\n",
       " (16, 'wondered'): 0.03801910733636995,\n",
       " (17, '&'): 0.1102663020827978,\n",
       " (17, ':'): 0.05683746640396231,\n",
       " (17, 'analyze'): 0.1856009767865732,\n",
       " (17, 'and'): 0.02333910901082183,\n",
       " (17, 'failing'): 0.09824451868650189,\n",
       " (17, 'fd'): 0.06795460063607452,\n",
       " (17, 'for'): 0.037262873630531414,\n",
       " (17, 'from'): 0.06556811340767789,\n",
       " (17, 'hive'): 0.0927952565768316,\n",
       " (17, 'insght_mtrc'): 0.19868049640296556,\n",
       " (17, 'insght_prdctn.fd'): 0.09934024820148278,\n",
       " (17, 'insght_prdctn.interactive'): 0.09934024820148278,\n",
       " (17, 'kpphv806llapfdqausc01'): 0.19868049640296556,\n",
       " (17, 'qa'): 0.13160173699393862,\n",
       " (17, 'query'): 0.075175935225898,\n",
       " (17, 'querying'): 0.1856009767865732,\n",
       " (17, 'table'): 0.11594019454936762,\n",
       " (17, 'view'): 0.11852221737754622,\n",
       " (18, 'nan'): 0.028015122647842554,\n",
       " (19, ','): 0.09865028388342077,\n",
       " (19, 'built'): 0.2976754764602249,\n",
       " (19, 'containercustom'): 0.3622997287348196,\n",
       " (19, 'filesmapreduce'): 0.3622997287348196,\n",
       " (19, 'issues'): 0.1467961024918992,\n",
       " (19, 'listing'): 0.3622997287348196,\n",
       " (19, 'of'): 0.1370902603190657,\n",
       " (19, 'oozie'): 0.21863225606604286,\n",
       " (19, 'or'): 0.10640102659603716,\n",
       " (19, 'pig'): 0.2330512241856302,\n",
       " (19, 'slow'): 0.2738245877479799,\n",
       " (19, 'sqoop'): 0.23997963804777042,\n",
       " (20, '%'): 0.08216774255689817,\n",
       " (20, '('): 0.017582906682948967,\n",
       " (20, ')'): 0.017364464555946625,\n",
       " (20, ','): 0.03992987680995603,\n",
       " (20, '.'): 0.004877409554567614,\n",
       " (20, '100'): 0.10207840979977004,\n",
       " (20, '4'): 0.04784558601185311,\n",
       " (20, 'a'): 0.012573628789125074,\n",
       " (20, 'already'): 0.044247004203842005,\n",
       " (20, 'amount'): 0.06024384642647409,\n",
       " (20, 'at'): 0.02882649726438767,\n",
       " (20, 'capacity.notebooks'): 0.07332256414871348,\n",
       " (20, 'client'): 0.03765908470164216,\n",
       " (20, 'cluster'): 0.009166948317564668,\n",
       " (20, 'excessive'): 0.06849559857599724,\n",
       " (20, 'hdinsight'): 0.022660508079448926,\n",
       " (20, 'in'): 0.009276167082755638,\n",
       " (20, 'is'): 0.010548759880097453,\n",
       " (20, 'it'): 0.01820514987992226,\n",
       " (20, 'its'): 0.04477619185349479,\n",
       " (20, 'jupyter'): 0.15311761469965504,\n",
       " (20, 'launch'): 0.056819059849667175,\n",
       " (20, 'memory'): 0.1297630980065599,\n",
       " (20, 'nodeswhen'): 0.07332256414871348,\n",
       " (20, 'notebook'): 0.055416880853757844,\n",
       " (20, 'notebooks'): 0.06241434115021163,\n",
       " (20, 'of'): 0.0277444574455252,\n",
       " (20, 'one'): 0.032635341724744846,\n",
       " (20, 'resources'): 0.04533000156343399,\n",
       " (20, 'spark'): 0.024881769455406214,\n",
       " (20, 'the'): 0.015510049824001195,\n",
       " (20, 'they'): 0.030661624405188367,\n",
       " (20, 'trigger'): 0.058408719285673386,\n",
       " (20, 'usage'): 0.048567307700144016,\n",
       " (20, 'uses'): 0.05199209427695093,\n",
       " (20, 'when'): 0.02882649726438767,\n",
       " (20, 'with'): 0.010011611841044245,\n",
       " (20, 'yarn'): 0.0990966116123628,\n",
       " (21, 'nan'): 0.23034656399337208,\n",
       " (22, '-'): 0.014837549668715564,\n",
       " (22, 'ahd501dj'): 0.11742102613028099,\n",
       " (22, 'cluster'): 0.007857384272198287,\n",
       " (22, 'enterprise'): 0.0320272500239548,\n",
       " (22, 'error120022721001340'): 0.0628479121274687,\n",
       " (22, 'errorranger'): 0.0628479121274687,\n",
       " (22, 'fails'): 0.07975109367113138,\n",
       " (22, 'in'): 0.007951000356647689,\n",
       " (22, 'package'): 0.031781356640495,\n",
       " (22, 'permissionmismatch'): 0.1885437363824061,\n",
       " (22, 'pyspark'): 0.1604940201005442,\n",
       " (22, 'security'): 0.02960202908643317,\n",
       " (22, 'start'): 0.06000448089028025,\n",
       " (22, 'to'): 0.010024178891986238,\n",
       " (22, 'with'): 0.025744144734113773,\n",
       " (23, 'errors.hive'): 0.23688828417276664,\n",
       " (23, 'errorsout'): 0.23688828417276664,\n",
       " (23, 'memory'): 0.2794897495525906,\n",
       " (23, 'of'): 0.08963593943938912,\n",
       " (23, 'out'): 0.11713769535698303,\n",
       " (24, '-'): 0.06609453943336933,\n",
       " (24, '['): 0.2586281258017643,\n",
       " (24, ']'): 0.26029622931705493,\n",
       " (24, 'azure'): 0.1981285213263611,\n",
       " (24, 'consent'): 0.5599177625901757,\n",
       " (24, 'fairfax120022724006081'): 0.27995888129508784,\n",
       " (24, 'fairfaxkafka'): 0.27995888129508784,\n",
       " (24, 'for'): 0.10501355295877035,\n",
       " (24, 'government'): 0.3505918450452625,\n",
       " (24, 'test'): 0.60741099494366,\n",
       " (25, 'nan'): 0.015587361473235705,\n",
       " (26, ':'): 0.013837393653320667,\n",
       " (26, 'down.interactive'): 0.01506185937273238,\n",
       " (26, 'down.prodsup'): 0.016123286357308725,\n",
       " (26, 'hive'): 0.010040673486498359,\n",
       " (26, 'kp10tntncapllapnsprdsup01'): 0.024371821946155284,\n",
       " (26, 'prodsup'): 0.014308764942230337,\n",
       " (26, 'query'): 0.00610066490053099,\n",
       " (26, 'service'): 0.020863143085761322,\n",
       " (27, '03/02/2020'): 0.23688828417276664,\n",
       " (27, '['): 0.21883918337072364,\n",
       " (27, ']'): 0.22025065557596954,\n",
       " (27, 'after'): 0.08604315585425457,\n",
       " (27, 'am'): 0.1620455861321777,\n",
       " (27, 'ambari.intermitient'): 0.23688828417276664,\n",
       " (27, 'connect'): 0.11084425598075986,\n",
       " (27, 'connection'): 0.11012532778798477,\n",
       " (27, 'even'): 0.14832731905761104,\n",
       " (27, 'failuresinteractive'): 0.23688828417276664,\n",
       " (27, 'hdi'): 0.0913425290220643,\n",
       " (27, 'hive2interatcive'): 0.23688828417276664,\n",
       " (27, 'i'): 0.10069754094056227,\n",
       " (27, 'query'): 0.089632845846263,\n",
       " (27, 'restating'): 0.23688828417276664,\n",
       " (27, 'services'): 0.09549210448403296,\n",
       " (27, 'the'): 0.020043756695632315,\n",
       " (27, 'to'): 0.009445860878987033,\n",
       " (27, 'unable'): 0.08346582084337148,\n",
       " (27, 'using'): 0.07723290317666488,\n",
       " (28, '0spark'): 0.3241629151837859,\n",
       " (28, '0spark2'): 0.3241629151837859,\n",
       " (28, 'connection'): 0.30139563394606356,\n",
       " (28, 'headnode'): 0.34407617164165977,\n",
       " (28, 'lost'): 0.4113477463037303,\n",
       " (28, 'server'): 0.2654220240805839,\n",
       " (28, 'spark2'): 0.47891131326924546,\n",
       " (28, 'thrift'): 0.7183669699038682,\n",
       " (28, 'to'): 0.025851829774069773,\n",
       " (29, '-'): 0.015146665286813805,\n",
       " (29, '18.0.4'): 0.12831448726024858,\n",
       " (29, 'failure'): 0.01990208485877681,\n",
       " (29, 'for'): 0.024065605886384872,\n",
       " (29, 'hdinsight'): 0.09913972284758907,\n",
       " (29, 'nodescreate'): 0.05993364875399759,\n",
       " (29, 'nodesubuntu'): 0.06415724363012429,\n",
       " (29, 'other'): 0.0205150067340157,\n",
       " (29, 'ubuntu'): 0.11986729750799519,\n",
       " (30, '.'): 0.019096298425510493,\n",
       " (30, '7'): 0.020789544152527817,\n",
       " (30, ':'): 0.029863753534285283,\n",
       " (30, 'a'): 0.00895071879903819,\n",
       " (30, 'am'): 0.01785247982812127,\n",
       " (30, 'and'): 0.006131460841826075,\n",
       " (30, 'any'): 0.011412540193611044,\n",
       " (30, 'beelinehive'): 0.02609786181564378,\n",
       " (30, 'execute'): 0.03405956970335307,\n",
       " (30, 'from'): 0.011483680879310818,\n",
       " (30, 'hive'): 0.008126138287801638,\n",
       " (30, 'is'): 0.007509286694306662,\n",
       " (30, 'kp10tntncapllapnsprdsup01'): 0.019724652507269742,\n",
       " (30, 'let'): 0.018505660674846942,\n",
       " (30, 'morning'): 0.02609786181564378,\n",
       " (30, 'ncap'): 0.02609786181564378,\n",
       " (30, 'not'): 0.008187614725438327,\n",
       " (30, 'please.unable'): 0.02609786181564378,\n",
       " (30, 'prdsup'): 0.01593728862582018,\n",
       " (30, 'queries'): 0.04345239870211948,\n",
       " (30, 'query'): 0.009874805050859484,\n",
       " (30, 'restart'): 0.02072477391345445,\n",
       " (30, 'run'): 0.02818873195960478,\n",
       " (30, 'since'): 0.012905000335938806,\n",
       " (30, 'single'): 0.02022373316683069,\n",
       " (30, 'successfully'): 0.012811869375266032,\n",
       " (30, 'there'): 0.010311025259556272,\n",
       " (30, 'to'): 0.009365811210521041,\n",
       " (30, 'today'): 0.02609786181564378,\n",
       " (30, 'us'): 0.015568596350440393,\n",
       " (30, 'view'): 0.015568596350440393,\n",
       " (31, 'nan'): 0.048212071533496485,\n",
       " (32, 'all'): 0.022786684325440267,\n",
       " (32, 'and'): 0.021814443598054577,\n",
       " (32, 'are'): 0.02365385186402849,\n",
       " (32, 'clusters'): 0.013775722135068176,\n",
       " (32, 'clustershive'): 0.030950228082874034,\n",
       " (32, 'down'): 0.025341801294126106,\n",
       " (32, 'from'): 0.013618837625715342,\n",
       " (32, 'hive'): 0.019274056642423987,\n",
       " (32, 'mapreduce'): 0.047967849521327065,\n",
       " (32, 'services'): 0.024952710719445798,\n",
       " (32, 'the'): 0.011784520268286336,\n",
       " (33, '-'): 0.031610431902915764,\n",
       " (33, 'accesscreate'): 0.12507891913877758,\n",
       " (33, 'clustercould'): 0.12507891913877758,\n",
       " (33, 'deploy'): 0.06931915049830939,\n",
       " (33, 'deployment'): 0.06876876336821612,\n",
       " (33, 'failed'): 0.05212815810431241,\n",
       " (33, 'failure'): 0.04153478579222987,\n",
       " (33, 'for'): 0.050223873154194514,\n",
       " (33, 'hdinsight'): 0.08276011646407434,\n",
       " (33, 'internet'): 0.10665940043470792,\n",
       " (33, 'kafka'): 0.07361971013591634,\n",
       " (33, 'not'): 0.02100301168699397,\n",
       " (33, 'other'): 0.042813927097076244,\n",
       " (33, 'without'): 0.07898623356921038,\n",
       " (34, '502hive'): 0.17597415395691235,\n",
       " (34, ':'): 0.050341755957795185,\n",
       " (34, 'code'): 0.09766443898762092,\n",
       " (34, 'connnection'): 0.17597415395691235,\n",
       " (34, 'getting'): 0.09184732598072835,\n",
       " (34, 'hive'): 0.10958677919549636,\n",
       " (34, 'http'): 0.11165153113958393,\n",
       " (34, 'is'): 0.025317023712233887,\n",
       " (34, 'issueunable'): 0.17597415395691235,\n",
       " (34, 'jdbc'): 0.0986012910682778,\n",
       " (34, 'org.apache.thrift.transport.ttransportexception'): 0.16438943658239338,\n",
       " (34, 'party'): 0.1561699487980568,\n",
       " (34, 'query'): 0.1331687995430193,\n",
       " (34, 'response'): 0.1363657436392012,\n",
       " (34, 'run'): 0.0633575308806355,\n",
       " (34, 'third'): 0.1561699487980568,\n",
       " (34, 'through'): 0.088987798593386,\n",
       " (34, 'to'): 0.014033850448780735,\n",
       " (34, 'tooluser'): 0.17597415395691235,\n",
       " (34, 'with'): 0.02402786841850619,\n",
       " (35, '.'): 0.017813147938420853,\n",
       " (35, ':'): 0.038303509967887645,\n",
       " (35, 'a'): 0.02296053952796753,\n",
       " (35, 'ambari-agent'): 0.09320202633892047,\n",
       " (35, 'and'): 0.010485686657035895,\n",
       " (35, 'are'): 0.034109540006823695,\n",
       " (35, 'bad'): 0.09009113843368845,\n",
       " (35, 'but'): 0.05569093756311928,\n",
       " (35, 'cluster'): 0.016739644753813742,\n",
       " (35, 'cpu.node'): 0.13389337801069418,\n",
       " (35, 'failed'): 0.05212815810431241,\n",
       " (35, 'fine'): 0.07430989922971157,\n",
       " (35, 'for'): 0.025111936577097257,\n",
       " (35, 'head'): 0.07167161972961879,\n",
       " (35, 'headnodes'): 0.09009113843368845,\n",
       " (35, 'hn0'): 0.06620826259307735,\n",
       " (35, 'hn1'): 0.07575971171881314,\n",
       " (35, 'in'): 0.03387817543267276,\n",
       " (35, 'kafka'): 0.07361971013591634,\n",
       " (35, 'more'): 0.06669680293768733,\n",
       " (35, 'nodes'): 0.04525125966491769,\n",
       " (35, 'or'): 0.039322118524622425,\n",
       " (35, 'running'): 0.050899631794239744,\n",
       " (35, 'sluggish'): 0.06823196744233849,\n",
       " (35, 'state'): 0.06669680293768733,\n",
       " (35, 'state.issue'): 0.12507891913877758,\n",
       " (35, 'the'): 0.016993619807166526,\n",
       " (35, 'unresponsive'): 0.13339360587537466,\n",
       " (35, 'utilizing'): 0.12507891913877758,\n",
       " (35, 'was'): 0.11005151707358897,\n",
       " (36, '-'): 0.011726450544630042,\n",
       " (36, 'cluster'): 0.006209868215124453,\n",
       " (36, 'does'): 0.05023504759304048,\n",
       " (36, 'error'): 0.032644013415315754,\n",
       " (36, 'failure'): 0.01540806569711753,\n",
       " (36, 'messagecluster'): 0.04967012410074139,\n",
       " (36, 'messagecreate'): 0.04967012410074139,\n",
       " (36, \"n't\"): 0.04369528778484428,\n",
       " (36, 'other'): 0.015882585858592802,\n",
       " (36, 'start..generic'): 0.09934024820148278,\n",
       " (37, '-'): 0.02019555371575174,\n",
       " (37, '502.3'): 0.07281673134191358,\n",
       " (37, 'access'): 0.10139174838563667,\n",
       " (37, 'application'): 0.040027092437496616,\n",
       " (37, 'bad'): 0.05755822733263428,\n",
       " (37, 'checking'): 0.0702844874975531,\n",
       " (37, 'error'): 0.028110122663188568,\n",
       " (37, 'gateway'): 0.04619280617271829,\n",
       " (37, 'http'): 0.05427504985951997,\n",
       " (37, 'logcan'): 0.0855429915068324,\n",
       " (37, 'logsspark'): 0.07591594733238873,\n",
       " (37, 'to'): 0.010233015952235951,\n",
       " (37, 'when'): 0.03363091347511895,\n",
       " (38, '-'): 0.017732681311391774,\n",
       " (38, '.'): 0.014989112289646814,\n",
       " (38, 'amabari'): 0.07511091937185284,\n",
       " (38, 'ambari'): 0.0442753650399573,\n",
       " (38, 'available'): 0.08417183383877373,\n",
       " (38, 'azure'): 0.07973464882646239,\n",
       " (38, 'but'): 0.0624825153147192,\n",
       " (38, 'edge'): 0.25011624618780964,\n",
       " (38, 'from'): 0.06610118750042325,\n",
       " (38, 'in'): 0.01900483012076765,\n",
       " (38, 'nodes'): 0.050769705965517414,\n",
       " (38, 'or'): 0.022058749416251605,\n",
       " (38, 'please'): 0.06494116651281592,\n",
       " (38, 'remove.120030224005315'): 0.07511091937185284,\n",
       " (38, 'remove.node'): 0.07511091937185284,\n",
       " (38, 'removed'): 0.09287122271532819,\n",
       " (38, 'sluggish'): 0.03827646954082403,\n",
       " (38, 'still'): 0.12505812309390482,\n",
       " (38, 'unresponsive'): 0.03741527969675143,\n",
       " (38, 'were'): 0.06086667664298259,\n",
       " (39, '('): 0.009846427742451423,\n",
       " (39, ')'): 0.009724100151330113,\n",
       " (39, ','): 0.02236073101357538,\n",
       " (39, '-'): 0.009693865783560835,\n",
       " (39, 'a'): 0.03520616060955021,\n",
       " (39, 'and'): 0.016078052874121708,\n",
       " (39, 'application'): 0.019213004369998378,\n",
       " (39, 'azure'): 0.014529424897266482,\n",
       " (39, 'client_id'): 0.041060635923279555,\n",
       " (39, 'cluster'): 0.01026698211567243,\n",
       " (39, 'customer'): 0.031791047602939854,\n",
       " (39, 'for'): 0.02310298165092948,\n",
       " (39, 'hard-coding'): 0.041060635923279555,\n",
       " (39, 'in'): 0.02077861426537263,\n",
       " (39, 'key'): 0.05358705633327549,\n",
       " (39, 'notebook'): 0.031033453278104396,\n",
       " (39, 'options'): 0.08734671838527756,\n",
       " (39, 'pyspark'): 0.06990406208823703,\n",
       " (39, 'required'): 0.02370937135365033,\n",
       " (39, 'secret'): 0.041060635923279555,\n",
       " (39, 'standard'): 0.02278836909711155,\n",
       " (39, 'tenant'): 0.03373655399882549,\n",
       " (39, 'the'): 0.013897004642305073,\n",
       " (39, 'using'): 0.026774073101243825,\n",
       " (39, 'vault'): 0.03643965471954659,\n",
       " (39, 'vault120030224006133'): 0.041060635923279555,\n",
       " (39, 'wanted'): 0.028581954743935614,\n",
       " (39, 'without'): 0.02422244496122452,\n",
       " (40, 'failedscaling'): 0.7698869235614916,\n",
       " (40, 'failissue'): 0.7698869235614916,\n",
       " (40, 'scale'): 0.36262485630880636,\n",
       " (40, 'scaling'): 0.34683814064326984,\n",
       " (40, 'up'): 0.5312136875848844,\n",
       " (40, 'with'): 0.10512192433096458,\n",
       " (41, '('): 0.08205356452042852,\n",
       " (41, ')'): 0.08103416792775092,\n",
       " (41, '1wn'): 0.3421719660273296,\n",
       " (41, 'ambari'): 0.10084944259101383,\n",
       " (41, 'and'): 0.026796754790202843,\n",
       " (41, 'collector'): 0.2474637386807504,\n",
       " (41, 'lost'): 0.21710019943807987,\n",
       " (41, 'metric'): 0.2811379499902124,\n",
       " (41, 'metrics'): 0.21710019943807987,\n",
       " (41, 'start'): 0.16334553131242957,\n",
       " (41, 'starthadoop'): 0.31964612668798714,\n",
       " (41, 'to'): 0.013644021269647935,\n",
       " (41, 'unable'): 0.2411234824364065,\n",
       " (41, 'wn71-hdi01p'): 0.3421719660273296,\n",
       " (42, 'nan'): 0.029198860224511958,\n",
       " (43, 'nan'): 0.02205445825468456,\n",
       " (44, ':'): 0.04517849893648286,\n",
       " (44, 'a'): 0.027081662007346314,\n",
       " (44, 'able'): 0.05049847811450019,\n",
       " (44, 'and'): 0.02473546596018724,\n",
       " (44, 'de'): 0.14015251815210225,\n",
       " (44, 'execute'): 0.20610406282029034,\n",
       " (44, 'from'): 0.06949099198762443,\n",
       " (44, 'gc'): 0.23871887137003378,\n",
       " (44, 'hivemetastore'): 0.2803050363042045,\n",
       " (44, 'in'): 0.019979436793627525,\n",
       " (44, 'kp08tntncapsparknsprdsup01'): 0.13443088863122507,\n",
       " (44, 'logsspark'): 0.14015251815210225,\n",
       " (44, 'lot'): 0.12580339538452728,\n",
       " (44, 'not'): 0.024772783015428786,\n",
       " (44, 'of'): 0.02987864647979637,\n",
       " (44, 'pauses'): 0.23871887137003378,\n",
       " (44, 'prdsup'): 0.09644102860752723,\n",
       " (44, 'queries'): 0.08764757345042903,\n",
       " (44, 'query'): 0.05975523056417533,\n",
       " (44, 'spark'): 0.05359150344241339,\n",
       " (44, 'to'): 0.02518896234396542,\n",
       " (44, 'wfunable'): 0.1579255227818444,\n",
       " (45, '-'): 0.030937869522002667,\n",
       " (45,\n",
       "  '//hn0-perfwu.z5pipfv3tdyuxorx05ypbu513e.xx.internal.cloudapp.net:8088'): 0.2620891654677418,\n",
       " (45, '120030321002006'): 0.1310445827338709,\n",
       " (45, ':'): 0.07497708334139709,\n",
       " (45, 'are'): 0.06676761022612297,\n",
       " (45, 'connection'): 0.12184078819096186,\n",
       " (45, 'due'): 0.11879998121299824,\n",
       " (45, 'failed'): 0.10203809671482428,\n",
       " (45, 'hdinsight'): 0.040499631461142764,\n",
       " (45, 'http'): 0.1662895144632101,\n",
       " (45, 'managers'): 0.10766985318774092,\n",
       " (45, 'manangers'): 0.1310445827338709,\n",
       " (45, 'or'): 0.03848547770494961,\n",
       " (45, 'resource'): 0.13356044690840727,\n",
       " (45, 'servicenode'): 0.12241766554008018,\n",
       " (45, 'sluggish'): 0.06678022345420363,\n",
       " (45, 'stopping'): 0.24483533108016037,\n",
       " (45, 'to'): 0.02612684923975137,\n",
       " (45, 'unresponsive'): 0.06527772202411951,\n",
       " (46, 'able'): 0.03580801175391832,\n",
       " (46, 'apihadoop'): 0.055991776259017564,\n",
       " (46, 'apinot'): 0.055991776259017564,\n",
       " (46, 'call'): 0.07105097436155342,\n",
       " (46, 'not'): 0.017566155229122228,\n",
       " (46, 'oozie'): 0.13515448556809923,\n",
       " (46, 'rest'): 0.11490505198463509,\n",
       " (46, 'to'): 0.011163290129711949,\n",
       " (47, '&'): 0.18990307580926288,\n",
       " (47, 'accepted'): 0.27257402333314246,\n",
       " (47, 'application'): 0.08005418487499323,\n",
       " (47, 'coresapplication'): 0.1710859830136648,\n",
       " (47, 'coresspark'): 0.1710859830136648,\n",
       " (47, 'free'): 0.2586121106508699,\n",
       " (47, 'in'): 0.043288779719526306,\n",
       " (47, 'memory'): 0.2018537080102043,\n",
       " (47, 'of'): 0.06473706737289213,\n",
       " (47, 'plenty'): 0.31964612668798714,\n",
       " (47, 'statue'): 0.31964612668798714,\n",
       " (47, 'stuck'): 0.21425057197210484,\n",
       " (47, 'with'): 0.046720855258206476,\n",
       " (48, '-'): 0.04276705492747428,\n",
       " (48, ':'): 0.025911197919453406,\n",
       " (48, 'application'): 0.08476325457352225,\n",
       " (48, 'downgc'): 0.1811498643674098,\n",
       " (48, 'hive'): 0.1128099197600698,\n",
       " (48, 'in'): 0.022917589263278634,\n",
       " (48, 'is'): 0.026061642056711357,\n",
       " (48, 'llap'): 0.10150132904087421,\n",
       " (48, 'logsinteractive'): 0.16922442001128732,\n",
       " (48, 'pauses'): 0.13691229387398995,\n",
       " (48, 'prodsup'): 0.16076318258623495,\n",
       " (48, 'query'): 0.0685427644706717,\n",
       " (48, 'services'): 0.07302337401720167,\n",
       " (49, \"'azure.storage'no\"): 0.08674782237312581,\n",
       " (49, 'azure.storagenotebooks'): 0.08674782237312581,\n",
       " (49, 'module'): 0.14768463821458527,\n",
       " (49, 'named'): 0.14768463821458527,\n",
       " (49, 'no'): 0.036472713971979764,\n",
       " (50, '('): 0.04923213871225711,\n",
       " (50, ')'): 0.04862050075665056,\n",
       " (50, ':'): 0.02936602430871386,\n",
       " (50, 'app'): 0.12855034318326292,\n",
       " (50, 'application'): 0.09606502184999188,\n",
       " (50, 'down.interactive'): 0.09589383800639614,\n",
       " (50, 'down.qa'): 0.10265158980819887,\n",
       " (50, 'is'): 0.02953652766427287,\n",
       " (50, 'llap'): 0.2875870989491436,\n",
       " (50, 'qa'): 0.06799423078020161,\n",
       " (50, 'query'): 0.03884089986671396,\n",
       " (50, 'slider'): 0.20530317961639774,\n",
       " (51, 'nan'): 0.2073119075940349,\n",
       " (52, '.'): 0.040970240258367964,\n",
       " (52, 'between'): 0.1581681557468971,\n",
       " (52, 'connectivity'): 0.15572864753842552,\n",
       " (52, 'head'): 0.16484472537812322,\n",
       " (52, 'lost'): 0.1953901794942719,\n",
       " (52, 'network'): 0.16073282046627463,\n",
       " (52, 'nodes'): 0.2081557944586214,\n",
       " (52, 'not'): 0.09661385376017227,\n",
       " (52, 'reachable'): 0.28768151401918846,\n",
       " (52, 'reachable.hn1'): 0.30795476942459665,\n",
       " (52, 'ssh'): 0.13873525625730795,\n",
       " (52, 'via'): 0.18370943693519665,\n",
       " (53, 'can'): 0.006896048502922023,\n",
       " (53, 'managercannot'): 0.02415331524898797,\n",
       " (53, 'managerhadoop'): 0.02415331524898797,\n",
       " (53, 'not'): 0.01515511431532114,\n",
       " (53, 'resource'): 0.024617023547824083,\n",
       " (53, 'start'): 0.02306054559704888,\n",
       " (54, '-'): 0.19387731567121672,\n",
       " (54, '502.3'): 0.6990406208823703,\n",
       " (54, 'bad'): 0.5525589823932892,\n",
       " (54, 'errorslegetting'): 0.4106063592327955,\n",
       " (54, 'errorsspark'): 0.38357535202558457,\n",
       " (54, 'gateway'): 0.4434509392580956,\n",
       " (54, 'getting'): 0.21431042728836616,\n",
       " (55, '.'): 0.054626987011157285,\n",
       " (55, 'downhive'): 0.32708882799977096,\n",
       " (55, 'for'): 0.07700993883643159,\n",
       " (55, 'hive'): 0.25570248478949154,\n",
       " (55, 'reason'): 0.3033104984038555,\n",
       " (55, 'service'): 0.13282867764601375,\n",
       " (55, 'services'): 0.1655196477723238,\n",
       " (55, 'unavailable'): 0.3373655399882549,\n",
       " (56, 'because'): 0.027704995861869447,\n",
       " (56, 'can'): 0.014777246791975764,\n",
       " (56, 'convert'): 0.04405718198838468,\n",
       " (56, 'does'): 0.026172881939231174,\n",
       " (56, 'existcannot'): 0.05175710410497422,\n",
       " (56, 'jupyter'): 0.07205534809395532,\n",
       " (56, \"n't\"): 0.02276561212319618,\n",
       " (56, 'not'): 0.00811881124035061,\n",
       " (56, 'notebook'): 0.03911779824971142,\n",
       " (56, 'notebook.spark'): 0.05175710410497422,\n",
       " (56, 'open'): 0.03283868562928939,\n",
       " (56, 'that'): 0.011049649596918837,\n",
       " (56, 'to'): 0.010319007682927012,\n",
       " (56, 'v5'): 0.05175710410497422,\n",
       " (56, 'version'): 0.025408413214813304,\n",
       " (57, ':'): 0.08390292659632531,\n",
       " (57, 'are'): 0.07471613525304238,\n",
       " (57, 'failinghive'): 0.5479647886079779,\n",
       " (57, 'hive'): 0.09132231599624697,\n",
       " (57, 'kpph10llapprdsupusc01'): 0.27398239430398896,\n",
       " (57, 'prdsup'): 0.17910476741397915,\n",
       " (57, 'queries'): 0.1627740649793682,\n",
       " (57, 'query'): 0.11097399961918275,\n",
       " (58, ':'): 0.015731798736810996,\n",
       " (58, 'clusterspark'): 0.08124388350103272,\n",
       " (58, 'clustertoken'): 0.10998384622307023,\n",
       " (58, 'hdi'): 0.0848180626633454,\n",
       " (58, 'issue'): 0.07646210097698976,\n",
       " (58, 'kpq044sparkespfdqawus201'): 0.10998384622307023,\n",
       " (58, 'on'): 0.08877762630075169,\n",
       " (58, 'refresh'): 0.195212435997571,\n",
       " (58, 'secure'): 0.1480068702839167,\n",
       " (58, 'spark'): 0.07464530836621865,\n",
       " (58, 'the'): 0.018612059788801434,\n",
       " (58, 'token'): 0.08124388350103272,\n",
       " (59, ','): 0.02842465806810429,\n",
       " (59, '-'): 0.02464542148362924,\n",
       " (59, '...'): 0.06811913940670614,\n",
       " (59,\n",
       "  '//docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-compare-storage-optionsand'): 0.10439144726257513,\n",
       " (59, ':'): 0.014931876767142641,\n",
       " (59, 'above'): 0.05793653160282597,\n",
       " (59, 'account'): 0.1537424325031924,\n",
       " (59, 'also'): 0.0438908930849248,\n",
       " (59, 'and'): 0.016350562244869533,\n",
       " (59, 'as'): 0.026253180694793262,\n",
       " (59, 'blov'): 0.10439144726257513,\n",
       " (59, 'check'): 0.05361632398199901,\n",
       " (59, 'cluster'): 0.02610249690425194,\n",
       " (59, 'create'): 0.03842839136239503,\n",
       " (59, 'failure'): 0.03238305332953515,\n",
       " (59, 'for'): 0.03915759601852454,\n",
       " (59, 'hdi'): 0.08050527981605667,\n",
       " (59, 'hdinsight'): 0.03226241828260525,\n",
       " (59, 'https'): 0.03167179571768715,\n",
       " (59, 'i'): 0.044375187533129135,\n",
       " (59, 'in'): 0.013206746355109721,\n",
       " (59, 'is'): 0.045055720165839974,\n",
       " (59, 'it'): 0.025919196439211357,\n",
       " (59, 'like'): 0.055879567924787536,\n",
       " (59, 'link'): 0.060278062763517784,\n",
       " (59, 'need'): 0.0453873779518007,\n",
       " (59, 'on'): 0.01685270194183761,\n",
       " (59, 'other'): 0.03338034994009335,\n",
       " (59, 'per'): 0.05966166201485888,\n",
       " (59, 'premium'): 0.35544438349951035,\n",
       " (59, 'preview'): 0.09751915729464015,\n",
       " (59, 'see'): 0.0416575154979566,\n",
       " (59, 'storage'): 0.1282123336660448,\n",
       " (59, 'storagelooks'): 0.10439144726257513,\n",
       " (59, 'the'): 0.022082104834171192,\n",
       " (59, 'to'): 0.004162582760231574,\n",
       " (59, 'unsupported'): 0.17772219174975518,\n",
       " (59, 'with'): 0.014253820248266384,\n",
       " (60, 'doubtadvisoryissue'): 0.8798707697845618,\n",
       " (60, 'down'): 0.3602156041093639,\n",
       " (60, 'scaling'): 0.3963864464494512,\n",
       " (60, 'technical'): 0.8219471829119669,\n",
       " (60, 'with'): 0.12013934209253094,\n",
       " (61, ','): 0.021779932805430564,\n",
       " (61, '-'): 0.018884154123819812,\n",
       " (61, '.'): 0.0053208104231646704,\n",
       " (61, '3.6'): 0.05671864830212829,\n",
       " (61, 'and'): 0.012528352888926005,\n",
       " (61, 'are'): 0.020377127796284287,\n",
       " (61, 'as'): 0.020116073519387045,\n",
       " (61, 'bug'): 0.05567913261805639,\n",
       " (61, 'by'): 0.024357257488718233,\n",
       " (61, 'change'): 0.035602190972448926,\n",
       " (61, 'changecustomer'): 0.07998825179859653,\n",
       " (61, 'cluster'): 0.010000307255525094,\n",
       " (61, 'clusters'): 0.035602190972448926,\n",
       " (61, 'customer'): 0.0412870748090128,\n",
       " (61, 'day'): 0.06371860285709824,\n",
       " (61, 'deploying'): 0.05567913261805639,\n",
       " (61, 'deployment'): 0.04108263785633691,\n",
       " (61, 'design'): 0.059086460728023794,\n",
       " (61, 'directory'): 0.04884675474926705,\n",
       " (61, 'each'): 0.1076413602064849,\n",
       " (61, 'failure'): 0.024812988914838623,\n",
       " (61, 'fixes'): 0.06371860285709824,\n",
       " (61, 'for'): 0.04500580841090158,\n",
       " (61, 'hdi'): 0.0616858637551603,\n",
       " (61, 'image'): 0.06808837216386725,\n",
       " (61, 'image.create'): 0.07998825179859653,\n",
       " (61, 'improvements'): 0.06808837216386725,\n",
       " (61, 'into'): 0.03818466965745108,\n",
       " (61, 'is'): 0.011507738051015405,\n",
       " (61, 'latest'): 0.05671864830212829,\n",
       " (61, 'major'): 0.06808837216386725,\n",
       " (61, 'may'): 0.04618708705256558,\n",
       " (61, 'minor'): 0.07472247117381518,\n",
       " (61, 'new'): 0.1492445245098271,\n",
       " (61, 'other'): 0.0511543025055976,\n",
       " (61, 'pull'): 0.0709863403627531,\n",
       " (61, 'pulled'): 0.07472247117381518,\n",
       " (61, 'so'): 0.03671286854956791,\n",
       " (61, 'sqoop'): 0.0529825174910662,\n",
       " (61, 'that'): 0.017076731195238205,\n",
       " (61, 'the'): 0.01692005435345585,\n",
       " (61, 'this'): 0.017231034154806424,\n",
       " (61, 'version'): 0.1963377384781028,\n",
       " (61, 'while'): 0.035602190972448926,\n",
       " (61, 'will'): 0.05810706629118491,\n",
       " (61, 'with'): 0.010921758372048269,\n",
       " (62, '.'): 0.03151556942951382,\n",
       " (62, 'connect'): 0.11084425598075986,\n",
       " (62, 'create'): 0.08720288809158872,\n",
       " (62, 'customization'): 0.17903915352752536,\n",
       " (62, 'external'): 0.1679744584332261,\n",
       " (62, 'fails'): 0.15030013807251685,\n",
       " (62, 'failure'): 0.07348462101702208,\n",
       " (62, 'metastoredeployment'): 0.23688828417276664,\n",
       " (62, 'mysql'): 0.4737765683455333,\n",
       " (62, 'other'): 0.07574771717175029,\n",
       " (62, 'to'): 0.028337582636961102,\n",
       " (62, 'unable'): 0.08346582084337148,\n",
       " (62, 'with'): 0.032345207486450644,\n",
       " (63, 'active'): 0.16343618152978842,\n",
       " (63, 'ap'): 0.30795476942459665,\n",
       " (63, 'error'): 0.10119644158747886,\n",
       " (63, 'flipover'): 0.30795476942459665,\n",
       " (63, 'gateway'): 0.16629410222178587,\n",
       " (63, 'ha'): 0.24531662099982823,\n",
       " (63, 'http'): 0.1953901794942719,\n",
       " (63, 'issuesspark'): 0.28768151401918846,\n",
       " (63, 'leadyarn'): 0.30795476942459665,\n",
       " (63, 'on'): 0.09943094145684189,\n",
       " (63, 'rest'): 0.21065926197183102,\n",
       " (63, 'returns'): 0.28768151401918846,\n",
       " (63, 'yarnui'): 0.27329741039659944,\n",
       " (64, 'ambari'): 0.2866247315744604,\n",
       " (64, 'dbhadoop'): 0.3241629151837859,\n",
       " (64, 'every'): 0.21471862351642615,\n",
       " (64, 'for'): 0.06079732013402494,\n",
       " (64, 'hdi'): 0.12499503971440376,\n",
       " (64, 'is'): 0.04663662262779927,\n",
       " (64, 'issues'): 0.13134388117696244,\n",
       " (64, 'minute'): 0.28768148462799936,\n",
       " (64, 'perf'): 0.2759370871904093,\n",
       " (64, 'start'): 0.1547483980854596,\n",
       " (64, 'to'): 0.012925914887034887,\n",
       " (64, 'trying'): 0.19337835466862802,\n",
       " (64, 'with'): 0.04426186287619561,\n",
       " (65, '('): 0.06564285161634281,\n",
       " (65, ')'): 0.06482733434220074,\n",
       " (65, 'academic/lab'): 0.2737375728218637,\n",
       " (65, 'accessing'): 0.08804157358123808,\n",
       " (65, 'ca'): 0.16923200583682024,\n",
       " (65, 'course'): 0.2737375728218637,\n",
       " (65, 'cse6242'): 0.2737375728218637,\n",
       " (65, 'for'): 0.05133995922428773,\n",
       " (65, 'georgia'): 0.2737375728218637,\n",
       " (65, 'i'): 0.11636160286464972,\n",
       " (65, 'in'): 0.017315511887810525,\n",
       " (65, 'issues'): 0.05545630538582859,\n",
       " (65, 'my'): 0.2858195474393561,\n",
       " (65, \"n't\"): 0.12040479300712646,\n",
       " (65, 'or'): 0.04019594338072515,\n",
       " (65, 'signing'): 0.12785845067519486,\n",
       " (65, 'subscription'): 0.16518881569434352,\n",
       " (65, 'subscriptions'): 0.10902960933325699,\n",
       " (65, 'tech'): 0.2737375728218637,\n",
       " (65, 'use'): 0.09898976692300478,\n",
       " (66, '.'): 0.0069441085183674514,\n",
       " (66, ':'): 0.029863753534285283,\n",
       " (66, 'an'): 0.03987174404612903,\n",
       " (66, 'cluster'): 0.01305124845212597,\n",
       " (66, 'connection'): 0.04852980546589159,\n",
       " (66, 'controller'): 0.09264318996494895,\n",
       " (66, 'debug'): 0.09264318996494895,\n",
       " (66, 'domain'): 0.07402264269938777,\n",
       " ...}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the TF-IDF according to weights¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "for i in tf_idf:\n",
    "    tf_idf[i] *= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tf_idf_title:\n",
    "    tf_idf[i] = tf_idf_title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42484"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: not possible to scale up\n",
      "\n",
      "['not', 'possible', 'to', 'scale', 'up']\n",
      "\n",
      "[40, 496, 760, 789, 928, 198, 145, 574, 738, 508]\n"
     ]
    }
   ],
   "source": [
    "matching_score(12, \"not possible to scale up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subject</th>\n",
       "      <th>DaysToSolution</th>\n",
       "      <th>initCasusePath1</th>\n",
       "      <th>initCasusePath2</th>\n",
       "      <th>initCasusePath3</th>\n",
       "      <th>Symptomstxt</th>\n",
       "      <th>Resolutiontxt</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Body</th>\n",
       "      <th>Resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>scale up failed</td>\n",
       "      <td>scale up failed</td>\n",
       "      <td>0.793977</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>scaling fail</td>\n",
       "      <td>retry</td>\n",
       "      <td>scale up failedscaling failIssue with scaling up</td>\n",
       "      <td>retry</td>\n",
       "      <td>retry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>auto scale in</td>\n",
       "      <td>auto scale in</td>\n",
       "      <td>4.742842</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Manual scale up failed twice in a row and took...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "      <td>auto scale inManual scale up failed twice in a...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>20.863643</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "      <td>LLAP Cluster: Scale up issuesLLAP Cluster: Sca...</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>0.059461</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Symptom: When scaling up this action fails</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>3.486474</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>itleHDI 4.0 : Spark :Scale up operation is fai...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Yarnui not reachable</td>\n",
       "      <td>Yarnui not reachable</td>\n",
       "      <td>0.355837</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Service unhealthy</td>\n",
       "      <td>Hadoop</td>\n",
       "      <td>Yarnui not reachable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarnui not reachableYarnui not reachableHadoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Attempting to scale the cluster to 256 nodes fail</td>\n",
       "      <td>Attempting to scale the cluster to 256 nodes fail</td>\n",
       "      <td>0.108130</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>scale up failed for nodes greater than 100</td>\n",
       "      <td>use ambari db with s4 tierscale up in interval...</td>\n",
       "      <td>Attempting to scale the cluster to 256 nodes f...</td>\n",
       "      <td>use ambari db with s4 tierscale up in interval...</td>\n",
       "      <td>use ambari db with s4 tierscale up in interval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Unable to scale the cluster</td>\n",
       "      <td>Unable to scale the cluster</td>\n",
       "      <td>18.790506</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Unable to scale the cluster.</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "      <td>Unable to scale the clusterUnable to scale the...</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Cluster not Scaling Up, Yarn not Accessible</td>\n",
       "      <td>Cluster not Scaling Up, Yarn not Accessible</td>\n",
       "      <td>0.759414</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Cluster scale up failing for our production jo...</td>\n",
       "      <td>Now able to access the Ambari UI and Yarn UI .</td>\n",
       "      <td>Cluster not Scaling Up, Yarn not AccessibleClu...</td>\n",
       "      <td>Now able to access the Ambari UI and Yarn UI .</td>\n",
       "      <td>Now able to access the Ambari UI and Yarn UI .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "40                                     scale up failed   \n",
       "496                                      auto scale in   \n",
       "760                      LLAP Cluster: Scale up issues   \n",
       "789  Error rescaling cluster Failed to scale the HD...   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...   \n",
       "145                               Yarnui not reachable   \n",
       "574  Attempting to scale the cluster to 256 nodes fail   \n",
       "738                        Unable to scale the cluster   \n",
       "508        Cluster not Scaling Up, Yarn not Accessible   \n",
       "\n",
       "                                               Subject  DaysToSolution  \\\n",
       "40                                     scale up failed        0.793977   \n",
       "496                                      auto scale in        4.742842   \n",
       "760                      LLAP Cluster: Scale up issues       20.863643   \n",
       "789  Error rescaling cluster Failed to scale the HD...        0.059461   \n",
       "928  Scale up cluster failed and cluster is not in ...        0.276127   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...        3.486474   \n",
       "145                               Yarnui not reachable        0.355837   \n",
       "574  Attempting to scale the cluster to 256 nodes fail        0.108130   \n",
       "738                        Unable to scale the cluster       18.790506   \n",
       "508        Cluster not Scaling Up, Yarn not Accessible        0.759414   \n",
       "\n",
       "                initCasusePath1          initCasusePath2  \\\n",
       "40   Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "496  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "760  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "789  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "928  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "198  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "145  Routing Azure HDInsight V5        Service unhealthy   \n",
       "574  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "738  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "508  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "\n",
       "           initCasusePath3                                        Symptomstxt  \\\n",
       "40   Issue with scaling up                                       scaling fail   \n",
       "496  Issue with scaling up  Manual scale up failed twice in a row and took...   \n",
       "760  Issue with scaling up                      LLAP Cluster: Scale up issues   \n",
       "789  Issue with scaling up         Symptom: When scaling up this action fails   \n",
       "928  Issue with scaling up  Scale up cluster failed and cluster is not in ...   \n",
       "198  Issue with scaling up  itleHDI 4.0 : Spark :Scale up operation is fai...   \n",
       "145                 Hadoop                               Yarnui not reachable   \n",
       "574  Issue with scaling up        scale up failed for nodes greater than 100    \n",
       "738  Issue with scaling up                       Unable to scale the cluster.   \n",
       "508  Issue with scaling up  Cluster scale up failing for our production jo...   \n",
       "\n",
       "                                         Resolutiontxt  \\\n",
       "40                                               retry   \n",
       "496  Asked cx to retry manual scale up and it was s...   \n",
       "760  Will track refund under 120080821000073. Pleas...   \n",
       "789  Resolution: Restarted Ambari and scale was suc...   \n",
       "928       System backup and run with scale-up 36 nodes   \n",
       "198  deleted the wnodes manually tested scaling dow...   \n",
       "145                                                NaN   \n",
       "574  use ambari db with s4 tierscale up in interval...   \n",
       "738  We see that cluster scale up failed due to Tim...   \n",
       "508     Now able to access the Ambari UI and Yarn UI .   \n",
       "\n",
       "                                                Titles  \\\n",
       "40    scale up failedscaling failIssue with scaling up   \n",
       "496  auto scale inManual scale up failed twice in a...   \n",
       "760  LLAP Cluster: Scale up issuesLLAP Cluster: Sca...   \n",
       "789  Error rescaling cluster Failed to scale the HD...   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...   \n",
       "145     Yarnui not reachableYarnui not reachableHadoop   \n",
       "574  Attempting to scale the cluster to 256 nodes f...   \n",
       "738  Unable to scale the clusterUnable to scale the...   \n",
       "508  Cluster not Scaling Up, Yarn not AccessibleClu...   \n",
       "\n",
       "                                                  Body  \\\n",
       "40                                               retry   \n",
       "496  Asked cx to retry manual scale up and it was s...   \n",
       "760  Will track refund under 120080821000073. Pleas...   \n",
       "789  Resolution: Restarted Ambari and scale was suc...   \n",
       "928       System backup and run with scale-up 36 nodes   \n",
       "198  deleted the wnodes manually tested scaling dow...   \n",
       "145                                                NaN   \n",
       "574  use ambari db with s4 tierscale up in interval...   \n",
       "738  We see that cluster scale up failed due to Tim...   \n",
       "508     Now able to access the Ambari UI and Yarn UI .   \n",
       "\n",
       "                                            Resolution  \n",
       "40                                               retry  \n",
       "496  Asked cx to retry manual scale up and it was s...  \n",
       "760  Will track refund under 120080821000073. Pleas...  \n",
       "789  Resolution: Restarted Ambari and scale was suc...  \n",
       "928       System backup and run with scale-up 36 nodes  \n",
       "198  deleted the wnodes manually tested scaling dow...  \n",
       "145                                                NaN  \n",
       "574  use ambari db with s4 tierscale up in interval...  \n",
       "738  We see that cluster scale up failed due to Tim...  \n",
       "508     Now able to access the Ambari UI and Yarn UI .  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData.loc[[40, 496, 760, 789, 928, 198, 145, 574, 738, 508],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Cosine Similarity Ranking¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorising tf-idf¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(out)\n",
    "\n",
    "#     for i in out:\n",
    "#         print(i, dataset[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: not possible to scale up\n",
      "\n",
      "['not', 'possible', 'to', 'scale', 'up']\n",
      "\n",
      "[738 760 496 248 147 789 928  40 421 198]\n"
     ]
    }
   ],
   "source": [
    "Q = cosine_similarity(10,\"not possible to scale up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subject</th>\n",
       "      <th>DaysToSolution</th>\n",
       "      <th>initCasusePath1</th>\n",
       "      <th>initCasusePath2</th>\n",
       "      <th>initCasusePath3</th>\n",
       "      <th>Symptomstxt</th>\n",
       "      <th>Resolutiontxt</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Body</th>\n",
       "      <th>Resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Unable to scale the cluster</td>\n",
       "      <td>Unable to scale the cluster</td>\n",
       "      <td>18.790506</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Unable to scale the cluster.</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "      <td>Unable to scale the clusterUnable to scale the...</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "      <td>We see that cluster scale up failed due to Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>20.863643</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>LLAP Cluster: Scale up issues</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "      <td>LLAP Cluster: Scale up issuesLLAP Cluster: Sca...</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "      <td>Will track refund under 120080821000073. Pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>auto scale in</td>\n",
       "      <td>auto scale in</td>\n",
       "      <td>4.742842</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Manual scale up failed twice in a row and took...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "      <td>auto scale inManual scale up failed twice in a...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "      <td>Asked cx to retry manual scale up and it was s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Cluster not sclae up due to RM</td>\n",
       "      <td>Cluster not sclae up due to RM</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Cluster not scale up due to RM</td>\n",
       "      <td>We introduced the ‘node label’ features in the...</td>\n",
       "      <td>Cluster not sclae up due to RMCluster not scal...</td>\n",
       "      <td>We introduced the ‘node label’ features in the...</td>\n",
       "      <td>We introduced the ‘node label’ features in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>HBase cluster getting restarted during region ...</td>\n",
       "      <td>HBase cluster getting restarted during region ...</td>\n",
       "      <td>16.920502</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Complete HBase cluster gets restarted during s...</td>\n",
       "      <td>Microsoft delivered the fix for scale up activ...</td>\n",
       "      <td>HBase cluster getting restarted during region ...</td>\n",
       "      <td>Microsoft delivered the fix for scale up activ...</td>\n",
       "      <td>Microsoft delivered the fix for scale up activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>0.059461</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Symptom: When scaling up this action fails</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "      <td>Error rescaling cluster Failed to scale the HD...</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "      <td>Resolution: Restarted Ambari and scale was suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "      <td>Scale up cluster failed and cluster is not in ...</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "      <td>System backup and run with scale-up 36 nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>scale up failed</td>\n",
       "      <td>scale up failed</td>\n",
       "      <td>0.793977</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>scaling fail</td>\n",
       "      <td>retry</td>\n",
       "      <td>scale up failedscaling failIssue with scaling up</td>\n",
       "      <td>retry</td>\n",
       "      <td>retry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Are there any recommended configuration tuning...</td>\n",
       "      <td>Are there any recommended configuration tuning...</td>\n",
       "      <td>0.444091</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with Autoscaling</td>\n",
       "      <td>Are there any recommended configuration tuning...</td>\n",
       "      <td>we have checked your latest scaling event that...</td>\n",
       "      <td>Are there any recommended configuration tuning...</td>\n",
       "      <td>we have checked your latest scaling event that...</td>\n",
       "      <td>we have checked your latest scaling event that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>3.486474</td>\n",
       "      <td>Routing Azure HDInsight V5</td>\n",
       "      <td>Scale HDInsight cluster</td>\n",
       "      <td>Issue with scaling up</td>\n",
       "      <td>itleHDI 4.0 : Spark :Scale up operation is fai...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "      <td>HDI  4.0 : Spark :Scale up operation is  faili...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "      <td>deleted the wnodes manually tested scaling dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "738                        Unable to scale the cluster   \n",
       "760                      LLAP Cluster: Scale up issues   \n",
       "496                                      auto scale in   \n",
       "248                     Cluster not sclae up due to RM   \n",
       "147  HBase cluster getting restarted during region ...   \n",
       "789  Error rescaling cluster Failed to scale the HD...   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "40                                     scale up failed   \n",
       "421  Are there any recommended configuration tuning...   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...   \n",
       "\n",
       "                                               Subject  DaysToSolution  \\\n",
       "738                        Unable to scale the cluster       18.790506   \n",
       "760                      LLAP Cluster: Scale up issues       20.863643   \n",
       "496                                      auto scale in        4.742842   \n",
       "248                     Cluster not sclae up due to RM        0.011344   \n",
       "147  HBase cluster getting restarted during region ...       16.920502   \n",
       "789  Error rescaling cluster Failed to scale the HD...        0.059461   \n",
       "928  Scale up cluster failed and cluster is not in ...        0.276127   \n",
       "40                                     scale up failed        0.793977   \n",
       "421  Are there any recommended configuration tuning...        0.444091   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...        3.486474   \n",
       "\n",
       "                initCasusePath1          initCasusePath2  \\\n",
       "738  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "760  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "496  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "248  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "147  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "789  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "928  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "40   Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "421  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "198  Routing Azure HDInsight V5  Scale HDInsight cluster   \n",
       "\n",
       "            initCasusePath3  \\\n",
       "738   Issue with scaling up   \n",
       "760   Issue with scaling up   \n",
       "496   Issue with scaling up   \n",
       "248   Issue with scaling up   \n",
       "147   Issue with scaling up   \n",
       "789   Issue with scaling up   \n",
       "928   Issue with scaling up   \n",
       "40    Issue with scaling up   \n",
       "421  Issue with Autoscaling   \n",
       "198   Issue with scaling up   \n",
       "\n",
       "                                           Symptomstxt  \\\n",
       "738                       Unable to scale the cluster.   \n",
       "760                      LLAP Cluster: Scale up issues   \n",
       "496  Manual scale up failed twice in a row and took...   \n",
       "248                     Cluster not scale up due to RM   \n",
       "147  Complete HBase cluster gets restarted during s...   \n",
       "789         Symptom: When scaling up this action fails   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "40                                        scaling fail   \n",
       "421  Are there any recommended configuration tuning...   \n",
       "198  itleHDI 4.0 : Spark :Scale up operation is fai...   \n",
       "\n",
       "                                         Resolutiontxt  \\\n",
       "738  We see that cluster scale up failed due to Tim...   \n",
       "760  Will track refund under 120080821000073. Pleas...   \n",
       "496  Asked cx to retry manual scale up and it was s...   \n",
       "248  We introduced the ‘node label’ features in the...   \n",
       "147  Microsoft delivered the fix for scale up activ...   \n",
       "789  Resolution: Restarted Ambari and scale was suc...   \n",
       "928       System backup and run with scale-up 36 nodes   \n",
       "40                                               retry   \n",
       "421  we have checked your latest scaling event that...   \n",
       "198  deleted the wnodes manually tested scaling dow...   \n",
       "\n",
       "                                                Titles  \\\n",
       "738  Unable to scale the clusterUnable to scale the...   \n",
       "760  LLAP Cluster: Scale up issuesLLAP Cluster: Sca...   \n",
       "496  auto scale inManual scale up failed twice in a...   \n",
       "248  Cluster not sclae up due to RMCluster not scal...   \n",
       "147  HBase cluster getting restarted during region ...   \n",
       "789  Error rescaling cluster Failed to scale the HD...   \n",
       "928  Scale up cluster failed and cluster is not in ...   \n",
       "40    scale up failedscaling failIssue with scaling up   \n",
       "421  Are there any recommended configuration tuning...   \n",
       "198  HDI  4.0 : Spark :Scale up operation is  faili...   \n",
       "\n",
       "                                                  Body  \\\n",
       "738  We see that cluster scale up failed due to Tim...   \n",
       "760  Will track refund under 120080821000073. Pleas...   \n",
       "496  Asked cx to retry manual scale up and it was s...   \n",
       "248  We introduced the ‘node label’ features in the...   \n",
       "147  Microsoft delivered the fix for scale up activ...   \n",
       "789  Resolution: Restarted Ambari and scale was suc...   \n",
       "928       System backup and run with scale-up 36 nodes   \n",
       "40                                               retry   \n",
       "421  we have checked your latest scaling event that...   \n",
       "198  deleted the wnodes manually tested scaling dow...   \n",
       "\n",
       "                                            Resolution  \n",
       "738  We see that cluster scale up failed due to Tim...  \n",
       "760  Will track refund under 120080821000073. Pleas...  \n",
       "496  Asked cx to retry manual scale up and it was s...  \n",
       "248  We introduced the ‘node label’ features in the...  \n",
       "147  Microsoft delivered the fix for scale up activ...  \n",
       "789  Resolution: Restarted Ambari and scale was suc...  \n",
       "928       System backup and run with scale-up 36 nodes  \n",
       "40                                               retry  \n",
       "421  we have checked your latest scaling event that...  \n",
       "198  deleted the wnodes manually tested scaling dow...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caseData.loc[[738,760, 496, 248, 147, 789, 928,  40, 421, 198],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
